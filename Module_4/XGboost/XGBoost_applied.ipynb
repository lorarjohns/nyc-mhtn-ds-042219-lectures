{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 2, \n",
    "                           alpha = 1, \n",
    "                           n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=0.3, gamma=0, learning_rate=0.1,\n",
       "              max_delta_step=0, max_depth=2, min_child_weight=1, missing=None,\n",
       "              n_estimators=100, n_jobs=1, nthread=None,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "              subsample=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 2, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.685093</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.671230</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.675102</td>\n",
       "      <td>0.008816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.659008</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.663125</td>\n",
       "      <td>0.007590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.644336</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.650664</td>\n",
       "      <td>0.011789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.636066</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.643942</td>\n",
       "      <td>0.013259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.623169</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.012165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.602033</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.610209</td>\n",
       "      <td>0.013601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.591650</td>\n",
       "      <td>0.015947</td>\n",
       "      <td>0.599721</td>\n",
       "      <td>0.021180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.585461</td>\n",
       "      <td>0.019497</td>\n",
       "      <td>0.593338</td>\n",
       "      <td>0.024632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.578619</td>\n",
       "      <td>0.018259</td>\n",
       "      <td>0.586940</td>\n",
       "      <td>0.022570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.570801</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.580262</td>\n",
       "      <td>0.021636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.560062</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.569363</td>\n",
       "      <td>0.024691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.556937</td>\n",
       "      <td>0.020256</td>\n",
       "      <td>0.566690</td>\n",
       "      <td>0.023334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.554872</td>\n",
       "      <td>0.019017</td>\n",
       "      <td>0.564808</td>\n",
       "      <td>0.022910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.546951</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.557979</td>\n",
       "      <td>0.020297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.541852</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.553449</td>\n",
       "      <td>0.020814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.536363</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.548007</td>\n",
       "      <td>0.021016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.529293</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.541409</td>\n",
       "      <td>0.021934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.523798</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>0.536885</td>\n",
       "      <td>0.024915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.520973</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.534507</td>\n",
       "      <td>0.025665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.516376</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.025853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.513121</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.527367</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.509301</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.523904</td>\n",
       "      <td>0.027239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.502340</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>0.517755</td>\n",
       "      <td>0.029960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.495447</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.510592</td>\n",
       "      <td>0.026663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.489835</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>0.505060</td>\n",
       "      <td>0.024653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.488421</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.504394</td>\n",
       "      <td>0.024912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.485823</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.501844</td>\n",
       "      <td>0.026256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.480789</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.498244</td>\n",
       "      <td>0.023527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.479223</td>\n",
       "      <td>0.012897</td>\n",
       "      <td>0.496588</td>\n",
       "      <td>0.025181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.401454</td>\n",
       "      <td>0.010871</td>\n",
       "      <td>0.435937</td>\n",
       "      <td>0.035241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.401162</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.435649</td>\n",
       "      <td>0.035531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.400962</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.435636</td>\n",
       "      <td>0.035789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.400442</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.435524</td>\n",
       "      <td>0.036278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.400060</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>0.435140</td>\n",
       "      <td>0.036231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.399748</td>\n",
       "      <td>0.010698</td>\n",
       "      <td>0.435059</td>\n",
       "      <td>0.036187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.399014</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>0.434782</td>\n",
       "      <td>0.036432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.398543</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>0.434627</td>\n",
       "      <td>0.036380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.398242</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>0.434306</td>\n",
       "      <td>0.036436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.398027</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>0.434199</td>\n",
       "      <td>0.036489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.397627</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.433942</td>\n",
       "      <td>0.036524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.397133</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.433835</td>\n",
       "      <td>0.037015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.396799</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.433926</td>\n",
       "      <td>0.037013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.396528</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>0.433716</td>\n",
       "      <td>0.037159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.396403</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.433744</td>\n",
       "      <td>0.037349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.396163</td>\n",
       "      <td>0.011408</td>\n",
       "      <td>0.433820</td>\n",
       "      <td>0.037432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.395454</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.433707</td>\n",
       "      <td>0.037657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.395105</td>\n",
       "      <td>0.011678</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.037710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.394756</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.433355</td>\n",
       "      <td>0.037991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.394392</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>0.432757</td>\n",
       "      <td>0.038672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.394160</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>0.432578</td>\n",
       "      <td>0.038866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.393845</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.432501</td>\n",
       "      <td>0.039088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.393636</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.432425</td>\n",
       "      <td>0.039194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.393192</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>0.432096</td>\n",
       "      <td>0.039302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.392460</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>0.431824</td>\n",
       "      <td>0.039392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.392240</td>\n",
       "      <td>0.011336</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>0.039511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.391888</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.431388</td>\n",
       "      <td>0.040072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.391675</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.431431</td>\n",
       "      <td>0.040272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.391332</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.431478</td>\n",
       "      <td>0.040197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.391117</td>\n",
       "      <td>0.011108</td>\n",
       "      <td>0.431293</td>\n",
       "      <td>0.040253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.685093           0.000379           0.686558   \n",
       "1              0.671230           0.009434           0.675102   \n",
       "2              0.659008           0.010491           0.663125   \n",
       "3              0.644336           0.014084           0.650664   \n",
       "4              0.636066           0.016528           0.643942   \n",
       "5              0.623169           0.012498           0.631285   \n",
       "6              0.602033           0.009843           0.610209   \n",
       "7              0.591650           0.015947           0.599721   \n",
       "8              0.585461           0.019497           0.593338   \n",
       "9              0.578619           0.018259           0.586940   \n",
       "10             0.570801           0.018858           0.580262   \n",
       "11             0.560062           0.022019           0.569363   \n",
       "12             0.556937           0.020256           0.566690   \n",
       "13             0.554872           0.019017           0.564808   \n",
       "14             0.546951           0.015151           0.557979   \n",
       "15             0.541852           0.011557           0.553449   \n",
       "16             0.536363           0.015298           0.548007   \n",
       "17             0.529293           0.017539           0.541409   \n",
       "18             0.523798           0.019147           0.536885   \n",
       "19             0.520973           0.019159           0.534507   \n",
       "20             0.516376           0.021708           0.530500   \n",
       "21             0.513121           0.022248           0.527367   \n",
       "22             0.509301           0.024330           0.523904   \n",
       "23             0.502340           0.024646           0.517755   \n",
       "24             0.495447           0.019774           0.510592   \n",
       "25             0.489835           0.016329           0.505060   \n",
       "26             0.488421           0.016179           0.504394   \n",
       "27             0.485823           0.015821           0.501844   \n",
       "28             0.480789           0.013510           0.498244   \n",
       "29             0.479223           0.012897           0.496588   \n",
       "..                  ...                ...                ...   \n",
       "102            0.401454           0.010871           0.435937   \n",
       "103            0.401162           0.010739           0.435649   \n",
       "104            0.400962           0.010795           0.435636   \n",
       "105            0.400442           0.010828           0.435524   \n",
       "106            0.400060           0.010824           0.435140   \n",
       "107            0.399748           0.010698           0.435059   \n",
       "108            0.399014           0.010926           0.434782   \n",
       "109            0.398543           0.010926           0.434627   \n",
       "110            0.398242           0.010878           0.434306   \n",
       "111            0.398027           0.010912           0.434199   \n",
       "112            0.397627           0.011141           0.433942   \n",
       "113            0.397133           0.011175           0.433835   \n",
       "114            0.396799           0.011203           0.433926   \n",
       "115            0.396528           0.011347           0.433716   \n",
       "116            0.396403           0.011355           0.433744   \n",
       "117            0.396163           0.011408           0.433820   \n",
       "118            0.395454           0.011635           0.433707   \n",
       "119            0.395105           0.011678           0.433470   \n",
       "120            0.394756           0.011652           0.433355   \n",
       "121            0.394392           0.011326           0.432757   \n",
       "122            0.394160           0.011433           0.432578   \n",
       "123            0.393845           0.011346           0.432501   \n",
       "124            0.393636           0.011348           0.432425   \n",
       "125            0.393192           0.011366           0.432096   \n",
       "126            0.392460           0.011382           0.431824   \n",
       "127            0.392240           0.011336           0.431829   \n",
       "128            0.391888           0.011091           0.431388   \n",
       "129            0.391675           0.011100           0.431431   \n",
       "130            0.391332           0.011102           0.431478   \n",
       "131            0.391117           0.011108           0.431293   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.000999  \n",
       "1            0.008816  \n",
       "2            0.007590  \n",
       "3            0.011789  \n",
       "4            0.013259  \n",
       "5            0.012165  \n",
       "6            0.013601  \n",
       "7            0.021180  \n",
       "8            0.024632  \n",
       "9            0.022570  \n",
       "10           0.021636  \n",
       "11           0.024691  \n",
       "12           0.023334  \n",
       "13           0.022910  \n",
       "14           0.020297  \n",
       "15           0.020814  \n",
       "16           0.021016  \n",
       "17           0.021934  \n",
       "18           0.024915  \n",
       "19           0.025665  \n",
       "20           0.025853  \n",
       "21           0.026611  \n",
       "22           0.027239  \n",
       "23           0.029960  \n",
       "24           0.026663  \n",
       "25           0.024653  \n",
       "26           0.024912  \n",
       "27           0.026256  \n",
       "28           0.023527  \n",
       "29           0.025181  \n",
       "..                ...  \n",
       "102          0.035241  \n",
       "103          0.035531  \n",
       "104          0.035789  \n",
       "105          0.036278  \n",
       "106          0.036231  \n",
       "107          0.036187  \n",
       "108          0.036432  \n",
       "109          0.036380  \n",
       "110          0.036436  \n",
       "111          0.036489  \n",
       "112          0.036524  \n",
       "113          0.037015  \n",
       "114          0.037013  \n",
       "115          0.037159  \n",
       "116          0.037349  \n",
       "117          0.037432  \n",
       "118          0.037657  \n",
       "119          0.037710  \n",
       "120          0.037991  \n",
       "121          0.038672  \n",
       "122          0.038866  \n",
       "123          0.039088  \n",
       "124          0.039194  \n",
       "125          0.039302  \n",
       "126          0.039392  \n",
       "127          0.039511  \n",
       "128          0.040072  \n",
       "129          0.040272  \n",
       "130          0.040197  \n",
       "131          0.040253  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8829\n",
      "AUC Score (Train): 0.928483\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807175\n",
      "F1: 0.703448\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup our parameters for XGBoost to test\n",
    "param_test1 = {\n",
    " 'max_depth':range(2,9,2),\n",
    " 'min_child_weight':range(1,6,1),\n",
    "    'colsample_bytree':(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8),\n",
    "    'learning_rate':[0.05,0.1,0.15,0.2,0.25,0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n",
    "    param_grid = param_test1, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    iid=False, \n",
    "    cv=5,\n",
    "    verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 960 candidates, totalling 4800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1292 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1928 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2658 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3548 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4602 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4800 out of 4800 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bytree=0.8,\n",
       "                                     gamma=0, learning_rate=0.1,\n",
       "                                     max_delta_step=0, max_depth=5,\n",
       "                                     min_child_weight=1, missing=None,\n",
       "                                     n_estimators=140, n_jobs=1, nthread=4,\n",
       "                                     objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=27, silent=True,\n",
       "                                     subsample=0.8),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n",
       "                                              0.8),\n",
       "                         'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
       "                         'max_depth': range(2, 9, 2),\n",
       "                         'min_child_weight': range(1, 6)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.15762239, 0.23695884, 0.16103115, 0.19712677, 0.13261642,\n",
       "        0.25872483, 0.22256236, 0.20490036, 0.17991939, 0.13963361,\n",
       "        0.25355053, 0.20522141, 0.13374901, 0.12944746, 0.30540481,\n",
       "        0.18356147, 0.09554105, 0.23040643, 0.20965419, 0.30203624,\n",
       "        0.12510977, 0.18571486, 0.15032167, 0.12441549, 0.12925515,\n",
       "        0.11968656, 0.24615641, 0.08567119, 0.18393879, 0.17198968,\n",
       "        0.14803801, 0.16115556, 0.17861061, 0.12702751, 0.17116303,\n",
       "        0.16619186, 0.16789737, 0.19511285, 0.11684718, 0.19913673,\n",
       "        0.09911437, 0.22306967, 0.18338213, 0.11925063, 0.08063507,\n",
       "        0.19889798, 0.17017961, 0.1760674 , 0.14562964, 0.13637123,\n",
       "        0.14200053, 0.1930357 , 0.24446664, 0.16150565, 0.15795417,\n",
       "        0.25898099, 0.24195952, 0.21129193, 0.18615503, 0.11365066,\n",
       "        0.09730043, 0.16101885, 0.18030896, 0.13203902, 0.07882972,\n",
       "        0.14766788, 0.16295171, 0.15954218, 0.11708021, 0.10511575,\n",
       "        0.32314043, 0.15320945, 0.17250533, 0.1327662 , 0.12164803,\n",
       "        0.16539016, 0.17043509, 0.21278772, 0.15293603, 0.29052219,\n",
       "        0.22901769, 0.19782124, 0.16546459, 0.15909452, 0.16886201,\n",
       "        0.11745267, 0.23986282, 0.13084564, 0.20729799, 0.14064445,\n",
       "        0.2161345 , 0.1874855 , 0.1436121 , 0.24569545, 0.25374684,\n",
       "        0.13809628, 0.29570599, 0.13855658, 0.09381795, 0.17411995,\n",
       "        0.17909493, 0.16872396, 0.14831142, 0.19629326, 0.21363912,\n",
       "        0.13238411, 0.17349815, 0.12538848, 0.18697515, 0.28227501,\n",
       "        0.13062582, 0.20632429, 0.19678764, 0.17786059, 0.21241055,\n",
       "        0.08268852, 0.26665869, 0.18094945, 0.22538762, 0.27721386,\n",
       "        0.22182708, 0.1465086 , 0.13600807, 0.24199405, 0.1579402 ,\n",
       "        0.22668877, 0.24336762, 0.1246273 , 0.23080273, 0.15192938,\n",
       "        0.17185149, 0.15321627, 0.19516721, 0.16730113, 0.21077151,\n",
       "        0.11876798, 0.17658362, 0.1374835 , 0.21256275, 0.23346696,\n",
       "        0.12750759, 0.22699628, 0.25259361, 0.12695045, 0.21553502,\n",
       "        0.19100776, 0.17301188, 0.19926867, 0.13259754, 0.25591216,\n",
       "        0.25640898, 0.14028759, 0.17041922, 0.19829741, 0.17995925,\n",
       "        0.16192179, 0.17269802, 0.17254505, 0.17074609, 0.20513048,\n",
       "        0.15799813, 0.20672626, 0.16116076, 0.23818321, 0.09189177,\n",
       "        0.28835521, 0.13948388, 0.22362747, 0.19810352, 0.17962141,\n",
       "        0.20673141, 0.19103441, 0.20521197, 0.17447062, 0.2120944 ,\n",
       "        0.22530403, 0.13112497, 0.14141502, 0.21491895, 0.23223858,\n",
       "        0.12467918, 0.11256485, 0.09460354, 0.20553408, 0.19876928,\n",
       "        0.19839368, 0.30232439, 0.16320767, 0.19892101, 0.14016542,\n",
       "        0.18418226, 0.20307193, 0.31510739, 0.16699581, 0.11406751,\n",
       "        0.34427013, 0.14585061, 0.22900434, 0.18516121, 0.16955032,\n",
       "        0.11963525, 0.15566664, 0.20851712, 0.13308959, 0.21451917,\n",
       "        0.19284739, 0.19965949, 0.16539311, 0.21283441, 0.22381215,\n",
       "        0.16710477, 0.20660663, 0.16290927, 0.17291861, 0.31925092,\n",
       "        0.22763124, 0.23597422, 0.10693069, 0.17808423, 0.27976904,\n",
       "        0.1473629 , 0.19775891, 0.14625864, 0.18168459, 0.16997991,\n",
       "        0.2173265 , 0.13167739, 0.23804212, 0.19987884, 0.20269217,\n",
       "        0.20188022, 0.13771024, 0.25059505, 0.1428854 , 0.15899906,\n",
       "        0.26394835, 0.1787291 , 0.29372277, 0.11643252, 0.18524046,\n",
       "        0.1610918 , 0.17599506, 0.20251031, 0.18053508, 0.09061813,\n",
       "        0.2599268 , 0.11566882, 0.20239849, 0.22556162, 0.30534053,\n",
       "        0.22672634, 0.1767262 , 0.26520715, 0.23786387, 0.40258613,\n",
       "        0.18637795, 0.19211655, 0.27494855, 0.25238676, 0.36214175,\n",
       "        0.14791203, 0.24497318, 0.1823585 , 0.28956108, 0.19018798,\n",
       "        0.1723279 , 0.25958099, 0.26711197, 0.19740176, 0.13100019,\n",
       "        0.30891447, 0.25508094, 0.2239234 , 0.30228863, 0.19725771,\n",
       "        0.3039814 , 0.5359354 , 0.22372627, 0.27283664, 0.2485774 ,\n",
       "        0.16348152, 0.148245  , 0.12663016, 0.16867294, 0.10470738,\n",
       "        0.27698078, 0.23104281, 0.16193399, 0.24589615, 0.26464601,\n",
       "        0.252036  , 0.25123506, 0.21310177, 0.2009728 , 0.2747191 ,\n",
       "        0.31717343, 0.29124146, 0.23683834, 0.23092861, 0.21440139,\n",
       "        0.16169219, 0.22251639, 0.28564124, 0.18508053, 0.10353808,\n",
       "        0.26028476, 0.22491117, 0.18745813, 0.15720716, 0.13591161,\n",
       "        0.23526874, 0.19820485, 0.28062015, 0.17219033, 0.23963046,\n",
       "        0.22291517, 0.50734344, 0.14315419, 0.23247857, 0.16443186,\n",
       "        0.11659856, 0.14111919, 0.20838923, 0.20636716, 0.22259917,\n",
       "        0.21060205, 0.22862782, 0.32441673, 0.21804442, 0.17248478,\n",
       "        0.26479063, 0.09379406, 0.31191359, 0.24163604, 0.25529876,\n",
       "        0.43548026, 0.18977985, 0.23110957, 0.27233834, 0.31951261,\n",
       "        0.22777286, 0.24356818, 0.21923714, 0.18882146, 0.23128777,\n",
       "        0.36793075, 0.2581286 , 0.28608603, 0.26391959, 0.18265042,\n",
       "        0.40275335, 0.23933372, 0.30261493, 0.17922444, 0.23262839,\n",
       "        0.33061514, 0.20994511, 0.31429515, 0.58473287, 0.31470819,\n",
       "        0.22867618, 0.2071712 , 0.16501012, 0.17873349, 0.18415723,\n",
       "        0.19324818, 0.2665616 , 0.35846725, 0.26502743, 0.35915036,\n",
       "        0.26996646, 0.35597672, 0.48315735, 0.34299989, 0.28923912,\n",
       "        0.5638998 , 0.51502872, 0.32049527, 0.29414573, 0.36269064,\n",
       "        0.24003663, 0.09325614, 0.27354856, 0.20432243, 0.12453785,\n",
       "        0.26101384, 0.46802502, 0.43377905, 0.25751204, 0.22961159,\n",
       "        0.39495144, 0.27431107, 0.23890529, 0.32083673, 0.30767665,\n",
       "        0.31644278, 0.41765103, 0.36265664, 0.46858602, 0.27435122,\n",
       "        0.19491601, 0.22514925, 0.25457206, 0.25021291, 0.1990366 ,\n",
       "        0.3463758 , 0.23209019, 0.25790582, 0.33078198, 0.25817761,\n",
       "        0.5231442 , 0.2784358 , 0.27502465, 0.27867217, 0.5076067 ,\n",
       "        0.38115063, 0.39172606, 0.6036284 , 0.33440104, 0.34005876,\n",
       "        0.14229445, 0.13690782, 0.18489146, 0.13025374, 0.30319052,\n",
       "        0.32228317, 0.30116086, 0.22925963, 0.24091949, 0.29313912,\n",
       "        0.33435888, 0.29127331, 0.34313149, 0.22763224, 0.35924339,\n",
       "        0.36619186, 0.70354986, 0.37471504, 0.38119187, 0.43816166,\n",
       "        0.12803583, 0.25195608, 0.15921445, 0.13471184, 0.1953639 ,\n",
       "        0.26662464, 0.22062297, 0.26958857, 0.21461921, 0.27896781,\n",
       "        0.37183928, 0.22471104, 0.27959676, 0.36211309, 0.17812958,\n",
       "        0.44449039, 0.38546782, 0.33107576, 0.34366698, 0.39935794,\n",
       "        0.181317  , 0.23299818, 0.18271937, 0.20581779, 0.24592128,\n",
       "        0.50343871, 0.40646243, 0.34234219, 0.24718313, 0.34994054,\n",
       "        0.46101785, 0.39593439, 0.26089215, 0.33235788, 0.26123424,\n",
       "        0.38447146, 0.47297502, 0.29205103, 0.27750239, 0.16177597,\n",
       "        0.15510421, 0.27756271, 0.28578596, 0.18567715, 0.27357969,\n",
       "        0.23831887, 0.34912801, 0.21336732, 0.28498912, 0.32764721,\n",
       "        0.379743  , 0.30210352, 0.35338492, 0.45039983, 0.37604313,\n",
       "        0.36864748, 0.3658505 , 0.24828358, 0.2787178 , 0.31440248,\n",
       "        0.20690098, 0.19153185, 0.21418495, 0.18954601, 0.21024551,\n",
       "        0.34627991, 0.23950763, 0.34793811, 0.31501007, 0.24481936,\n",
       "        0.25323129, 0.4821269 , 0.32025714, 0.40456672, 0.39925866,\n",
       "        0.4934042 , 0.43315792, 0.33555408, 0.33999915, 0.46760492,\n",
       "        0.25788236, 0.15868154, 0.18533521, 0.18879395, 0.18645005,\n",
       "        0.37553267, 0.23111291, 0.26216855, 0.30021977, 0.29119329,\n",
       "        0.30566421, 0.2616118 , 0.20546703, 0.46188426, 0.23761773,\n",
       "        0.4929512 , 0.44387345, 0.37912116, 0.21636047, 0.20156555,\n",
       "        0.17554107, 0.24531717, 0.29264627, 0.13754697, 0.14575849,\n",
       "        0.30592122, 0.30509334, 0.2066577 , 0.17529712, 0.26498384,\n",
       "        0.34763279, 0.2980576 , 0.52631717, 0.26391363, 0.22994099,\n",
       "        0.35665841, 0.38827071, 0.242553  , 0.22909827, 0.30027699,\n",
       "        0.30911975, 0.22480917, 0.2042294 , 0.18030887, 0.18709073,\n",
       "        0.20320792, 0.28181701, 0.32468305, 0.30901451, 0.46567693,\n",
       "        0.232693  , 0.28303742, 0.27032442, 0.29690242, 0.24130964,\n",
       "        0.31887441, 0.39597077, 0.28743691, 0.2754086 , 0.39498305,\n",
       "        0.17369628, 0.14370627, 0.20351825, 0.27647243, 0.23206434,\n",
       "        0.25596838, 0.20816703, 0.20793772, 0.42846966, 0.20453053,\n",
       "        0.27108064, 0.28001928, 0.38483391, 0.29398904, 0.33116226,\n",
       "        0.3917161 , 0.37208862, 0.25625682, 0.29909506, 0.29934292,\n",
       "        0.27661572, 0.21943307, 0.20514278, 0.11488857, 0.20661135,\n",
       "        0.28824658, 0.29456887, 0.34920936, 0.27909479, 0.33504601,\n",
       "        0.39435534, 0.4073422 , 0.44464297, 0.25060911, 0.28797817,\n",
       "        0.42892857, 0.5476584 , 0.338661  , 0.28270283, 0.25851002,\n",
       "        0.21442323, 0.26280103, 0.19306855, 0.18925676, 0.23435602,\n",
       "        0.25098491, 0.23401637, 0.37675781, 0.33363256, 0.25729046,\n",
       "        0.53467112, 0.22282519, 0.26752377, 0.39606256, 0.26803999,\n",
       "        0.33475604, 0.38563647, 0.4431982 , 0.48064733, 0.250635  ,\n",
       "        0.27856646, 0.31681786, 0.21060796, 0.09501648, 0.25213747,\n",
       "        0.22679839, 0.24699969, 0.30562677, 0.38308711, 0.20925498,\n",
       "        0.3198648 , 0.48146791, 0.28491259, 0.398173  , 0.29529467,\n",
       "        0.40395374, 0.4117507 , 0.2413312 , 0.25134473, 0.2681313 ,\n",
       "        0.15982337, 0.22097683, 0.19867182, 0.2303154 , 0.15652695,\n",
       "        0.36267643, 0.24342756, 0.30353408, 0.29994011, 0.24238   ,\n",
       "        0.43262115, 0.33725877, 0.3693419 , 0.45534215, 0.19990473,\n",
       "        0.42478495, 0.24688396, 0.42354178, 0.28653584, 0.36634932,\n",
       "        0.15708227, 0.15910044, 0.28371916, 0.22043176, 0.20288591,\n",
       "        0.30877771, 0.30537372, 0.29582748, 0.33961539, 0.31268353,\n",
       "        0.35191078, 0.39684167, 0.27515597, 0.32881393, 0.36803012,\n",
       "        0.46922445, 0.31225858, 0.36869659, 0.28030219, 0.21958771,\n",
       "        0.2246346 , 0.17389803, 0.21283274, 0.24901037, 0.15584965,\n",
       "        0.26523142, 0.30513859, 0.24327765, 0.27291188, 0.3189353 ,\n",
       "        0.31410856, 0.30298214, 0.36095753, 0.33273492, 0.32530107,\n",
       "        0.44584594, 0.26307087, 0.34945579, 0.28825903, 0.43999   ,\n",
       "        0.17640882, 0.23590059, 0.27410398, 0.20938621, 0.21134076,\n",
       "        0.1528193 , 0.21855817, 0.21866689, 0.27035418, 0.32901015,\n",
       "        0.39383721, 0.28493953, 0.51314011, 0.31353831, 0.33571463,\n",
       "        0.3701611 , 0.42514067, 0.36835847, 0.31610889, 0.22755079,\n",
       "        0.27419   , 0.24577117, 0.14442172, 0.18406329, 0.12834315,\n",
       "        0.27339573, 0.27797585, 0.29071465, 0.34797149, 0.26704488,\n",
       "        0.36137981, 0.49035869, 0.29680285, 0.21046271, 0.35230198,\n",
       "        0.50888419, 0.26921401, 0.29407101, 0.38264718, 0.22566886,\n",
       "        0.31407161, 0.09227118, 0.25843654, 0.23775859, 0.18674068,\n",
       "        0.27588601, 0.29417362, 0.24432859, 0.30254912, 0.38395262,\n",
       "        0.32548242, 0.38702154, 0.24296465, 0.36109285, 0.26028385,\n",
       "        0.45947237, 0.26841159, 0.28581223, 0.27734008, 0.24623194,\n",
       "        0.14284253, 0.25314193, 0.2307735 , 0.27133746, 0.30367746,\n",
       "        0.20910406, 0.30277219, 0.18684545, 0.2302711 , 0.23076229,\n",
       "        0.34967709, 0.51319938, 0.33383102, 0.25763397, 0.22866321,\n",
       "        0.38257365, 0.24130111, 0.46403451, 0.29294782, 0.37524595,\n",
       "        0.19971838, 0.16990938, 0.2438952 , 0.22725563, 0.22291574,\n",
       "        0.21257396, 0.34900775, 0.43185606, 0.29373546, 0.40489764,\n",
       "        0.51265326, 0.55695329, 0.33643789, 0.22702217, 0.40947609,\n",
       "        0.47865157, 0.47460208, 0.37059722, 0.38928857, 0.3596055 ,\n",
       "        0.23540907, 0.19556136, 0.33134761, 0.23239226, 0.14512467,\n",
       "        0.25961781, 0.37592182, 0.24463954, 0.36943421, 0.20363111,\n",
       "        0.43850918, 0.36194658, 0.38222585, 0.24067011, 0.34918818,\n",
       "        0.51879025, 0.28236308, 0.61881781, 0.27699132, 0.35688963,\n",
       "        0.18619165, 0.15577378, 0.22869272, 0.25743179, 0.18898268,\n",
       "        0.27787962, 0.25473919, 0.49076738, 0.27252827, 0.23791137,\n",
       "        0.28480678, 0.32433419, 0.28420758, 0.45508161, 0.29524007,\n",
       "        0.30024748, 0.38448639, 0.25547838, 0.23418441, 0.32706299,\n",
       "        0.1997438 , 0.18172197, 0.23111424, 0.17260261, 0.23637457,\n",
       "        0.38241782, 0.23929706, 0.27611485, 0.281359  , 0.16673336,\n",
       "        0.38387604, 0.21866941, 0.29351926, 0.34130635, 0.26792459,\n",
       "        0.37044315, 0.44876823, 0.24687719, 0.28075299, 0.2931129 ,\n",
       "        0.34400702, 0.1983768 , 0.15629058, 0.13665667, 0.15403066,\n",
       "        0.30803819, 0.20603876, 0.34175344, 0.2470665 , 0.32166996,\n",
       "        0.21071081, 0.38550882, 0.29536171, 0.28584938, 0.28041534,\n",
       "        0.22645745, 0.3388844 , 0.37863698, 0.30493913, 0.3005672 ,\n",
       "        0.24783969, 0.169488  , 0.12381563, 0.16003017, 0.18062615,\n",
       "        0.19408817, 0.22926178, 0.22145872, 0.35891037, 0.29886947,\n",
       "        0.23284464, 0.32828407, 0.19565248, 0.25554695, 0.34529886,\n",
       "        0.45421214, 0.23112078, 0.22221265, 0.53606329, 0.34682717,\n",
       "        0.25983944, 0.24493008, 0.13253407, 0.11909094, 0.20619698,\n",
       "        0.17953815, 0.30823112, 0.26455932, 0.24172177, 0.23885498,\n",
       "        0.31051798, 0.2700047 , 0.45458102, 0.27516356, 0.15513749,\n",
       "        0.41247311, 0.24908023, 0.33290663, 0.33764052, 0.26518502,\n",
       "        0.24622178, 0.19131308, 0.20692415, 0.20399699, 0.2167901 ,\n",
       "        0.23992767, 0.38355432, 0.24579   , 0.22080026, 0.20121632,\n",
       "        0.33359351, 0.27122207, 0.28950424, 0.33911328, 0.26310019,\n",
       "        0.35343289, 0.31353245, 0.30385528, 0.30765948, 0.16114669]),\n",
       " 'std_fit_time': array([0.09643682, 0.02698559, 0.0641896 , 0.10372966, 0.05157504,\n",
       "        0.11647022, 0.12325726, 0.10541949, 0.07664583, 0.0956302 ,\n",
       "        0.13650797, 0.13692091, 0.07322964, 0.05232371, 0.23539544,\n",
       "        0.11764718, 0.08220215, 0.15205093, 0.14766422, 0.14827953,\n",
       "        0.05596668, 0.08670109, 0.05509878, 0.08862399, 0.09277673,\n",
       "        0.05982395, 0.10580866, 0.0362169 , 0.08647976, 0.11930134,\n",
       "        0.11736379, 0.11937153, 0.11249242, 0.05742775, 0.10813231,\n",
       "        0.10051249, 0.1432377 , 0.0817759 , 0.13632406, 0.09499974,\n",
       "        0.05977425, 0.11821362, 0.07729069, 0.09032971, 0.05506899,\n",
       "        0.19456124, 0.12880913, 0.13266703, 0.11561928, 0.06325961,\n",
       "        0.09117131, 0.06232368, 0.17709508, 0.06250549, 0.08738167,\n",
       "        0.10937884, 0.14805893, 0.15392774, 0.10748291, 0.09704269,\n",
       "        0.08250049, 0.0809132 , 0.14061974, 0.07753183, 0.03997104,\n",
       "        0.07974601, 0.16957697, 0.08419763, 0.05558189, 0.04798603,\n",
       "        0.15679551, 0.03738635, 0.08419597, 0.06066341, 0.04486606,\n",
       "        0.0811972 , 0.06765782, 0.1109382 , 0.10112882, 0.08513235,\n",
       "        0.14694198, 0.13044424, 0.05268041, 0.11306438, 0.08192727,\n",
       "        0.12298286, 0.14525765, 0.07207978, 0.0959286 , 0.16229057,\n",
       "        0.15606706, 0.15724686, 0.07385428, 0.17657855, 0.23199518,\n",
       "        0.07217513, 0.16632391, 0.07065308, 0.06674999, 0.11390084,\n",
       "        0.06359265, 0.07926215, 0.04683023, 0.10591699, 0.07066438,\n",
       "        0.05333043, 0.07296517, 0.02749795, 0.14398106, 0.21301863,\n",
       "        0.05074344, 0.13279935, 0.10340851, 0.08520582, 0.09714895,\n",
       "        0.07560832, 0.15740698, 0.08623592, 0.13265966, 0.17348883,\n",
       "        0.10846642, 0.12612905, 0.06398533, 0.11241681, 0.08906615,\n",
       "        0.1243405 , 0.20056335, 0.08725491, 0.13127331, 0.05492008,\n",
       "        0.21191797, 0.07234019, 0.06892806, 0.10492764, 0.25566527,\n",
       "        0.06040025, 0.09234915, 0.08587666, 0.16156597, 0.04354496,\n",
       "        0.09157345, 0.1046474 , 0.16668648, 0.05718122, 0.08463725,\n",
       "        0.04850397, 0.11204013, 0.12154567, 0.0841509 , 0.12243339,\n",
       "        0.11138602, 0.10758629, 0.11970297, 0.0898821 , 0.1298419 ,\n",
       "        0.10854507, 0.14996705, 0.16039701, 0.17540445, 0.09567197,\n",
       "        0.08282521, 0.10520955, 0.09559724, 0.18964989, 0.04077408,\n",
       "        0.07588961, 0.07467132, 0.17007115, 0.09770711, 0.11197886,\n",
       "        0.12555271, 0.06716099, 0.09294488, 0.11009883, 0.11885533,\n",
       "        0.09742153, 0.12243576, 0.17433426, 0.13619052, 0.21648264,\n",
       "        0.09408934, 0.0889887 , 0.03679889, 0.09363169, 0.05954793,\n",
       "        0.0932102 , 0.2552252 , 0.12640393, 0.1423677 , 0.07774137,\n",
       "        0.07024562, 0.15237189, 0.1189267 , 0.07520677, 0.0525548 ,\n",
       "        0.30012823, 0.06557981, 0.13031831, 0.12835536, 0.09904435,\n",
       "        0.06533367, 0.1149554 , 0.07429387, 0.08724628, 0.06127976,\n",
       "        0.16128946, 0.06028856, 0.04721039, 0.13980583, 0.05591369,\n",
       "        0.18119023, 0.09090676, 0.05987153, 0.07909092, 0.20538649,\n",
       "        0.09898868, 0.07398676, 0.03859112, 0.11148786, 0.12660081,\n",
       "        0.07267927, 0.12396811, 0.05253639, 0.11673605, 0.07465193,\n",
       "        0.12587148, 0.06936392, 0.08192973, 0.08461682, 0.08670855,\n",
       "        0.11915173, 0.07508978, 0.10407635, 0.08449757, 0.13231393,\n",
       "        0.11584125, 0.09550812, 0.14400403, 0.09604068, 0.13167972,\n",
       "        0.11690707, 0.11978307, 0.15362664, 0.10595593, 0.0516095 ,\n",
       "        0.16572811, 0.04217442, 0.08298953, 0.0491987 , 0.12636352,\n",
       "        0.16935879, 0.11919969, 0.02805047, 0.06552708, 0.37954154,\n",
       "        0.08344898, 0.14365614, 0.14947865, 0.06090011, 0.15512003,\n",
       "        0.10195788, 0.11284455, 0.08701195, 0.15193629, 0.09788423,\n",
       "        0.12758223, 0.11433823, 0.09245634, 0.07871096, 0.06757466,\n",
       "        0.13848469, 0.12172119, 0.18958252, 0.18050318, 0.1067346 ,\n",
       "        0.08515948, 0.3866653 , 0.15452243, 0.21574351, 0.12653394,\n",
       "        0.07632768, 0.05709437, 0.13988185, 0.12922579, 0.03593622,\n",
       "        0.16807057, 0.10475973, 0.10111812, 0.23481585, 0.08359701,\n",
       "        0.16884286, 0.13787958, 0.14781634, 0.13786239, 0.06841195,\n",
       "        0.11718074, 0.1041246 , 0.11871198, 0.10702837, 0.07653839,\n",
       "        0.10317081, 0.11224553, 0.07111626, 0.15225367, 0.05800289,\n",
       "        0.07849264, 0.11396852, 0.14293462, 0.07582859, 0.04525191,\n",
       "        0.0898607 , 0.09827162, 0.20095432, 0.05778687, 0.22398538,\n",
       "        0.23430281, 0.2273131 , 0.06848241, 0.12975683, 0.02911804,\n",
       "        0.08033234, 0.08686099, 0.05073439, 0.13630879, 0.17017058,\n",
       "        0.14375264, 0.16136684, 0.03781113, 0.12450437, 0.12688158,\n",
       "        0.08075295, 0.05279137, 0.17938082, 0.07958966, 0.1102275 ,\n",
       "        0.15017419, 0.10336884, 0.11402195, 0.14602868, 0.23399438,\n",
       "        0.13050129, 0.08244962, 0.06643819, 0.0611621 , 0.09129206,\n",
       "        0.20825678, 0.093115  , 0.10429412, 0.03693133, 0.120112  ,\n",
       "        0.42408716, 0.13455184, 0.1550723 , 0.10172504, 0.17928863,\n",
       "        0.17891659, 0.08523419, 0.15909279, 0.12801958, 0.21995079,\n",
       "        0.13390147, 0.11243234, 0.05892085, 0.09400527, 0.08615982,\n",
       "        0.11085126, 0.15168583, 0.24932966, 0.12917319, 0.13805147,\n",
       "        0.08802471, 0.15805904, 0.08190825, 0.18995101, 0.1666728 ,\n",
       "        0.22057654, 0.24854513, 0.0992444 , 0.06209745, 0.37144498,\n",
       "        0.16967432, 0.03641314, 0.11373145, 0.11842849, 0.0816504 ,\n",
       "        0.18199024, 0.09593092, 0.14227938, 0.13813577, 0.15085114,\n",
       "        0.22636993, 0.14506414, 0.18676337, 0.1939277 , 0.25686901,\n",
       "        0.04771567, 0.32624123, 0.07010399, 0.32092457, 0.09130816,\n",
       "        0.07112267, 0.09677291, 0.14328354, 0.11014717, 0.14061195,\n",
       "        0.13854982, 0.08874596, 0.13998382, 0.11609785, 0.1095345 ,\n",
       "        0.45738948, 0.16065547, 0.07466113, 0.14578131, 0.06458912,\n",
       "        0.20187059, 0.15067577, 0.4917811 , 0.16438486, 0.18177047,\n",
       "        0.10017086, 0.11718503, 0.10790869, 0.05449038, 0.14511158,\n",
       "        0.14362706, 0.1216748 , 0.07690212, 0.19220448, 0.19380837,\n",
       "        0.22002868, 0.08638794, 0.09209748, 0.10900204, 0.04598879,\n",
       "        0.23129373, 0.47319942, 0.19798395, 0.11776826, 0.17004212,\n",
       "        0.10804073, 0.09345209, 0.08817548, 0.06281897, 0.08663986,\n",
       "        0.09099674, 0.13452628, 0.08889285, 0.13233852, 0.09251442,\n",
       "        0.11921154, 0.13465968, 0.14031472, 0.24565547, 0.10356865,\n",
       "        0.06121804, 0.07827718, 0.11385285, 0.18443774, 0.22362211,\n",
       "        0.13584477, 0.10190208, 0.10162001, 0.13476773, 0.10352026,\n",
       "        0.1929733 , 0.19297086, 0.16005081, 0.12138442, 0.15094129,\n",
       "        0.15632046, 0.15513301, 0.13117763, 0.13801388, 0.14156344,\n",
       "        0.17262017, 0.2200463 , 0.1043504 , 0.1292893 , 0.13403345,\n",
       "        0.07051284, 0.08915255, 0.13233186, 0.07972209, 0.197487  ,\n",
       "        0.14777776, 0.17466732, 0.05445199, 0.09009768, 0.14786705,\n",
       "        0.08852252, 0.14318836, 0.23190443, 0.20072555, 0.09813858,\n",
       "        0.15335569, 0.22583228, 0.08638758, 0.09716456, 0.13601406,\n",
       "        0.14736414, 0.07752928, 0.10468574, 0.07655969, 0.10398776,\n",
       "        0.22672491, 0.15150227, 0.09241361, 0.12536796, 0.15892371,\n",
       "        0.12467509, 0.13556615, 0.19108843, 0.13440484, 0.15691946,\n",
       "        0.16450032, 0.0713559 , 0.13475333, 0.017425  , 0.3756016 ,\n",
       "        0.13305972, 0.06944076, 0.08414872, 0.07822981, 0.02498813,\n",
       "        0.13026891, 0.14744604, 0.0870731 , 0.13057858, 0.11734246,\n",
       "        0.17445154, 0.08587071, 0.12361784, 0.22219413, 0.15975577,\n",
       "        0.21679404, 0.17103707, 0.18894057, 0.14201576, 0.09140749,\n",
       "        0.05550639, 0.23119027, 0.20882292, 0.0435467 , 0.07643901,\n",
       "        0.17870787, 0.1496141 , 0.09050363, 0.0750971 , 0.12765894,\n",
       "        0.1690053 , 0.09814169, 0.15085798, 0.08390036, 0.14676171,\n",
       "        0.10800413, 0.20451764, 0.1575553 , 0.08277593, 0.07699977,\n",
       "        0.12930812, 0.11217838, 0.11376061, 0.14673079, 0.07714544,\n",
       "        0.04713821, 0.1656696 , 0.15433842, 0.12392157, 0.37279026,\n",
       "        0.12475946, 0.08961833, 0.11135173, 0.17413131, 0.16086397,\n",
       "        0.0826136 , 0.16198427, 0.10428091, 0.07383675, 0.1224686 ,\n",
       "        0.08555688, 0.06310142, 0.16667427, 0.06987999, 0.10936259,\n",
       "        0.07173716, 0.08159066, 0.10926567, 0.10799694, 0.09568908,\n",
       "        0.10583928, 0.11780247, 0.12669377, 0.07326204, 0.13012176,\n",
       "        0.15923721, 0.18958127, 0.14280234, 0.07902937, 0.14962416,\n",
       "        0.07841024, 0.13492494, 0.10767028, 0.03783906, 0.09480351,\n",
       "        0.14139766, 0.11544648, 0.1617281 , 0.04392912, 0.1088208 ,\n",
       "        0.21624573, 0.13952277, 0.19176022, 0.1280688 , 0.10762891,\n",
       "        0.17989571, 0.23916715, 0.08136272, 0.13661928, 0.04835377,\n",
       "        0.07927035, 0.0730612 , 0.05874764, 0.08254104, 0.09073058,\n",
       "        0.07522857, 0.13991376, 0.10834319, 0.15803246, 0.09460575,\n",
       "        0.34464623, 0.1100181 , 0.12127961, 0.15472838, 0.17127796,\n",
       "        0.18864727, 0.15656193, 0.141832  , 0.28385446, 0.09005462,\n",
       "        0.04918915, 0.05772278, 0.09491978, 0.04336624, 0.10667136,\n",
       "        0.07340161, 0.13307512, 0.10962956, 0.11506254, 0.10507807,\n",
       "        0.12280524, 0.14125337, 0.12256629, 0.03896114, 0.12647326,\n",
       "        0.14353756, 0.27093275, 0.14800544, 0.09913406, 0.13956751,\n",
       "        0.10331227, 0.14023294, 0.09823389, 0.14835912, 0.04631982,\n",
       "        0.18296488, 0.10155246, 0.05473802, 0.11764434, 0.06837872,\n",
       "        0.16446225, 0.14972234, 0.08491932, 0.19591265, 0.05333532,\n",
       "        0.10555231, 0.09426418, 0.20655996, 0.13556717, 0.11424889,\n",
       "        0.08334311, 0.06124732, 0.03092859, 0.07319656, 0.06349976,\n",
       "        0.04870594, 0.09584385, 0.04757628, 0.13021854, 0.108586  ,\n",
       "        0.12552262, 0.05592864, 0.09285037, 0.17670927, 0.25075411,\n",
       "        0.20241197, 0.03884683, 0.19416127, 0.10335207, 0.09948725,\n",
       "        0.09043744, 0.10246453, 0.12708555, 0.17290458, 0.0305937 ,\n",
       "        0.06321861, 0.12851498, 0.04418817, 0.09163282, 0.11333416,\n",
       "        0.12945696, 0.13080202, 0.21625333, 0.17518996, 0.05929613,\n",
       "        0.17921829, 0.09625831, 0.12416424, 0.16742517, 0.17977413,\n",
       "        0.0306249 , 0.10272471, 0.10522134, 0.14180811, 0.03718869,\n",
       "        0.0605611 , 0.09601213, 0.04565627, 0.12854281, 0.11924585,\n",
       "        0.11987359, 0.15320334, 0.55677573, 0.09135566, 0.06114907,\n",
       "        0.12747394, 0.18853395, 0.18621829, 0.1539386 , 0.11081727,\n",
       "        0.17078325, 0.20063543, 0.02972422, 0.07277253, 0.08700724,\n",
       "        0.07244491, 0.07237275, 0.14764207, 0.14870438, 0.0772749 ,\n",
       "        0.15258769, 0.28697256, 0.09002042, 0.08036079, 0.10842292,\n",
       "        0.21277069, 0.06598418, 0.11876616, 0.19029718, 0.15934505,\n",
       "        0.07015204, 0.03930509, 0.15837766, 0.09365986, 0.07687854,\n",
       "        0.15837047, 0.10267899, 0.04498615, 0.12713134, 0.17663549,\n",
       "        0.1336299 , 0.15807019, 0.06816407, 0.15882657, 0.08777013,\n",
       "        0.30247364, 0.14177073, 0.11786242, 0.16763909, 0.13440966,\n",
       "        0.1097808 , 0.09473549, 0.11594887, 0.13203321, 0.24569131,\n",
       "        0.10612633, 0.25539115, 0.06878165, 0.09423436, 0.02859337,\n",
       "        0.16423977, 0.40134504, 0.10326171, 0.13582484, 0.12686385,\n",
       "        0.1320526 , 0.12165135, 0.11018737, 0.17682306, 0.31843759,\n",
       "        0.086167  , 0.06563151, 0.11534389, 0.13266787, 0.06390019,\n",
       "        0.16880262, 0.23247288, 0.228377  , 0.10810731, 0.17327382,\n",
       "        0.14886858, 0.20977839, 0.13624927, 0.08362504, 0.27695911,\n",
       "        0.14377544, 0.22411572, 0.08004852, 0.11289487, 0.18411841,\n",
       "        0.10520864, 0.0664923 , 0.22329737, 0.05600337, 0.09456574,\n",
       "        0.11163965, 0.13488382, 0.03406676, 0.10857793, 0.13330669,\n",
       "        0.11073474, 0.20404886, 0.16056958, 0.115796  , 0.15331527,\n",
       "        0.15286205, 0.10665752, 0.37281479, 0.15966879, 0.0708702 ,\n",
       "        0.03039828, 0.09848728, 0.1057635 , 0.09467898, 0.12586326,\n",
       "        0.14397272, 0.10320901, 0.46527445, 0.06040345, 0.10055141,\n",
       "        0.07755923, 0.09136176, 0.12230882, 0.23868349, 0.10351978,\n",
       "        0.18909625, 0.28865481, 0.11072618, 0.0927044 , 0.17036955,\n",
       "        0.12215038, 0.10703862, 0.20621988, 0.035543  , 0.11272246,\n",
       "        0.18681543, 0.1782504 , 0.06983929, 0.15826497, 0.12148909,\n",
       "        0.18904271, 0.0778843 , 0.07036798, 0.10063639, 0.10252064,\n",
       "        0.05455698, 0.19224778, 0.09445271, 0.13536068, 0.17657849,\n",
       "        0.31450409, 0.06226961, 0.06972014, 0.03993261, 0.04804044,\n",
       "        0.14737382, 0.09743112, 0.20086161, 0.07104115, 0.12358864,\n",
       "        0.12121846, 0.30057482, 0.13083596, 0.15725397, 0.10592659,\n",
       "        0.15895371, 0.22233346, 0.19968897, 0.14954314, 0.13373827,\n",
       "        0.20892085, 0.0708914 , 0.05524479, 0.09124892, 0.10470328,\n",
       "        0.06595324, 0.12477713, 0.10550943, 0.15395139, 0.08217928,\n",
       "        0.10037286, 0.16810596, 0.09786937, 0.09280239, 0.17677978,\n",
       "        0.09627922, 0.12690332, 0.10917258, 0.23980609, 0.2100491 ,\n",
       "        0.11237524, 0.10983073, 0.11382806, 0.07218113, 0.10299265,\n",
       "        0.07819507, 0.16040485, 0.08093878, 0.11038578, 0.17657295,\n",
       "        0.06412677, 0.10522754, 0.22967461, 0.16606212, 0.02848706,\n",
       "        0.15281176, 0.10643988, 0.21996484, 0.17924976, 0.10995619,\n",
       "        0.09515852, 0.18269701, 0.12721025, 0.10680902, 0.11870622,\n",
       "        0.08890952, 0.1834715 , 0.14744452, 0.10988627, 0.10418161,\n",
       "        0.19275256, 0.09739272, 0.15028139, 0.11271279, 0.06325856,\n",
       "        0.1696261 , 0.17331908, 0.18514798, 0.0863288 , 0.08222191]),\n",
       " 'mean_score_time': array([0.00288019, 0.00274897, 0.00298119, 0.00276752, 0.00264654,\n",
       "        0.00272303, 0.00271277, 0.00266991, 0.00286446, 0.00302515,\n",
       "        0.00280609, 0.00279341, 0.00287519, 0.00292993, 0.0028687 ,\n",
       "        0.00282788, 0.00298295, 0.00293474, 0.00278635, 0.00259032,\n",
       "        0.00267053, 0.00297518, 0.00272202, 0.00278673, 0.00266056,\n",
       "        0.00311465, 0.00290709, 0.00279088, 0.00263119, 0.0028264 ,\n",
       "        0.00295238, 0.00276041, 0.0028604 , 0.00262313, 0.00292149,\n",
       "        0.00285263, 0.00290132, 0.00278635, 0.00282822, 0.00277815,\n",
       "        0.00283136, 0.00303659, 0.00274248, 0.0027873 , 0.00283375,\n",
       "        0.00267496, 0.00291681, 0.00281782, 0.00272002, 0.00284038,\n",
       "        0.00284538, 0.00289569, 0.00309095, 0.00294518, 0.00292187,\n",
       "        0.0030858 , 0.00311813, 0.00276823, 0.00267596, 0.00278921,\n",
       "        0.00278859, 0.0024714 , 0.00309458, 0.00306821, 0.00310764,\n",
       "        0.0032351 , 0.00303626, 0.00277524, 0.00284843, 0.00308628,\n",
       "        0.00280099, 0.00319819, 0.00307684, 0.00276694, 0.00280848,\n",
       "        0.00296626, 0.00316634, 0.00300856, 0.00307198, 0.00296102,\n",
       "        0.00271077, 0.00287962, 0.00300622, 0.00302329, 0.00279703,\n",
       "        0.0030828 , 0.00281448, 0.00300536, 0.00277462, 0.00305147,\n",
       "        0.00291824, 0.00283651, 0.00263124, 0.00263467, 0.00277543,\n",
       "        0.00297332, 0.00283141, 0.003057  , 0.00273614, 0.00312815,\n",
       "        0.00271363, 0.00253143, 0.00309434, 0.00307345, 0.00337973,\n",
       "        0.00300498, 0.00271397, 0.00336061, 0.0032269 , 0.003195  ,\n",
       "        0.00327263, 0.00333543, 0.00283694, 0.00392599, 0.00321088,\n",
       "        0.00320063, 0.00371022, 0.00319777, 0.00346465, 0.00327187,\n",
       "        0.00323324, 0.00307837, 0.00323391, 0.002983  , 0.00328846,\n",
       "        0.00315886, 0.00301914, 0.00309305, 0.00314879, 0.00313864,\n",
       "        0.00318155, 0.00320601, 0.0031548 , 0.00311642, 0.00288076,\n",
       "        0.00328746, 0.00331478, 0.00343113, 0.00308714, 0.00323815,\n",
       "        0.00308247, 0.00304356, 0.00300231, 0.00298924, 0.00297174,\n",
       "        0.00298848, 0.00293117, 0.00279016, 0.00310545, 0.00284052,\n",
       "        0.00312901, 0.00315824, 0.00320268, 0.00299273, 0.00302238,\n",
       "        0.00322022, 0.00316439, 0.00339713, 0.00289416, 0.00317898,\n",
       "        0.00323067, 0.0029707 , 0.00311937, 0.00284572, 0.00326142,\n",
       "        0.00309124, 0.00314574, 0.00318303, 0.00341921, 0.00296745,\n",
       "        0.00307946, 0.00319018, 0.00309691, 0.00297217, 0.00327024,\n",
       "        0.00336118, 0.00300174, 0.00290055, 0.00310144, 0.00338583,\n",
       "        0.00294437, 0.00306025, 0.00295653, 0.0031116 , 0.00316224,\n",
       "        0.00302343, 0.00337501, 0.00346522, 0.00302482, 0.00321116,\n",
       "        0.00333962, 0.00329418, 0.00304265, 0.00354228, 0.00325556,\n",
       "        0.00302677, 0.0031857 , 0.00306249, 0.00312829, 0.0031323 ,\n",
       "        0.00326552, 0.00314755, 0.00341525, 0.00306401, 0.00314441,\n",
       "        0.00317764, 0.00306358, 0.00318437, 0.0030818 , 0.00277719,\n",
       "        0.00304923, 0.00318279, 0.00309677, 0.0031682 , 0.00331202,\n",
       "        0.00360603, 0.00328436, 0.00318155, 0.0031074 , 0.0032434 ,\n",
       "        0.00310907, 0.00304823, 0.00318689, 0.00315795, 0.00340433,\n",
       "        0.00318651, 0.00335498, 0.00316   , 0.00321226, 0.00351262,\n",
       "        0.00330329, 0.00323296, 0.00308089, 0.00328445, 0.00323701,\n",
       "        0.00295525, 0.0033812 , 0.00328054, 0.00309072, 0.00317273,\n",
       "        0.00299058, 0.0031755 , 0.00319562, 0.00372849, 0.00332026,\n",
       "        0.00319266, 0.00311961, 0.00323386, 0.00338416, 0.00337262,\n",
       "        0.0034018 , 0.00356984, 0.00345659, 0.00352578, 0.0033391 ,\n",
       "        0.00376506, 0.00330501, 0.00335784, 0.00322928, 0.00343146,\n",
       "        0.0032567 , 0.00323362, 0.00328813, 0.00310578, 0.00334854,\n",
       "        0.00346255, 0.0032455 , 0.00324135, 0.00330729, 0.00314007,\n",
       "        0.00340967, 0.00322285, 0.00359497, 0.0033587 , 0.00352807,\n",
       "        0.00362439, 0.00380507, 0.00349956, 0.00350585, 0.00348763,\n",
       "        0.00322475, 0.00300193, 0.00334606, 0.00325685, 0.00308962,\n",
       "        0.00355496, 0.00374608, 0.00315318, 0.00336285, 0.00353599,\n",
       "        0.00350876, 0.00335627, 0.00344687, 0.00335898, 0.00316653,\n",
       "        0.00343008, 0.0032342 , 0.00313563, 0.00358238, 0.0035655 ,\n",
       "        0.00286527, 0.00341134, 0.00323305, 0.00346255, 0.00330248,\n",
       "        0.00299263, 0.00322604, 0.00329242, 0.00354857, 0.00338559,\n",
       "        0.00350833, 0.00362315, 0.003193  , 0.00331178, 0.00346122,\n",
       "        0.00312748, 0.00343461, 0.0035346 , 0.00320621, 0.00329719,\n",
       "        0.00328202, 0.00333185, 0.00355835, 0.00316658, 0.00318236,\n",
       "        0.00329652, 0.00330944, 0.00369525, 0.00348678, 0.00347204,\n",
       "        0.00347652, 0.00323453, 0.00320339, 0.00324526, 0.00350699,\n",
       "        0.00340486, 0.003302  , 0.00370669, 0.00345759, 0.00325298,\n",
       "        0.00278645, 0.00313587, 0.00358338, 0.00340095, 0.00353665,\n",
       "        0.00348501, 0.00318842, 0.003232  , 0.00358176, 0.00338626,\n",
       "        0.00347652, 0.00342984, 0.0031486 , 0.00358281, 0.00329208,\n",
       "        0.00340071, 0.0034174 , 0.00345526, 0.00335078, 0.00341935,\n",
       "        0.00573611, 0.00321498, 0.00371633, 0.0037581 , 0.00336452,\n",
       "        0.00358319, 0.0034224 , 0.00339565, 0.00317364, 0.00346684,\n",
       "        0.00340447, 0.00368075, 0.0043334 , 0.00337052, 0.00345201,\n",
       "        0.00388017, 0.00346346, 0.00357728, 0.00334144, 0.00354524,\n",
       "        0.00344048, 0.00328965, 0.00371661, 0.00327253, 0.00341372,\n",
       "        0.00332012, 0.00368085, 0.00347905, 0.0036994 , 0.00346575,\n",
       "        0.00368032, 0.00326872, 0.00350876, 0.0038022 , 0.00360622,\n",
       "        0.00387578, 0.00345449, 0.0036541 , 0.00343103, 0.00342317,\n",
       "        0.0035234 , 0.00336061, 0.00322976, 0.00309114, 0.00365319,\n",
       "        0.00390782, 0.00342922, 0.00360126, 0.00343957, 0.00346861,\n",
       "        0.00346713, 0.00352745, 0.00343423, 0.00374064, 0.00379791,\n",
       "        0.00358305, 0.00372629, 0.00363111, 0.00332808, 0.00361261,\n",
       "        0.00313392, 0.00343451, 0.00344315, 0.00333939, 0.0031981 ,\n",
       "        0.00365157, 0.00346556, 0.00366964, 0.00351062, 0.0033926 ,\n",
       "        0.00354996, 0.00399623, 0.00345812, 0.00377231, 0.00400224,\n",
       "        0.00366173, 0.0038456 , 0.00397215, 0.00356717, 0.00308261,\n",
       "        0.00327659, 0.00349312, 0.00309682, 0.00348387, 0.00307488,\n",
       "        0.00344753, 0.00316625, 0.00322385, 0.00366445, 0.0034337 ,\n",
       "        0.0037755 , 0.00360804, 0.00346766, 0.00349345, 0.00323687,\n",
       "        0.00360398, 0.00379581, 0.00359044, 0.00359979, 0.00365024,\n",
       "        0.00334468, 0.00318503, 0.00422721, 0.00361514, 0.00381212,\n",
       "        0.00329595, 0.00360889, 0.00369143, 0.00385399, 0.00361843,\n",
       "        0.00386386, 0.00368299, 0.00380445, 0.00364628, 0.00364933,\n",
       "        0.00382719, 0.00397935, 0.00349016, 0.00340385, 0.00398564,\n",
       "        0.00346437, 0.00355287, 0.00388551, 0.00336084, 0.00370893,\n",
       "        0.00389743, 0.00344996, 0.00372691, 0.00362501, 0.0040082 ,\n",
       "        0.00377288, 0.00479803, 0.00369439, 0.00357251, 0.00356345,\n",
       "        0.00371542, 0.00358109, 0.00368519, 0.00352864, 0.00377102,\n",
       "        0.00330439, 0.00337319, 0.00334358, 0.00337496, 0.00327387,\n",
       "        0.00347524, 0.00407476, 0.00363479, 0.00340958, 0.00321984,\n",
       "        0.00362554, 0.00367908, 0.00361743, 0.00364771, 0.00365553,\n",
       "        0.00385237, 0.00368052, 0.00381265, 0.00364785, 0.00329733,\n",
       "        0.00372519, 0.0035706 , 0.00357566, 0.00338354, 0.00321202,\n",
       "        0.00333982, 0.00364389, 0.0039638 , 0.00343795, 0.00390353,\n",
       "        0.00378332, 0.00374937, 0.00326319, 0.00361218, 0.00349112,\n",
       "        0.00385761, 0.00407171, 0.0034224 , 0.00407476, 0.0035532 ,\n",
       "        0.00322518, 0.00327663, 0.00340219, 0.0035078 , 0.00331755,\n",
       "        0.00375381, 0.0036335 , 0.00372229, 0.00378785, 0.00364099,\n",
       "        0.00377841, 0.00421672, 0.00369282, 0.00397587, 0.00399709,\n",
       "        0.00356064, 0.00369315, 0.00390835, 0.0039783 , 0.00341649,\n",
       "        0.00324507, 0.00360084, 0.00364442, 0.00364599, 0.00352812,\n",
       "        0.00382628, 0.00389023, 0.00325961, 0.00351648, 0.00378437,\n",
       "        0.00402093, 0.00412145, 0.00361834, 0.00371294, 0.00356278,\n",
       "        0.00374131, 0.00364981, 0.00357156, 0.00350523, 0.00369854,\n",
       "        0.00356784, 0.00341554, 0.0035768 , 0.00383749, 0.00321546,\n",
       "        0.0036068 , 0.00345855, 0.0036613 , 0.00394707, 0.00354204,\n",
       "        0.0038085 , 0.00408335, 0.00350533, 0.00390201, 0.00357332,\n",
       "        0.00381322, 0.00362439, 0.0036468 , 0.00343442, 0.00333657,\n",
       "        0.00332937, 0.0037858 , 0.00361276, 0.00376997, 0.00385666,\n",
       "        0.00402317, 0.00355086, 0.00374975, 0.00376368, 0.00336041,\n",
       "        0.00385265, 0.00370126, 0.00389113, 0.00387931, 0.00362606,\n",
       "        0.00378861, 0.00386505, 0.0038342 , 0.00372028, 0.00366402,\n",
       "        0.00345302, 0.0034646 , 0.00377798, 0.00373845, 0.00340343,\n",
       "        0.00354342, 0.00356212, 0.00374784, 0.00339756, 0.00325789,\n",
       "        0.00413203, 0.00372586, 0.00382099, 0.00341477, 0.00382051,\n",
       "        0.00394182, 0.00512118, 0.00416346, 0.00385933, 0.00376554,\n",
       "        0.00322337, 0.00412135, 0.00333233, 0.00327201, 0.00349979,\n",
       "        0.00334635, 0.00343318, 0.00368047, 0.00352316, 0.00364585,\n",
       "        0.00373468, 0.00375009, 0.00383425, 0.00359178, 0.00416274,\n",
       "        0.00392318, 0.00378714, 0.00376987, 0.00361848, 0.00360765,\n",
       "        0.00329657, 0.00374041, 0.00330358, 0.00325751, 0.00362206,\n",
       "        0.00334735, 0.00392323, 0.00381269, 0.00404015, 0.0038538 ,\n",
       "        0.00362644, 0.00378904, 0.00369625, 0.00391922, 0.00379944,\n",
       "        0.00415359, 0.00377288, 0.00395017, 0.00377297, 0.00364075,\n",
       "        0.00290351, 0.00376897, 0.00338564, 0.00341482, 0.00378251,\n",
       "        0.00382051, 0.00361667, 0.00359097, 0.00356441, 0.0035419 ,\n",
       "        0.0039125 , 0.00393271, 0.00377192, 0.00372443, 0.00354433,\n",
       "        0.00375652, 0.0037827 , 0.00400305, 0.00379343, 0.0033957 ,\n",
       "        0.00334744, 0.0033576 , 0.00390501, 0.00340643, 0.00319104,\n",
       "        0.00410566, 0.00369658, 0.00397344, 0.00350223, 0.00347958,\n",
       "        0.00367389, 0.00377798, 0.00376463, 0.00376449, 0.00363636,\n",
       "        0.00374861, 0.0035706 , 0.00358396, 0.00382037, 0.00327325,\n",
       "        0.00319896, 0.00359616, 0.00359969, 0.00360994, 0.00332217,\n",
       "        0.00391531, 0.00370669, 0.00384712, 0.00369592, 0.00335588,\n",
       "        0.00336356, 0.00402226, 0.00387821, 0.0034739 , 0.00348983,\n",
       "        0.00379043, 0.00409331, 0.00369763, 0.00358706, 0.00389032,\n",
       "        0.00335732, 0.00376282, 0.00335279, 0.0034934 , 0.00361905,\n",
       "        0.00380592, 0.00365586, 0.0033011 , 0.00346384, 0.00379725,\n",
       "        0.0035111 , 0.00379195, 0.00418425, 0.00377688, 0.00328083,\n",
       "        0.00369134, 0.00392218, 0.00374599, 0.00367537, 0.00401917,\n",
       "        0.00347381, 0.00355921, 0.00338464, 0.0035008 , 0.00364051,\n",
       "        0.00335836, 0.00358982, 0.00386233, 0.00339489, 0.00364752,\n",
       "        0.00389838, 0.00396566, 0.00354676, 0.00379014, 0.00358877,\n",
       "        0.00413017, 0.00381322, 0.00391254, 0.00357933, 0.00355949,\n",
       "        0.00321202, 0.00374908, 0.00332427, 0.00357523, 0.00358529,\n",
       "        0.00369787, 0.00361538, 0.00382442, 0.00374126, 0.00429621,\n",
       "        0.0041142 , 0.00380511, 0.00387583, 0.00360355, 0.00400662,\n",
       "        0.00385027, 0.00406275, 0.00386076, 0.00347037, 0.00386119,\n",
       "        0.00368791, 0.00356297, 0.003549  , 0.00338316, 0.00365992,\n",
       "        0.00380363, 0.00378428, 0.00355511, 0.00405998, 0.0034018 ,\n",
       "        0.00344977, 0.00365162, 0.00358505, 0.00381432, 0.00350986,\n",
       "        0.00389128, 0.00389104, 0.00385342, 0.0040453 , 0.0034668 ,\n",
       "        0.00362954, 0.00353951, 0.00371661, 0.00324054, 0.00371175,\n",
       "        0.00399127, 0.00368743, 0.00429659, 0.00368347, 0.00396547,\n",
       "        0.0036304 , 0.00351281, 0.00399718, 0.00338235, 0.00396905,\n",
       "        0.00364237, 0.0039875 , 0.00373435, 0.00355072, 0.00363126,\n",
       "        0.00349431, 0.00354667, 0.00345731, 0.00343981, 0.00337501,\n",
       "        0.00367746, 0.0035996 , 0.00379004, 0.00352631, 0.00415859,\n",
       "        0.00393543, 0.00355597, 0.00382948, 0.00361772, 0.00325747,\n",
       "        0.00382953, 0.00347366, 0.0037262 , 0.00356789, 0.00362978,\n",
       "        0.00349774, 0.00388732, 0.00335851, 0.00359468, 0.00324922,\n",
       "        0.00358047, 0.00332694, 0.00342622, 0.00366058, 0.00345531,\n",
       "        0.00368161, 0.00362296, 0.00395036, 0.00379281, 0.00384154,\n",
       "        0.00462618, 0.00358233, 0.00383663, 0.00362582, 0.00395718,\n",
       "        0.00366197, 0.00349531, 0.00336118, 0.00298223, 0.00352111,\n",
       "        0.00348697, 0.00349503, 0.00391254, 0.00355964, 0.00353599,\n",
       "        0.00423803, 0.00367198, 0.00371542, 0.00343661, 0.00384922,\n",
       "        0.00384459, 0.00387416, 0.00404463, 0.00394359, 0.00395384,\n",
       "        0.0040648 , 0.00340023, 0.00358424, 0.0034791 , 0.00353823,\n",
       "        0.00348482, 0.00386286, 0.0035295 , 0.00405307, 0.0035152 ,\n",
       "        0.0040926 , 0.00378146, 0.00371485, 0.00362177, 0.00367508,\n",
       "        0.00376945, 0.00382428, 0.00405102, 0.003969  , 0.00341554,\n",
       "        0.00326734, 0.00337639, 0.00354261, 0.00339327, 0.00356655,\n",
       "        0.00378742, 0.0037446 , 0.00477858, 0.0038413 , 0.00376205,\n",
       "        0.00402832, 0.00356836, 0.00374556, 0.00353699, 0.00352979,\n",
       "        0.00358481, 0.00406704, 0.00337176, 0.00359888, 0.0037509 ,\n",
       "        0.0038722 , 0.00315585, 0.0035542 , 0.00361314, 0.00363226,\n",
       "        0.00367522, 0.00366788, 0.00351992, 0.00376015, 0.00361786,\n",
       "        0.00362849, 0.00410314, 0.00385919, 0.00395923, 0.00390968,\n",
       "        0.00370545, 0.00365963, 0.00365648, 0.00371575, 0.00341663]),\n",
       " 'std_score_time': array([2.96553942e-04, 1.99120483e-04, 3.65932086e-04, 2.25973346e-04,\n",
       "        1.42137119e-04, 2.90581035e-04, 3.39981195e-04, 1.02563073e-04,\n",
       "        1.74991675e-04, 2.92296923e-04, 6.27552549e-04, 1.71952835e-04,\n",
       "        1.69786881e-04, 1.32603776e-04, 3.43982407e-05, 7.15409898e-05,\n",
       "        2.93774207e-04, 2.23221699e-04, 1.45214758e-04, 3.15328292e-04,\n",
       "        1.86961641e-04, 2.35500280e-04, 1.69691532e-04, 1.13395004e-04,\n",
       "        9.75264771e-05, 5.48879927e-04, 3.72715957e-04, 1.39572395e-04,\n",
       "        3.33676763e-04, 1.29140268e-04, 3.17916756e-04, 3.40261153e-04,\n",
       "        2.00783982e-04, 2.85514124e-04, 4.87908989e-04, 1.79838450e-04,\n",
       "        3.05402493e-04, 3.58055539e-04, 5.39349213e-04, 2.47741047e-04,\n",
       "        3.14977224e-04, 2.41439026e-04, 3.37606143e-04, 2.16458620e-04,\n",
       "        1.05341261e-04, 2.60020120e-04, 2.82578946e-04, 2.78733789e-04,\n",
       "        3.74925647e-04, 3.66383831e-04, 3.77794882e-04, 4.98867678e-04,\n",
       "        2.39440283e-04, 3.35441334e-04, 1.96292053e-04, 6.11500170e-04,\n",
       "        3.77758197e-04, 2.95609903e-04, 1.51807114e-04, 1.87828293e-04,\n",
       "        1.48270550e-04, 1.32849981e-04, 1.20680938e-04, 3.29413452e-04,\n",
       "        3.71252213e-04, 4.43959543e-04, 2.58324567e-04, 3.41279133e-04,\n",
       "        3.23082719e-04, 3.36273847e-04, 2.07046867e-04, 4.10128092e-04,\n",
       "        3.18066126e-04, 1.34109347e-04, 1.61363949e-04, 1.97960748e-04,\n",
       "        2.47284762e-04, 1.28580216e-04, 2.68681868e-04, 4.02653861e-04,\n",
       "        1.45738505e-04, 4.38919733e-04, 1.28868010e-04, 2.86416898e-04,\n",
       "        4.24600546e-04, 5.96207948e-04, 3.45995927e-04, 2.61541882e-04,\n",
       "        3.87167654e-04, 1.06309601e-04, 2.71440573e-04, 2.03358744e-04,\n",
       "        2.52084527e-04, 2.65372821e-04, 4.17354321e-04, 3.69769338e-04,\n",
       "        2.95668931e-04, 1.88136160e-04, 3.46879005e-04, 3.13651674e-04,\n",
       "        2.45164303e-04, 3.42124284e-04, 3.23092649e-04, 2.47313466e-04,\n",
       "        1.90531599e-04, 1.75310554e-04, 3.74528874e-04, 2.33638802e-04,\n",
       "        3.21596337e-04, 1.33855028e-04, 3.78761501e-04, 2.79819299e-04,\n",
       "        3.40668298e-04, 1.02467639e-03, 7.12019593e-05, 2.27461701e-04,\n",
       "        3.38402681e-04, 2.72062599e-04, 6.02125173e-04, 2.03208305e-04,\n",
       "        3.22078083e-04, 4.70610978e-05, 3.62037713e-04, 4.35119396e-04,\n",
       "        3.09703088e-04, 3.53020084e-04, 3.17108591e-04, 2.55893514e-04,\n",
       "        1.71909418e-04, 2.67844470e-04, 2.62728257e-04, 1.57720765e-04,\n",
       "        3.15905046e-04, 4.27879280e-04, 1.70088382e-04, 9.75625140e-05,\n",
       "        2.10232004e-04, 4.42486626e-04, 1.29806303e-04, 3.13952692e-04,\n",
       "        2.18999809e-04, 1.50261565e-04, 2.68043421e-04, 2.34057519e-04,\n",
       "        2.15255543e-04, 1.30947298e-04, 3.24497633e-04, 2.32954664e-04,\n",
       "        3.69020158e-04, 2.94364868e-04, 1.82422404e-04, 3.46450388e-04,\n",
       "        1.17904966e-04, 3.01055723e-04, 2.02190392e-04, 1.49374337e-04,\n",
       "        4.72284605e-04, 3.56510598e-04, 3.12074310e-04, 3.31816297e-04,\n",
       "        1.70737309e-04, 4.58463392e-04, 2.08349553e-04, 2.59734623e-04,\n",
       "        1.41823957e-04, 3.40399703e-04, 2.35980124e-04, 1.80665576e-04,\n",
       "        4.27879546e-04, 3.42651134e-04, 3.48442416e-04, 2.77959607e-04,\n",
       "        4.78853388e-04, 2.20818979e-04, 5.27089410e-04, 4.48067627e-04,\n",
       "        1.65310881e-04, 2.84838524e-04, 2.05235532e-04, 1.76030003e-04,\n",
       "        1.83393741e-04, 2.94069910e-04, 1.55886564e-04, 3.52615581e-04,\n",
       "        1.50765443e-04, 3.35283171e-04, 1.72963791e-04, 4.88410370e-04,\n",
       "        3.75949798e-04, 1.55638918e-04, 3.30015087e-04, 2.66596726e-04,\n",
       "        1.43808854e-04, 4.62697619e-04, 1.30168195e-04, 1.36601521e-04,\n",
       "        1.48720258e-04, 4.02400050e-04, 3.68086382e-04, 2.07891672e-04,\n",
       "        2.40769271e-04, 1.95984040e-04, 3.07715458e-04, 2.61670081e-04,\n",
       "        5.41038751e-04, 3.99023876e-04, 4.63883402e-05, 1.00305663e-04,\n",
       "        1.02263952e-04, 3.84959489e-04, 3.20947561e-04, 7.18333616e-05,\n",
       "        1.75969477e-04, 3.15220257e-04, 4.51129398e-04, 3.00995554e-04,\n",
       "        2.01401735e-04, 9.12277593e-05, 3.94149240e-04, 1.62653230e-04,\n",
       "        2.34568048e-04, 4.46382553e-04, 1.20935474e-04, 5.55618293e-04,\n",
       "        3.97672882e-04, 2.38454597e-04, 3.03096220e-04, 3.17244029e-04,\n",
       "        1.76637656e-04, 6.21536704e-04, 5.02470399e-04, 1.86292961e-04,\n",
       "        3.45837647e-04, 3.60890636e-04, 1.19421484e-04, 2.78418165e-04,\n",
       "        2.77230860e-04, 3.24353265e-04, 1.39407941e-04, 1.78254052e-04,\n",
       "        2.51733288e-04, 1.54893415e-04, 1.07290755e-04, 4.71511705e-04,\n",
       "        1.25891819e-04, 7.39483809e-04, 2.29220864e-04, 1.63336627e-04,\n",
       "        1.94247528e-04, 3.21822319e-04, 4.84334654e-05, 4.46873687e-04,\n",
       "        1.61495812e-04, 4.70597484e-04, 2.02366196e-04, 5.68574325e-04,\n",
       "        2.03588661e-04, 1.28489380e-04, 3.27994255e-04, 9.82926215e-05,\n",
       "        3.32417941e-04, 5.61814842e-04, 5.16923186e-04, 1.91857349e-04,\n",
       "        1.98276459e-04, 5.69588383e-04, 2.39554873e-04, 1.44184660e-04,\n",
       "        2.91119000e-04, 1.52418266e-04, 4.63488524e-04, 2.28334537e-04,\n",
       "        5.45877851e-04, 3.22503150e-04, 3.26116020e-04, 2.05699802e-04,\n",
       "        4.41338965e-04, 2.17295399e-04, 2.88137019e-04, 3.28845048e-04,\n",
       "        1.66849717e-04, 2.66213663e-04, 3.12588183e-04, 2.62336737e-04,\n",
       "        1.73897510e-04, 3.05544943e-04, 1.99428920e-04, 1.89633444e-04,\n",
       "        3.42208604e-04, 1.78220846e-04, 4.34226500e-04, 2.08658045e-04,\n",
       "        5.50903641e-04, 4.41569556e-04, 1.65947730e-04, 3.75697071e-04,\n",
       "        2.67043514e-04, 2.73856111e-04, 2.52529087e-04, 7.12269899e-04,\n",
       "        3.82043568e-04, 3.86665496e-04, 7.85645619e-05, 2.70969612e-04,\n",
       "        3.99949354e-04, 3.42480600e-04, 1.35511969e-04, 4.37594164e-05,\n",
       "        3.26048348e-04, 3.56262953e-04, 3.90985389e-04, 4.08234090e-04,\n",
       "        2.42271411e-04, 1.40371708e-04, 5.40227830e-04, 3.22995095e-04,\n",
       "        8.34276178e-05, 2.00668373e-04, 4.10833897e-04, 1.32691932e-04,\n",
       "        2.32702806e-04, 1.55279324e-04, 3.69708716e-04, 2.60395860e-04,\n",
       "        1.47853770e-04, 1.81575846e-04, 4.83124420e-04, 2.23236040e-04,\n",
       "        2.39484768e-04, 2.78849478e-04, 4.10535820e-04, 1.97080587e-04,\n",
       "        3.16984185e-04, 2.07745949e-04, 3.05698089e-04, 4.16874923e-04,\n",
       "        2.24415879e-04, 6.40882463e-04, 3.18956635e-04, 3.49148848e-04,\n",
       "        1.83027618e-04, 3.76748173e-04, 1.85410537e-04, 1.06580263e-04,\n",
       "        1.52240178e-04, 3.64900594e-04, 4.35935796e-04, 1.23447513e-04,\n",
       "        5.13813379e-04, 3.06471406e-04, 3.03306765e-04, 3.98290504e-04,\n",
       "        2.24041643e-04, 1.49574749e-04, 7.92725104e-05, 1.42948082e-04,\n",
       "        3.36795509e-04, 4.29348123e-04, 1.19745707e-04, 4.73582375e-04,\n",
       "        4.08494607e-03, 2.79474930e-04, 3.16161649e-04, 8.22790725e-04,\n",
       "        4.72092421e-04, 3.55961286e-04, 2.81431884e-04, 1.74960163e-04,\n",
       "        1.87435425e-04, 2.42030536e-04, 2.34280320e-04, 4.17341747e-04,\n",
       "        1.25456469e-03, 2.38536567e-04, 4.00580461e-04, 1.99047480e-04,\n",
       "        2.50326402e-04, 3.79040835e-04, 6.82926704e-05, 3.24428046e-04,\n",
       "        3.70949294e-04, 2.05451829e-04, 3.62183690e-04, 3.17456858e-04,\n",
       "        1.55383620e-04, 2.29278687e-04, 3.46199034e-04, 3.06297097e-04,\n",
       "        3.99268557e-04, 2.19862814e-04, 4.65771026e-04, 1.80756885e-04,\n",
       "        2.30667253e-04, 1.84284675e-04, 4.48862006e-04, 3.92831764e-04,\n",
       "        2.43125581e-04, 3.85848620e-04, 1.93712089e-04, 9.97396307e-05,\n",
       "        2.20879259e-04, 2.54283339e-04, 3.94046543e-04, 4.38751066e-04,\n",
       "        2.98103383e-04, 3.78552986e-04, 3.54121805e-04, 2.93486023e-04,\n",
       "        1.90184670e-04, 3.45036073e-04, 4.17502980e-04, 4.34497865e-04,\n",
       "        3.30305220e-04, 2.50008801e-04, 4.59992541e-04, 2.24298168e-04,\n",
       "        3.53652135e-04, 3.68696584e-04, 1.72234347e-04, 1.48490996e-04,\n",
       "        1.90618992e-04, 2.33389826e-04, 2.76474634e-04, 3.18927663e-04,\n",
       "        3.80473905e-04, 5.81441130e-04, 5.15895289e-04, 4.73925561e-04,\n",
       "        5.47015164e-04, 2.69934841e-04, 3.16059906e-04, 4.35975804e-04,\n",
       "        1.50715441e-04, 3.36506229e-04, 2.13589549e-04, 2.91950169e-04,\n",
       "        3.51590264e-04, 3.95077833e-04, 4.04978318e-04, 3.01109342e-04,\n",
       "        1.49474417e-04, 4.61838620e-04, 4.32241430e-04, 4.85423731e-04,\n",
       "        3.95876601e-04, 3.27522953e-04, 3.33209712e-04, 3.66918289e-04,\n",
       "        4.00575268e-04, 1.63591425e-04, 2.70655207e-04, 4.08397829e-04,\n",
       "        2.86859210e-04, 4.23741673e-04, 1.34155709e-04, 6.27281774e-05,\n",
       "        1.31966904e-04, 3.85025694e-04, 3.02359490e-04, 5.53246794e-04,\n",
       "        4.25736189e-04, 4.57519252e-04, 1.01933044e-03, 3.36612451e-04,\n",
       "        8.39818815e-04, 3.69262628e-04, 1.80377507e-04, 2.37623223e-04,\n",
       "        3.30518169e-04, 7.09993396e-05, 3.20278830e-04, 4.57512011e-04,\n",
       "        3.33103743e-04, 3.46349336e-04, 1.80536216e-04, 3.51479823e-04,\n",
       "        2.23859054e-04, 2.89919557e-04, 4.64945900e-04, 7.91892687e-04,\n",
       "        4.64218413e-04, 3.61834876e-04, 3.64351738e-04, 4.33174858e-04,\n",
       "        1.15175038e-04, 4.06923678e-04, 5.30718866e-04, 2.29942744e-04,\n",
       "        1.95087456e-04, 2.64700660e-04, 4.58171003e-04, 1.47634302e-03,\n",
       "        2.59809949e-04, 3.81512085e-04, 2.25833340e-04, 2.14203860e-04,\n",
       "        3.34479851e-04, 3.70655354e-04, 2.87845009e-04, 1.55999257e-04,\n",
       "        3.98680140e-04, 2.07865968e-04, 4.10998115e-04, 1.14505531e-04,\n",
       "        2.80308009e-04, 1.26647393e-04, 4.64684120e-04, 4.48221820e-04,\n",
       "        3.31553569e-04, 5.02813603e-04, 3.22779050e-04, 3.66063212e-04,\n",
       "        4.71916670e-04, 3.27684965e-04, 4.48201884e-04, 3.88178601e-04,\n",
       "        1.02376327e-04, 6.77552457e-04, 4.68360526e-04, 1.25538555e-04,\n",
       "        3.34223039e-04, 5.58519150e-04, 3.89095165e-04, 2.33504367e-04,\n",
       "        4.48890646e-04, 1.75123106e-04, 1.78509173e-04, 5.33435570e-04,\n",
       "        2.14524655e-04, 5.02142982e-04, 2.13380052e-04, 2.87793288e-04,\n",
       "        3.61902767e-04, 1.85699614e-04, 2.25133169e-04, 5.67652256e-04,\n",
       "        5.49480667e-04, 3.62076543e-04, 3.96291109e-04, 1.75088696e-04,\n",
       "        3.38615546e-04, 3.28217321e-04, 2.06328623e-04, 3.84031024e-04,\n",
       "        5.29376903e-04, 5.94185128e-04, 3.10696605e-04, 7.12954740e-04,\n",
       "        5.94230969e-04, 3.99285300e-04, 2.89981389e-04, 7.00080932e-04,\n",
       "        4.65742292e-04, 4.41702241e-04, 5.63059061e-04, 5.28884200e-04,\n",
       "        1.72154288e-04, 3.72603234e-04, 2.23374488e-04, 2.47101248e-04,\n",
       "        3.87777757e-04, 5.14617277e-04, 2.06599041e-04, 2.45793325e-04,\n",
       "        1.38540767e-04, 4.74173487e-04, 4.37319402e-04, 4.02352651e-04,\n",
       "        2.89591627e-04, 5.40343557e-04, 5.62901630e-04, 4.91242505e-04,\n",
       "        2.16237087e-04, 6.06633113e-04, 5.78544101e-04, 3.83517177e-04,\n",
       "        1.96691345e-04, 3.54750227e-04, 2.92890524e-04, 5.16614791e-04,\n",
       "        3.64885178e-04, 2.02663276e-04, 2.59395706e-04, 2.11691082e-04,\n",
       "        1.68369749e-04, 1.94843388e-04, 4.95483288e-04, 3.97639381e-04,\n",
       "        3.13801147e-04, 4.29720123e-04, 1.60766268e-04, 3.23243172e-04,\n",
       "        1.88346609e-04, 3.55791284e-04, 4.58286692e-04, 5.25677974e-04,\n",
       "        2.61942558e-04, 2.20586199e-04, 3.40649976e-04, 2.32702034e-04,\n",
       "        3.95492476e-04, 2.83187819e-04, 3.97215513e-04, 2.73765979e-04,\n",
       "        4.85903498e-04, 5.33870353e-04, 4.80350441e-04, 4.54370534e-04,\n",
       "        2.68290299e-05, 4.12728594e-04, 4.14891471e-04, 3.96703653e-04,\n",
       "        6.10756740e-05, 2.31810597e-04, 4.15138697e-04, 5.51532305e-04,\n",
       "        2.71389195e-04, 6.66347921e-04, 1.31081399e-04, 2.32381930e-04,\n",
       "        2.04366476e-04, 2.63780722e-04, 3.63338275e-04, 2.95508317e-04,\n",
       "        4.27276427e-05, 2.17262227e-04, 3.59672917e-04, 4.37374022e-04,\n",
       "        3.60229711e-04, 1.76126800e-04, 4.18002508e-04, 6.12796539e-04,\n",
       "        3.37580523e-04, 1.96428470e-04, 4.51662125e-04, 4.51711401e-04,\n",
       "        1.55459776e-03, 4.43010653e-04, 5.11832559e-04, 5.79479707e-04,\n",
       "        2.13783142e-04, 3.76381720e-04, 3.58757061e-04, 2.63745283e-04,\n",
       "        1.75272484e-04, 3.82580172e-04, 4.08259315e-04, 3.94055152e-04,\n",
       "        3.12896292e-04, 7.17195573e-04, 3.32575018e-04, 5.61438770e-04,\n",
       "        3.62249677e-04, 5.26775866e-04, 3.86928730e-04, 4.20648839e-04,\n",
       "        5.70953085e-04, 3.65336095e-04, 2.13643854e-04, 3.23142329e-04,\n",
       "        1.66099597e-04, 3.65708598e-04, 1.57093667e-04, 2.08560037e-04,\n",
       "        4.01392728e-04, 3.26857017e-04, 3.94671620e-04, 3.81447839e-04,\n",
       "        3.70610883e-04, 3.76663659e-04, 3.43625399e-04, 3.43193474e-04,\n",
       "        3.76885538e-04, 2.44193407e-04, 4.31897714e-04, 4.31207525e-04,\n",
       "        2.39795504e-04, 4.29619785e-04, 3.27379252e-04, 2.65520579e-04,\n",
       "        4.58037980e-04, 4.72154571e-04, 1.76799528e-04, 4.50856288e-04,\n",
       "        1.54158437e-04, 3.42636436e-04, 1.76910544e-04, 2.60550499e-04,\n",
       "        3.16683040e-04, 4.78007202e-04, 2.21851756e-04, 4.82793490e-04,\n",
       "        3.02528605e-04, 4.39948077e-04, 5.84166506e-04, 7.63346146e-05,\n",
       "        4.93829547e-04, 3.54056037e-04, 4.42028287e-04, 2.56225136e-04,\n",
       "        3.02788607e-04, 2.73492005e-04, 2.13155085e-04, 2.49208374e-04,\n",
       "        2.93105653e-04, 6.19130581e-04, 3.47944013e-04, 3.54705614e-04,\n",
       "        3.69701188e-04, 4.54979706e-04, 2.90150909e-04, 3.13817290e-04,\n",
       "        1.83328726e-04, 3.78285848e-04, 3.44188083e-04, 4.70969804e-04,\n",
       "        2.45747891e-04, 6.64573379e-04, 2.85448336e-04, 1.10548399e-04,\n",
       "        3.87809248e-04, 5.08796947e-04, 2.47881466e-04, 2.79299216e-04,\n",
       "        2.32678915e-04, 4.10240524e-04, 3.13625554e-04, 2.36149769e-04,\n",
       "        2.34284824e-04, 2.81607269e-04, 3.78912112e-04, 6.66426981e-04,\n",
       "        3.72037127e-04, 1.63877603e-04, 1.18150453e-04, 1.11627100e-04,\n",
       "        4.45570521e-04, 4.37278472e-04, 2.54449145e-04, 3.54426211e-04,\n",
       "        5.53882303e-04, 2.99932642e-04, 5.63412626e-04, 7.42396462e-04,\n",
       "        4.19093792e-04, 3.76294357e-04, 1.70069399e-04, 3.36581999e-04,\n",
       "        2.80838652e-04, 3.90542598e-04, 1.32631431e-04, 1.59771304e-04,\n",
       "        2.92307440e-04, 2.53713066e-04, 3.29055003e-04, 1.73394252e-04,\n",
       "        4.54288104e-04, 3.34084289e-04, 4.16723611e-04, 4.69762984e-04,\n",
       "        2.00806901e-04, 1.26402381e-04, 4.59310689e-04, 1.43516705e-04,\n",
       "        2.81531119e-04, 4.56722909e-04, 6.20490999e-04, 2.32963106e-04,\n",
       "        1.84007400e-04, 2.21803786e-04, 4.38018964e-04, 7.41176932e-04,\n",
       "        2.96558228e-04, 1.50069373e-04, 1.91792690e-04, 4.61360845e-04,\n",
       "        3.14685553e-04, 1.99750757e-04, 4.09302235e-04, 1.74458849e-04,\n",
       "        3.98639811e-04, 2.19045798e-04, 4.70622211e-04, 3.50585051e-04,\n",
       "        3.55757444e-04, 2.42303243e-04, 3.93851646e-04, 5.42844527e-04,\n",
       "        4.91450338e-04, 7.73814987e-04, 9.10495867e-04, 4.43495841e-04,\n",
       "        5.45369075e-04, 4.63113925e-04, 4.72435953e-04, 2.96267388e-04,\n",
       "        6.31117291e-04, 2.70269745e-04, 1.96176185e-04, 6.38672902e-04,\n",
       "        1.38755303e-04, 2.51376158e-04, 3.39162763e-04, 4.80034362e-04,\n",
       "        1.48878717e-04, 3.78651719e-04, 3.89041078e-04, 2.17483928e-04,\n",
       "        4.44313154e-04, 2.93561628e-04, 1.99444664e-04, 2.57072902e-04,\n",
       "        7.63729402e-05, 2.73723244e-04, 2.10178904e-04, 1.64007127e-04,\n",
       "        1.18869144e-04, 5.39342890e-04, 4.13331580e-04, 1.80320837e-04,\n",
       "        5.48111435e-04, 5.95398253e-04, 3.48164818e-04, 4.25504387e-04,\n",
       "        4.75958382e-04, 5.75260221e-04, 2.48142337e-04, 6.05792989e-04,\n",
       "        3.15300746e-04, 1.81150970e-04, 5.35961678e-04, 4.59987692e-04,\n",
       "        9.20184586e-04, 1.84897050e-04, 5.47756474e-04, 4.41553124e-04,\n",
       "        2.16599173e-04, 4.46244966e-04, 2.64001331e-04, 3.78789024e-04,\n",
       "        2.71903502e-04, 4.38436466e-04, 4.70841793e-04, 1.39440231e-04,\n",
       "        3.18428248e-04, 3.41194883e-04, 2.78982967e-04, 3.92362569e-04,\n",
       "        2.32621409e-04, 3.55536532e-04, 2.24930441e-04, 2.91281176e-04,\n",
       "        4.48782328e-04, 3.49690259e-04, 3.22053374e-04, 2.48045236e-04,\n",
       "        1.47361091e-04, 3.39927755e-04, 2.04372061e-04, 3.02652402e-04,\n",
       "        4.33626955e-04, 5.93632418e-04, 2.76079062e-04, 3.24738267e-04,\n",
       "        2.51757981e-04, 3.82920000e-04, 3.14713839e-04, 7.83429955e-05,\n",
       "        4.08790763e-04, 5.78931401e-04, 3.47558372e-04, 3.84235648e-04,\n",
       "        5.05957054e-04, 3.45659759e-04, 3.26575901e-04, 6.21578363e-04,\n",
       "        2.59918883e-04, 4.23284212e-04, 3.73637816e-04, 4.09847948e-04,\n",
       "        3.93242970e-04, 5.23285095e-04, 3.55624417e-04, 4.08568886e-04,\n",
       "        2.36681264e-04, 2.89827909e-04, 2.40168984e-04, 9.33864016e-04,\n",
       "        5.83542619e-04, 5.38271496e-04, 3.88974884e-04, 3.77824491e-04,\n",
       "        2.01844026e-04, 1.08471535e-04, 5.29002021e-04, 3.29976054e-04,\n",
       "        2.61176844e-04, 5.26138944e-04, 5.84030000e-04, 5.03879707e-04,\n",
       "        4.53865220e-04, 7.28774591e-04, 1.30936184e-04, 3.36681228e-04,\n",
       "        2.84809793e-04, 4.55256406e-04, 5.97205009e-04, 3.97312767e-04,\n",
       "        4.95420507e-04, 3.88348266e-04, 4.93173325e-04, 2.60075554e-04,\n",
       "        5.37127425e-04, 2.91049285e-04, 4.07992935e-04, 2.74510369e-04,\n",
       "        6.70924512e-04, 3.21658435e-04, 7.81532704e-04, 1.61243767e-04,\n",
       "        4.16455865e-04, 6.87171625e-04, 1.98205692e-04, 2.86884772e-04,\n",
       "        3.82765904e-04, 2.94386749e-04, 7.53078210e-05, 6.03898709e-04,\n",
       "        4.97262460e-04, 2.34066310e-04, 4.77100824e-04, 3.23205783e-04,\n",
       "        4.06171783e-04, 2.63095540e-04, 3.13980306e-04, 9.76202710e-05,\n",
       "        5.12919096e-04, 2.03815921e-04, 3.25022169e-04, 1.39690794e-04,\n",
       "        2.98320294e-04, 3.56953640e-04, 3.89430855e-04, 3.31346816e-04,\n",
       "        2.80714663e-04, 2.05135379e-04, 3.93271787e-04, 1.85683757e-04,\n",
       "        3.18079136e-04, 1.56154858e-04, 3.97907366e-04, 3.79386830e-04,\n",
       "        2.47885465e-04, 5.95480372e-04, 4.55055436e-04, 1.85782660e-04,\n",
       "        8.27561412e-04, 3.41107240e-04, 4.07497709e-04, 3.29297596e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
       "                    0.6, 0.6, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,\n",
       "                    0.7, 0.7, 0.7, 0.7, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "                    0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "                    8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8,\n",
       "                    8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6,\n",
       "                    6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6,\n",
       "                    6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4,\n",
       "                    6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4,\n",
       "                    4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4,\n",
       "                    4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2,\n",
       "                    2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2,\n",
       "                    2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8,\n",
       "                    2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "                    8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8,\n",
       "                    8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6,\n",
       "                    6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6,\n",
       "                    6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4,\n",
       "                    6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4,\n",
       "                    4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4,\n",
       "                    4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2,\n",
       "                    2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2,\n",
       "                    2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8,\n",
       "                    2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "                    8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8,\n",
       "                    8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6,\n",
       "                    6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6,\n",
       "                    6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4,\n",
       "                    6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4,\n",
       "                    4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4,\n",
       "                    4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2,\n",
       "                    2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2,\n",
       "                    2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8,\n",
       "                    2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "                    8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8,\n",
       "                    8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6,\n",
       "                    6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6,\n",
       "                    6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4,\n",
       "                    6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4,\n",
       "                    4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4,\n",
       "                    4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2,\n",
       "                    2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2,\n",
       "                    2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8,\n",
       "                    2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "                    8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8,\n",
       "                    8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6,\n",
       "                    6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6,\n",
       "                    6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4,\n",
       "                    6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4,\n",
       "                    4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 4,\n",
       "                    4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2, 2, 2,\n",
       "                    2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 2, 2,\n",
       "                    2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8,\n",
       "                    2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "                    8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8,\n",
       "                    8, 8, 8, 8, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6,\n",
       "                    6, 8, 8, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "                    3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5,\n",
       "                    1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3,\n",
       "                    4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1,\n",
       "                    2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "                    5, 1, 2, 3, 4, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.1,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.6,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.7,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.15,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.25,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 2},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 4},\n",
       "  {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5}],\n",
       " 'split0_test_score': array([0.88492895, 0.88376427, 0.88236664, 0.88248311, 0.88073608,\n",
       "        0.89622642, 0.89343117, 0.8873748 , 0.88632658, 0.88516189,\n",
       "        0.89238295, 0.8914512 , 0.88632658, 0.88492895, 0.88259958,\n",
       "        0.89541113, 0.89051945, 0.88725833, 0.88725833, 0.8839972 ,\n",
       "        0.89296529, 0.89494526, 0.89447939, 0.89319823, 0.89424645,\n",
       "        0.90146751, 0.89308176, 0.89436292, 0.89447939, 0.89121826,\n",
       "        0.90170044, 0.89797345, 0.89098532, 0.89354764, 0.89121826,\n",
       "        0.90216632, 0.89564407, 0.89051945, 0.89424645, 0.8895877 ,\n",
       "        0.89692523, 0.89983694, 0.89715816, 0.8973911 , 0.89610995,\n",
       "        0.90193338, 0.90123457, 0.89913813, 0.89284882, 0.8933147 ,\n",
       "        0.90170044, 0.89983694, 0.89634288, 0.89401351, 0.89378057,\n",
       "        0.90309807, 0.90006988, 0.8970417 , 0.89447939, 0.89238295,\n",
       "        0.89529467, 0.89878873, 0.89995341, 0.89808991, 0.89692523,\n",
       "        0.90589331, 0.90193338, 0.8951782 , 0.89727463, 0.89750757,\n",
       "        0.89564407, 0.90146751, 0.90170044, 0.89774051, 0.89890519,\n",
       "        0.89634288, 0.90309807, 0.89610995, 0.89750757, 0.89797345,\n",
       "        0.89168414, 0.89785698, 0.89715816, 0.89774051, 0.89564407,\n",
       "        0.90076869, 0.90123457, 0.89657582, 0.89727463, 0.89820638,\n",
       "        0.89610995, 0.9051945 , 0.90006988, 0.8970417 , 0.90496157,\n",
       "        0.89587701, 0.89680876, 0.89867226, 0.89727463, 0.90472863,\n",
       "        0.89436292, 0.90146751, 0.89529467, 0.89424645, 0.89680876,\n",
       "        0.90170044, 0.90426275, 0.89843932, 0.89378057, 0.90356394,\n",
       "        0.8970417 , 0.90309807, 0.89494526, 0.90239925, 0.90659213,\n",
       "        0.8933147 , 0.89960401, 0.89610995, 0.90006988, 0.90682506,\n",
       "        0.88492895, 0.88376427, 0.88236664, 0.88248311, 0.88073608,\n",
       "        0.89622642, 0.89343117, 0.8873748 , 0.88632658, 0.88516189,\n",
       "        0.89238295, 0.8914512 , 0.88632658, 0.88492895, 0.88259958,\n",
       "        0.89541113, 0.89051945, 0.88725833, 0.88725833, 0.8839972 ,\n",
       "        0.89296529, 0.89494526, 0.89447939, 0.89319823, 0.89424645,\n",
       "        0.90146751, 0.89308176, 0.89436292, 0.89447939, 0.89121826,\n",
       "        0.90170044, 0.89797345, 0.89098532, 0.89354764, 0.89121826,\n",
       "        0.90216632, 0.89564407, 0.89051945, 0.89424645, 0.8895877 ,\n",
       "        0.89692523, 0.89983694, 0.89715816, 0.8973911 , 0.89610995,\n",
       "        0.90193338, 0.90123457, 0.89913813, 0.89284882, 0.8933147 ,\n",
       "        0.90170044, 0.89983694, 0.89634288, 0.89401351, 0.89378057,\n",
       "        0.90309807, 0.90006988, 0.8970417 , 0.89447939, 0.89238295,\n",
       "        0.89529467, 0.89878873, 0.89995341, 0.89808991, 0.89692523,\n",
       "        0.90589331, 0.90193338, 0.8951782 , 0.89727463, 0.89750757,\n",
       "        0.89564407, 0.90146751, 0.90170044, 0.89774051, 0.89890519,\n",
       "        0.89634288, 0.90309807, 0.89610995, 0.89750757, 0.89797345,\n",
       "        0.89168414, 0.89785698, 0.89715816, 0.89774051, 0.89564407,\n",
       "        0.90076869, 0.90123457, 0.89657582, 0.89727463, 0.89820638,\n",
       "        0.89610995, 0.9051945 , 0.90006988, 0.8970417 , 0.90496157,\n",
       "        0.89587701, 0.89680876, 0.89867226, 0.89727463, 0.90472863,\n",
       "        0.89436292, 0.90146751, 0.89529467, 0.89424645, 0.89680876,\n",
       "        0.90170044, 0.90426275, 0.89843932, 0.89378057, 0.90356394,\n",
       "        0.8970417 , 0.90309807, 0.89494526, 0.90239925, 0.90659213,\n",
       "        0.8933147 , 0.89960401, 0.89610995, 0.90006988, 0.90682506,\n",
       "        0.90729094, 0.90659213, 0.90030282, 0.89797345, 0.89541113,\n",
       "        0.90915444, 0.907058  , 0.90146751, 0.89774051, 0.8970417 ,\n",
       "        0.91055206, 0.90845563, 0.90426275, 0.90053576, 0.89983694,\n",
       "        0.91171675, 0.90798975, 0.90589331, 0.90170044, 0.90053576,\n",
       "        0.90880503, 0.90600978, 0.90449569, 0.90146751, 0.89820638,\n",
       "        0.91125087, 0.90775681, 0.90472863, 0.90449569, 0.90379688,\n",
       "        0.9126485 , 0.91660843, 0.90566038, 0.907058  , 0.90566038,\n",
       "        0.91031912, 0.91567668, 0.90566038, 0.90402982, 0.907058  ,\n",
       "        0.90775681, 0.9111344 , 0.90472863, 0.90065222, 0.90158397,\n",
       "        0.91940368, 0.91800606, 0.91031912, 0.91055206, 0.90635919,\n",
       "        0.91148381, 0.91381318, 0.91311437, 0.91404612, 0.907058  ,\n",
       "        0.90729094, 0.91497787, 0.91288143, 0.91311437, 0.90612625,\n",
       "        0.91101794, 0.90752388, 0.90170044, 0.90414628, 0.89913813,\n",
       "        0.91660843, 0.91940368, 0.914512  , 0.91194969, 0.91334731,\n",
       "        0.90589331, 0.91497787, 0.91427906, 0.91614256, 0.91544375,\n",
       "        0.90239925, 0.91241556, 0.91241556, 0.91218262, 0.91474493,\n",
       "        0.91241556, 0.91031912, 0.90775681, 0.9048451 , 0.90379688,\n",
       "        0.91055206, 0.91777312, 0.91497787, 0.91800606, 0.91497787,\n",
       "        0.90146751, 0.91404612, 0.91847193, 0.91404612, 0.91660843,\n",
       "        0.90752388, 0.91474493, 0.91125087, 0.90962031, 0.91334731,\n",
       "        0.91288143, 0.91381318, 0.9089215 , 0.9089215 , 0.90589331,\n",
       "        0.91614256, 0.91474493, 0.91800606, 0.91381318, 0.91660843,\n",
       "        0.91311437, 0.9126485 , 0.91008619, 0.91521081, 0.91986956,\n",
       "        0.907058  , 0.91847193, 0.90938737, 0.91404612, 0.91427906,\n",
       "        0.91392965, 0.91695784, 0.91497787, 0.90996972, 0.90426275,\n",
       "        0.91358025, 0.91497787, 0.91101794, 0.90915444, 0.91031912,\n",
       "        0.9208013 , 0.91940368, 0.91288143, 0.91125087, 0.910785  ,\n",
       "        0.91986956, 0.92313068, 0.91730724, 0.91194969, 0.9089215 ,\n",
       "        0.90682506, 0.90787328, 0.910785  , 0.90950384, 0.90402982,\n",
       "        0.91288143, 0.91381318, 0.91963662, 0.91241556, 0.91311437,\n",
       "        0.91870487, 0.91917074, 0.91777312, 0.91684137, 0.91614256,\n",
       "        0.91497787, 0.92103424, 0.91893781, 0.91497787, 0.91241556,\n",
       "        0.91800606, 0.91730724, 0.91288143, 0.90798975, 0.90566038,\n",
       "        0.91940368, 0.91544375, 0.91684137, 0.91870487, 0.92150012,\n",
       "        0.90449569, 0.91427906, 0.92150012, 0.92150012, 0.92685768,\n",
       "        0.90635919, 0.91777312, 0.9226648 , 0.91963662, 0.92103424,\n",
       "        0.91730724, 0.91497787, 0.91404612, 0.91148381, 0.91544375,\n",
       "        0.910785  , 0.92150012, 0.91474493, 0.9282553 , 0.92522711,\n",
       "        0.90612625, 0.91986956, 0.92126718, 0.92802236, 0.92569299,\n",
       "        0.90379688, 0.91660843, 0.91986956, 0.92546005, 0.92569299,\n",
       "        0.91788959, 0.91754018, 0.914512  , 0.92476124, 0.92522711,\n",
       "        0.91381318, 0.92429536, 0.91893781, 0.92592593, 0.92732355,\n",
       "        0.90076869, 0.91358025, 0.91847193, 0.92778942, 0.92965292,\n",
       "        0.90123457, 0.91218262, 0.91637549, 0.92476124, 0.92662474,\n",
       "        0.92021896, 0.92103424, 0.92103424, 0.91940368, 0.91917074,\n",
       "        0.90775681, 0.91358025, 0.92569299, 0.92219893, 0.92802236,\n",
       "        0.89727463, 0.90682506, 0.91917074, 0.92662474, 0.9263918 ,\n",
       "        0.89797345, 0.90985325, 0.91567668, 0.9226648 , 0.9282553 ,\n",
       "        0.90542744, 0.90426275, 0.903331  , 0.90368041, 0.90309807,\n",
       "        0.91334731, 0.91241556, 0.90868856, 0.90985325, 0.90938737,\n",
       "        0.91730724, 0.91754018, 0.91730724, 0.91427906, 0.91031912,\n",
       "        0.92313068, 0.91660843, 0.91707431, 0.91497787, 0.91101794,\n",
       "        0.91637549, 0.91125087, 0.91497787, 0.91008619, 0.91031912,\n",
       "        0.91684137, 0.92056837, 0.91940368, 0.91497787, 0.91637549,\n",
       "        0.91358025, 0.91963662, 0.9208013 , 0.92406243, 0.91847193,\n",
       "        0.91521081, 0.91963662, 0.91754018, 0.92173305, 0.91963662,\n",
       "        0.91148381, 0.91125087, 0.907058  , 0.914512  , 0.91241556,\n",
       "        0.91800606, 0.91660843, 0.91614256, 0.92126718, 0.92173305,\n",
       "        0.90845563, 0.91707431, 0.91427906, 0.91986956, 0.9226648 ,\n",
       "        0.910785  , 0.91590962, 0.91754018, 0.92522711, 0.92126718,\n",
       "        0.91556021, 0.91323084, 0.91171675, 0.91893781, 0.91544375,\n",
       "        0.910785  , 0.91358025, 0.92476124, 0.92056837, 0.91800606,\n",
       "        0.90635919, 0.91288143, 0.91777312, 0.92313068, 0.92406243,\n",
       "        0.90123457, 0.91381318, 0.91497787, 0.91893781, 0.92150012,\n",
       "        0.91637549, 0.9208013 , 0.92196599, 0.92173305, 0.91986956,\n",
       "        0.91031912, 0.91311437, 0.91707431, 0.92522711, 0.91986956,\n",
       "        0.90985325, 0.91241556, 0.92056837, 0.91870487, 0.92965292,\n",
       "        0.90635919, 0.91125087, 0.91055206, 0.91940368, 0.92289774,\n",
       "        0.91521081, 0.91497787, 0.92126718, 0.92592593, 0.92126718,\n",
       "        0.903331  , 0.91194969, 0.92336362, 0.91707431, 0.91637549,\n",
       "        0.90915444, 0.90263219, 0.91940368, 0.91544375, 0.92103424,\n",
       "        0.90845563, 0.91311437, 0.91241556, 0.92056837, 0.91870487,\n",
       "        0.91544375, 0.91125087, 0.91253203, 0.90635919, 0.90391335,\n",
       "        0.91660843, 0.91381318, 0.91590962, 0.91590962, 0.91008619,\n",
       "        0.91637549, 0.91847193, 0.92662474, 0.92406243, 0.91474493,\n",
       "        0.91800606, 0.91963662, 0.92173305, 0.92359655, 0.91684137,\n",
       "        0.91241556, 0.91031912, 0.91218262, 0.91381318, 0.91358025,\n",
       "        0.914512  , 0.914512  , 0.92010249, 0.92103424, 0.91917074,\n",
       "        0.92196599, 0.91637549, 0.92546005, 0.92569299, 0.9263918 ,\n",
       "        0.92056837, 0.9208013 , 0.92615886, 0.92429536, 0.92755649,\n",
       "        0.91940368, 0.91637549, 0.91917074, 0.91986956, 0.9208013 ,\n",
       "        0.91963662, 0.90962031, 0.914512  , 0.92709061, 0.92173305,\n",
       "        0.91288143, 0.91614256, 0.9208013 , 0.92499418, 0.9263918 ,\n",
       "        0.9126485 , 0.91986956, 0.92056837, 0.92802236, 0.92965292,\n",
       "        0.91963662, 0.91917074, 0.92056837, 0.91847193, 0.92219893,\n",
       "        0.91917074, 0.91358025, 0.91800606, 0.92429536, 0.92569299,\n",
       "        0.91194969, 0.91031912, 0.92546005, 0.93081761, 0.92429536,\n",
       "        0.90798975, 0.91847193, 0.92359655, 0.93221523, 0.9208013 ,\n",
       "        0.91707431, 0.91940368, 0.92126718, 0.92406243, 0.92499418,\n",
       "        0.91427906, 0.9126485 , 0.91870487, 0.92546005, 0.92918705,\n",
       "        0.90868856, 0.90659213, 0.91381318, 0.92848824, 0.92546005,\n",
       "        0.907058  , 0.91241556, 0.910785  , 0.91637549, 0.92499418,\n",
       "        0.91847193, 0.91917074, 0.92359655, 0.92406243, 0.92546005,\n",
       "        0.90938737, 0.91311437, 0.90682506, 0.9208013 , 0.91334731,\n",
       "        0.91055206, 0.90449569, 0.91381318, 0.91194969, 0.92685768,\n",
       "        0.90472863, 0.914512  , 0.91497787, 0.91754018, 0.92126718,\n",
       "        0.91940368, 0.91288143, 0.90927091, 0.91136734, 0.90694153,\n",
       "        0.91847193, 0.91684137, 0.91544375, 0.91777312, 0.91567668,\n",
       "        0.91684137, 0.91777312, 0.91777312, 0.92243187, 0.91940368,\n",
       "        0.91963662, 0.91427906, 0.92150012, 0.92685768, 0.91963662,\n",
       "        0.91777312, 0.91497787, 0.91427906, 0.91544375, 0.91870487,\n",
       "        0.91614256, 0.91404612, 0.91614256, 0.92219893, 0.92802236,\n",
       "        0.91800606, 0.91870487, 0.91963662, 0.92685768, 0.9282553 ,\n",
       "        0.91963662, 0.91870487, 0.91707431, 0.92918705, 0.92988586,\n",
       "        0.91800606, 0.91101794, 0.91707431, 0.91800606, 0.92103424,\n",
       "        0.91381318, 0.91754018, 0.92359655, 0.92429536, 0.92103424,\n",
       "        0.91194969, 0.91521081, 0.92033543, 0.92732355, 0.92289774,\n",
       "        0.90775681, 0.91055206, 0.92010249, 0.92778942, 0.92755649,\n",
       "        0.91101794, 0.90752388, 0.91509434, 0.91707431, 0.92033543,\n",
       "        0.91427906, 0.92173305, 0.91521081, 0.92382949, 0.92126718,\n",
       "        0.91288143, 0.91008619, 0.91963662, 0.92685768, 0.92476124,\n",
       "        0.910785  , 0.90822269, 0.9226648 , 0.92872117, 0.91777312,\n",
       "        0.91171675, 0.91381318, 0.91521081, 0.92709061, 0.92243187,\n",
       "        0.91730724, 0.91148381, 0.92033543, 0.92219893, 0.92429536,\n",
       "        0.9051945 , 0.91008619, 0.91288143, 0.92382949, 0.92289774,\n",
       "        0.90472863, 0.91986956, 0.91707431, 0.91800606, 0.91590962,\n",
       "        0.90775681, 0.910785  , 0.91521081, 0.92359655, 0.91427906,\n",
       "        0.91031912, 0.90752388, 0.91940368, 0.92569299, 0.91754018,\n",
       "        0.89983694, 0.90659213, 0.90426275, 0.91241556, 0.91707431,\n",
       "        0.90449569, 0.90635919, 0.91311437, 0.91777312, 0.91660843,\n",
       "        0.9129979 , 0.91055206, 0.90868856, 0.91288143, 0.90996972,\n",
       "        0.91730724, 0.91194969, 0.91218262, 0.91427906, 0.91427906,\n",
       "        0.91800606, 0.91660843, 0.9208013 , 0.92615886, 0.92103424,\n",
       "        0.92173305, 0.91777312, 0.91963662, 0.92359655, 0.91893781,\n",
       "        0.91544375, 0.91194969, 0.91521081, 0.91963662, 0.91171675,\n",
       "        0.91031912, 0.91334731, 0.91684137, 0.9226648 , 0.92709061,\n",
       "        0.91870487, 0.91660843, 0.91777312, 0.92709061, 0.92988586,\n",
       "        0.90752388, 0.92010249, 0.92709061, 0.92802236, 0.92872117,\n",
       "        0.92289774, 0.91358025, 0.91986956, 0.91893781, 0.91800606,\n",
       "        0.91288143, 0.90729094, 0.91940368, 0.92592593, 0.92755649,\n",
       "        0.90589331, 0.91008619, 0.92732355, 0.92522711, 0.92615886,\n",
       "        0.907058  , 0.91171675, 0.92685768, 0.92289774, 0.92546005,\n",
       "        0.91567668, 0.91637549, 0.91730724, 0.92662474, 0.9245283 ,\n",
       "        0.90682506, 0.90612625, 0.92173305, 0.92313068, 0.91917074,\n",
       "        0.91288143, 0.91171675, 0.91893781, 0.9226648 , 0.92359655,\n",
       "        0.90938737, 0.91427906, 0.91707431, 0.92499418, 0.92196599,\n",
       "        0.91754018, 0.91590962, 0.92476124, 0.92522711, 0.92778942,\n",
       "        0.91194969, 0.91404612, 0.91474493, 0.91637549, 0.92289774,\n",
       "        0.90542744, 0.91101794, 0.91707431, 0.91986956, 0.92336362,\n",
       "        0.90775681, 0.91311437, 0.91637549, 0.91660843, 0.91660843,\n",
       "        0.91125087, 0.91637549, 0.91870487, 0.92685768, 0.9245283 ,\n",
       "        0.90659213, 0.91311437, 0.90915444, 0.92150012, 0.91637549,\n",
       "        0.90566038, 0.90356394, 0.91148381, 0.91777312, 0.91754018,\n",
       "        0.90729094, 0.90263219, 0.91055206, 0.91404612, 0.91194969]),\n",
       " 'split1_test_score': array([0.79583042, 0.79885861, 0.79781039, 0.79280224, 0.78930818,\n",
       "        0.79489867, 0.79559748, 0.79501514, 0.78965758, 0.78430002,\n",
       "        0.79536455, 0.79722805, 0.79419986, 0.78721174, 0.78418355,\n",
       "        0.79979036, 0.80025623, 0.79513161, 0.78674587, 0.78488237,\n",
       "        0.81900769, 0.81586303, 0.81574656, 0.81271838, 0.80491498,\n",
       "        0.80840904, 0.81446541, 0.81469835, 0.80980666, 0.80281854,\n",
       "        0.81236897, 0.81632891, 0.81306778, 0.80724435, 0.80398323,\n",
       "        0.81679478, 0.81795947, 0.81353366, 0.80724435, 0.80514792,\n",
       "        0.82879106, 0.82646168, 0.8237829 , 0.8200559 , 0.81516422,\n",
       "        0.81959003, 0.81889122, 0.81446541, 0.81726066, 0.81027254,\n",
       "        0.81749359, 0.82028884, 0.82075472, 0.81609597, 0.81353366,\n",
       "        0.82145353, 0.82518053, 0.8219194 , 0.81679478, 0.81330072,\n",
       "        0.83193571, 0.8297228 , 0.82855812, 0.82576287, 0.82040531,\n",
       "        0.81935709, 0.82098765, 0.82168647, 0.81656184, 0.8122525 ,\n",
       "        0.81865828, 0.82448171, 0.83100396, 0.81656184, 0.81574656,\n",
       "        0.82471465, 0.82914046, 0.82867459, 0.82145353, 0.81761006,\n",
       "        0.83112043, 0.8293734 , 0.83007221, 0.82657815, 0.81912416,\n",
       "        0.81353366, 0.82028884, 0.82401584, 0.81912416, 0.81667831,\n",
       "        0.82261822, 0.82634521, 0.82774284, 0.8200559 , 0.81737713,\n",
       "        0.82354997, 0.83356627, 0.83216865, 0.81446541, 0.8159795 ,\n",
       "        0.82681109, 0.82634521, 0.82914046, 0.82890752, 0.82413231,\n",
       "        0.81726066, 0.82168647, 0.8234335 , 0.81574656, 0.81667831,\n",
       "        0.82168647, 0.82657815, 0.83333333, 0.817843  , 0.81761006,\n",
       "        0.82518053, 0.83146983, 0.83216865, 0.82110412, 0.81318425,\n",
       "        0.79583042, 0.79885861, 0.79781039, 0.79280224, 0.78930818,\n",
       "        0.79489867, 0.79559748, 0.79501514, 0.78965758, 0.78430002,\n",
       "        0.79536455, 0.79722805, 0.79419986, 0.78721174, 0.78418355,\n",
       "        0.79979036, 0.80025623, 0.79513161, 0.78674587, 0.78488237,\n",
       "        0.81900769, 0.81586303, 0.81574656, 0.81271838, 0.80491498,\n",
       "        0.80840904, 0.81446541, 0.81469835, 0.80980666, 0.80281854,\n",
       "        0.81236897, 0.81632891, 0.81306778, 0.80724435, 0.80398323,\n",
       "        0.81679478, 0.81795947, 0.81353366, 0.80724435, 0.80514792,\n",
       "        0.82879106, 0.82646168, 0.8237829 , 0.8200559 , 0.81516422,\n",
       "        0.81959003, 0.81889122, 0.81446541, 0.81726066, 0.81027254,\n",
       "        0.81749359, 0.82028884, 0.82075472, 0.81609597, 0.81353366,\n",
       "        0.82145353, 0.82518053, 0.8219194 , 0.81679478, 0.81330072,\n",
       "        0.83193571, 0.8297228 , 0.82855812, 0.82576287, 0.82040531,\n",
       "        0.81935709, 0.82098765, 0.82168647, 0.81656184, 0.8122525 ,\n",
       "        0.81865828, 0.82448171, 0.83100396, 0.81656184, 0.81574656,\n",
       "        0.82471465, 0.82914046, 0.82867459, 0.82145353, 0.81761006,\n",
       "        0.83112043, 0.8293734 , 0.83007221, 0.82657815, 0.81912416,\n",
       "        0.81353366, 0.82028884, 0.82401584, 0.81912416, 0.81667831,\n",
       "        0.82261822, 0.82634521, 0.82774284, 0.8200559 , 0.81737713,\n",
       "        0.82354997, 0.83356627, 0.83216865, 0.81446541, 0.8159795 ,\n",
       "        0.82681109, 0.82634521, 0.82914046, 0.82890752, 0.82413231,\n",
       "        0.81726066, 0.82168647, 0.8234335 , 0.81574656, 0.81667831,\n",
       "        0.82168647, 0.82657815, 0.83333333, 0.817843  , 0.81761006,\n",
       "        0.82518053, 0.83146983, 0.83216865, 0.82110412, 0.81318425,\n",
       "        0.81155369, 0.81341719, 0.81399953, 0.80736082, 0.80328442,\n",
       "        0.80910785, 0.81027254, 0.81167016, 0.80142092, 0.79792686,\n",
       "        0.81306778, 0.8119031 , 0.81446541, 0.80235267, 0.79839273,\n",
       "        0.81493128, 0.81982297, 0.81609597, 0.80281854, 0.79862567,\n",
       "        0.82925693, 0.82774284, 0.8331004 , 0.82296762, 0.82122059,\n",
       "        0.82261822, 0.8256464 , 0.81702772, 0.81656184, 0.81993944,\n",
       "        0.81842534, 0.82727696, 0.8293734 , 0.81819241, 0.82296762,\n",
       "        0.82681109, 0.83077102, 0.83053809, 0.82028884, 0.82040531,\n",
       "        0.82587934, 0.82820871, 0.83216865, 0.82983927, 0.82983927,\n",
       "        0.81982297, 0.8331004 , 0.83240158, 0.82704403, 0.82506406,\n",
       "        0.82215234, 0.83542977, 0.83542977, 0.83077102, 0.82576287,\n",
       "        0.83053809, 0.84404845, 0.83659446, 0.8256464 , 0.825297  ,\n",
       "        0.83286746, 0.82844165, 0.83030515, 0.82681109, 0.82692756,\n",
       "        0.8256464 , 0.83379921, 0.83403215, 0.82436525, 0.82273468,\n",
       "        0.82797577, 0.83659446, 0.83729327, 0.8334498 , 0.82459818,\n",
       "        0.84171908, 0.83426508, 0.83612858, 0.8297228 , 0.82762637,\n",
       "        0.83542977, 0.82983927, 0.82960634, 0.82483112, 0.82716049,\n",
       "        0.83333333, 0.83193571, 0.83181924, 0.83077102, 0.82483112,\n",
       "        0.83869089, 0.84288376, 0.83997205, 0.8297228 , 0.82785931,\n",
       "        0.82704403, 0.84707664, 0.83740974, 0.83601211, 0.82576287,\n",
       "        0.83799208, 0.8312369 , 0.83275099, 0.82855812, 0.82599581,\n",
       "        0.82518053, 0.83333333, 0.8353133 , 0.82832518, 0.82785931,\n",
       "        0.83706033, 0.84521314, 0.83973911, 0.8334498 , 0.82995574,\n",
       "        0.82587934, 0.83985558, 0.84230142, 0.8334498 , 0.82459818,\n",
       "        0.82157   , 0.82541346, 0.82704403, 0.82075472, 0.81446541,\n",
       "        0.81772653, 0.82634521, 0.8256464 , 0.8200559 , 0.81015607,\n",
       "        0.82145353, 0.82867459, 0.8237829 , 0.82122059, 0.81073841,\n",
       "        0.82285115, 0.83216865, 0.82844165, 0.8219194 , 0.81120429,\n",
       "        0.83216865, 0.83286746, 0.83542977, 0.83146983, 0.82855812,\n",
       "        0.82401584, 0.82960634, 0.82914046, 0.82098765, 0.82366643,\n",
       "        0.82471465, 0.83799208, 0.83612858, 0.82820871, 0.825297  ,\n",
       "        0.8312369 , 0.83775914, 0.83892383, 0.82867459, 0.82622874,\n",
       "        0.83263452, 0.83053809, 0.84218495, 0.83216865, 0.8297228 ,\n",
       "        0.82494759, 0.83053809, 0.8331004 , 0.83275099, 0.82832518,\n",
       "        0.8331004 , 0.83799208, 0.83799208, 0.82948987, 0.83088749,\n",
       "        0.83216865, 0.84055439, 0.8412532 , 0.82855812, 0.82879106,\n",
       "        0.83659446, 0.84171908, 0.84404845, 0.84020498, 0.83065455,\n",
       "        0.83962264, 0.83333333, 0.83542977, 0.82483112, 0.82832518,\n",
       "        0.82681109, 0.84078733, 0.8431167 , 0.83764267, 0.8353133 ,\n",
       "        0.83379921, 0.84381551, 0.84102027, 0.83857442, 0.8409038 ,\n",
       "        0.8412532 , 0.83752621, 0.83263452, 0.83694386, 0.82762637,\n",
       "        0.83799208, 0.83542977, 0.83484743, 0.83391568, 0.82669462,\n",
       "        0.83333333, 0.84055439, 0.83927324, 0.8334498 , 0.82902399,\n",
       "        0.8275099 , 0.84591195, 0.84463079, 0.83088749, 0.83181924,\n",
       "        0.8468437 , 0.83542977, 0.83484743, 0.83251805, 0.82995574,\n",
       "        0.83077102, 0.83077102, 0.83018868, 0.83181924, 0.82669462,\n",
       "        0.83286746, 0.85033776, 0.83973911, 0.83181924, 0.83228512,\n",
       "        0.8312369 , 0.8393897 , 0.84230142, 0.82739343, 0.82506406,\n",
       "        0.82785931, 0.83193571, 0.8334498 , 0.83053809, 0.82354997,\n",
       "        0.8237829 , 0.82401584, 0.82657815, 0.82785931, 0.82273468,\n",
       "        0.82797577, 0.83263452, 0.83240158, 0.82797577, 0.82855812,\n",
       "        0.8293734 , 0.83519683, 0.83286746, 0.8312369 , 0.82995574,\n",
       "        0.8312369 , 0.83449802, 0.83379921, 0.83577918, 0.82855812,\n",
       "        0.83542977, 0.83333333, 0.83519683, 0.83193571, 0.8315863 ,\n",
       "        0.83333333, 0.83729327, 0.84637782, 0.84078733, 0.83251805,\n",
       "        0.83752621, 0.83403215, 0.84521314, 0.83706033, 0.82855812,\n",
       "        0.82867459, 0.83775914, 0.83542977, 0.83461449, 0.83275099,\n",
       "        0.83170277, 0.83892383, 0.84171908, 0.8353133 , 0.83484743,\n",
       "        0.82704403, 0.84078733, 0.8431167 , 0.83880736, 0.84253436,\n",
       "        0.83356627, 0.84055439, 0.84171908, 0.83461449, 0.83973911,\n",
       "        0.83892383, 0.84265083, 0.83566271, 0.83834149, 0.8334498 ,\n",
       "        0.84055439, 0.8412532 , 0.83892383, 0.83880736, 0.84160261,\n",
       "        0.82727696, 0.84381551, 0.83438155, 0.83414861, 0.83973911,\n",
       "        0.83519683, 0.84661076, 0.83671092, 0.84183555, 0.84393198,\n",
       "        0.84008852, 0.8409038 , 0.83251805, 0.83997205, 0.8334498 ,\n",
       "        0.83007221, 0.84940601, 0.84323317, 0.84416492, 0.84346611,\n",
       "        0.82424878, 0.84800839, 0.84206848, 0.84509667, 0.84043792,\n",
       "        0.8256464 , 0.84358258, 0.83950617, 0.84439786, 0.83997205,\n",
       "        0.8431167 , 0.83962264, 0.83612858, 0.83298393, 0.83135337,\n",
       "        0.84195201, 0.84381551, 0.8505707 , 0.85534591, 0.8409038 ,\n",
       "        0.82541346, 0.85196832, 0.84544608, 0.83740974, 0.8371768 ,\n",
       "        0.81609597, 0.84078733, 0.84218495, 0.83624505, 0.83601211,\n",
       "        0.82774284, 0.83065455, 0.83403215, 0.82762637, 0.81959003,\n",
       "        0.82285115, 0.82145353, 0.82471465, 0.8275099 , 0.82902399,\n",
       "        0.82518053, 0.83473096, 0.83193571, 0.82983927, 0.83298393,\n",
       "        0.83030515, 0.8331004 , 0.83333333, 0.83216865, 0.83065455,\n",
       "        0.82471465, 0.8331004 , 0.83379921, 0.83205218, 0.83251805,\n",
       "        0.82611228, 0.83333333, 0.83065455, 0.83135337, 0.83112043,\n",
       "        0.82774284, 0.83729327, 0.83834149, 0.8315863 , 0.83368274,\n",
       "        0.82681109, 0.83799208, 0.8371768 , 0.83275099, 0.83438155,\n",
       "        0.82354997, 0.83636152, 0.83414861, 0.83601211, 0.82925693,\n",
       "        0.83496389, 0.84579548, 0.84509667, 0.83764267, 0.83950617,\n",
       "        0.82681109, 0.83671092, 0.84369904, 0.84020498, 0.83671092,\n",
       "        0.82867459, 0.84206848, 0.84113673, 0.8390403 , 0.83857442,\n",
       "        0.83053809, 0.83030515, 0.83857442, 0.82785931, 0.83135337,\n",
       "        0.84032145, 0.84800839, 0.83612858, 0.84241789, 0.84696017,\n",
       "        0.82914046, 0.83973911, 0.84113673, 0.84346611, 0.84509667,\n",
       "        0.82448171, 0.84463079, 0.83473096, 0.84509667, 0.84672723,\n",
       "        0.83985558, 0.83624505, 0.8390403 , 0.83694386, 0.8297228 ,\n",
       "        0.82960634, 0.8427673 , 0.84824132, 0.84183555, 0.84323317,\n",
       "        0.8219194 , 0.83973911, 0.85383182, 0.84579548, 0.84696017,\n",
       "        0.82611228, 0.83834149, 0.84579548, 0.84579548, 0.84509667,\n",
       "        0.84078733, 0.8312369 , 0.83589564, 0.83438155, 0.82925693,\n",
       "        0.85220126, 0.84998835, 0.84160261, 0.85441416, 0.84369904,\n",
       "        0.82797577, 0.84067086, 0.85511297, 0.85068717, 0.84416492,\n",
       "        0.83403215, 0.83449802, 0.85744235, 0.85022129, 0.84393198,\n",
       "        0.83216865, 0.82995574, 0.83414861, 0.83473096, 0.82902399,\n",
       "        0.82820871, 0.82704403, 0.83216865, 0.83356627, 0.83577918,\n",
       "        0.8331004 , 0.8293734 , 0.83286746, 0.83775914, 0.83927324,\n",
       "        0.83146983, 0.83612858, 0.83799208, 0.83869089, 0.84113673,\n",
       "        0.82960634, 0.83473096, 0.83240158, 0.83112043, 0.83368274,\n",
       "        0.83799208, 0.8412532 , 0.84253436, 0.83834149, 0.84206848,\n",
       "        0.83636152, 0.83775914, 0.84300023, 0.84696017, 0.84346611,\n",
       "        0.83053809, 0.83799208, 0.84230142, 0.8427673 , 0.84859073,\n",
       "        0.82634521, 0.83193571, 0.83601211, 0.83764267, 0.83461449,\n",
       "        0.83519683, 0.84579548, 0.84463079, 0.85045423, 0.84486373,\n",
       "        0.83845795, 0.84160261, 0.84649429, 0.84789192, 0.84789192,\n",
       "        0.83589564, 0.84532961, 0.84882367, 0.84975542, 0.84859073,\n",
       "        0.83426508, 0.8393897 , 0.84043792, 0.84160261, 0.83834149,\n",
       "        0.83426508, 0.84730957, 0.85546238, 0.85604472, 0.85488004,\n",
       "        0.83612858, 0.85103657, 0.86000466, 0.85208479, 0.84789192,\n",
       "        0.8256464 , 0.85208479, 0.85604472, 0.84626136, 0.84253436,\n",
       "        0.83869089, 0.83962264, 0.83414861, 0.8409038 , 0.83508036,\n",
       "        0.84777545, 0.85161891, 0.8509201 , 0.8546471 , 0.85045423,\n",
       "        0.84008852, 0.84183555, 0.85720941, 0.85557885, 0.84952248,\n",
       "        0.84567901, 0.85767529, 0.85744235, 0.85488004, 0.83834149,\n",
       "        0.84614489, 0.83985558, 0.84369904, 0.84486373, 0.84742604,\n",
       "        0.8393897 , 0.85313301, 0.86000466, 0.86443047, 0.85208479,\n",
       "        0.83053809, 0.85161891, 0.85755882, 0.85814116, 0.84591195,\n",
       "        0.84078733, 0.85208479, 0.86419753, 0.85557885, 0.85010482,\n",
       "        0.8315863 , 0.83065455, 0.83484743, 0.83321686, 0.82867459,\n",
       "        0.83193571, 0.83496389, 0.83845795, 0.84043792, 0.84043792,\n",
       "        0.83542977, 0.83845795, 0.84136967, 0.84486373, 0.84463079,\n",
       "        0.83822502, 0.84032145, 0.84241789, 0.8427673 , 0.84439786,\n",
       "        0.83007221, 0.83356627, 0.83787561, 0.83321686, 0.83484743,\n",
       "        0.84544608, 0.84486373, 0.85068717, 0.84113673, 0.84416492,\n",
       "        0.83659446, 0.8449802 , 0.84928954, 0.84393198, 0.84532961,\n",
       "        0.83799208, 0.84661076, 0.84882367, 0.84556254, 0.84579548,\n",
       "        0.83729327, 0.84218495, 0.83880736, 0.83880736, 0.83438155,\n",
       "        0.8431167 , 0.85534591, 0.85255066, 0.8427673 , 0.84136967,\n",
       "        0.84148614, 0.8487072 , 0.8490566 , 0.84998835, 0.84300023,\n",
       "        0.8393897 , 0.85744235, 0.8527836 , 0.84602842, 0.84346611,\n",
       "        0.84218495, 0.84195201, 0.84323317, 0.84463079, 0.8371768 ,\n",
       "        0.83892383, 0.85208479, 0.84393198, 0.85371535, 0.84532961,\n",
       "        0.8412532 , 0.84672723, 0.85627766, 0.84928954, 0.84393198,\n",
       "        0.83985558, 0.85534591, 0.84649429, 0.85557885, 0.84602842,\n",
       "        0.84334964, 0.84102027, 0.84463079, 0.84532961, 0.83438155,\n",
       "        0.83822502, 0.84859073, 0.84626136, 0.85767529, 0.83962264,\n",
       "        0.84008852, 0.85115304, 0.86396459, 0.85394829, 0.85115304,\n",
       "        0.84218495, 0.85231773, 0.86279991, 0.86279991, 0.84556254,\n",
       "        0.84661076, 0.84102027, 0.85371535, 0.8490566 , 0.84952248,\n",
       "        0.84940601, 0.85604472, 0.85115304, 0.85418123, 0.85173538,\n",
       "        0.83146983, 0.84626136, 0.85488004, 0.86093641, 0.84055439,\n",
       "        0.84334964, 0.85557885, 0.8565106 , 0.85720941, 0.84102027]),\n",
       " 'split2_test_score': array([0.83379921, 0.83519683, 0.83636152, 0.83449802, 0.83403215,\n",
       "        0.82855812, 0.82716049, 0.8312369 , 0.82925693, 0.82948987,\n",
       "        0.82646168, 0.825297  , 0.82669462, 0.82785931, 0.82506406,\n",
       "        0.82669462, 0.82483112, 0.82622874, 0.82785931, 0.82552993,\n",
       "        0.83810855, 0.83834149, 0.83915677, 0.83927324, 0.83973911,\n",
       "        0.83170277, 0.82902399, 0.83251805, 0.8334498 , 0.8334498 ,\n",
       "        0.83112043, 0.83205218, 0.82948987, 0.83461449, 0.83391568,\n",
       "        0.82809224, 0.83391568, 0.83368274, 0.83321686, 0.83298393,\n",
       "        0.83461449, 0.83682739, 0.83414861, 0.83729327, 0.83263452,\n",
       "        0.82902399, 0.83251805, 0.83601211, 0.83368274, 0.83601211,\n",
       "        0.83368274, 0.83368274, 0.83554624, 0.83764267, 0.83461449,\n",
       "        0.82925693, 0.83601211, 0.83694386, 0.83740974, 0.83461449,\n",
       "        0.82634521, 0.83088749, 0.8315863 , 0.83112043, 0.83146983,\n",
       "        0.82436525, 0.83694386, 0.8427673 , 0.83740974, 0.8353133 ,\n",
       "        0.825297  , 0.83391568, 0.8409038 , 0.83857442, 0.83787561,\n",
       "        0.82250175, 0.8353133 , 0.84160261, 0.84067086, 0.83740974,\n",
       "        0.82308409, 0.83007221, 0.83181924, 0.8293734 , 0.82983927,\n",
       "        0.82401584, 0.83484743, 0.84393198, 0.83624505, 0.83484743,\n",
       "        0.82436525, 0.83321686, 0.83601211, 0.84043792, 0.83438155,\n",
       "        0.81947356, 0.83647799, 0.83740974, 0.83973911, 0.83251805,\n",
       "        0.81795947, 0.82762637, 0.83647799, 0.83624505, 0.83298393,\n",
       "        0.82599581, 0.83461449, 0.84043792, 0.83880736, 0.83298393,\n",
       "        0.82879106, 0.83508036, 0.83647799, 0.84113673, 0.8371768 ,\n",
       "        0.82459818, 0.8390403 , 0.83997205, 0.84300023, 0.83251805,\n",
       "        0.83379921, 0.83519683, 0.83636152, 0.83449802, 0.83403215,\n",
       "        0.82855812, 0.82716049, 0.8312369 , 0.82925693, 0.82948987,\n",
       "        0.82646168, 0.825297  , 0.82669462, 0.82785931, 0.82506406,\n",
       "        0.82669462, 0.82483112, 0.82622874, 0.82785931, 0.82552993,\n",
       "        0.83810855, 0.83834149, 0.83915677, 0.83927324, 0.83973911,\n",
       "        0.83170277, 0.82902399, 0.83251805, 0.8334498 , 0.8334498 ,\n",
       "        0.83112043, 0.83205218, 0.82948987, 0.83461449, 0.83391568,\n",
       "        0.82809224, 0.83391568, 0.83368274, 0.83321686, 0.83298393,\n",
       "        0.83461449, 0.83682739, 0.83414861, 0.83729327, 0.83263452,\n",
       "        0.82902399, 0.83251805, 0.83601211, 0.83368274, 0.83601211,\n",
       "        0.83368274, 0.83368274, 0.83554624, 0.83764267, 0.83461449,\n",
       "        0.82925693, 0.83601211, 0.83694386, 0.83740974, 0.83461449,\n",
       "        0.82634521, 0.83088749, 0.8315863 , 0.83112043, 0.83146983,\n",
       "        0.82436525, 0.83694386, 0.8427673 , 0.83740974, 0.8353133 ,\n",
       "        0.825297  , 0.83391568, 0.8409038 , 0.83857442, 0.83787561,\n",
       "        0.82250175, 0.8353133 , 0.84160261, 0.84067086, 0.83740974,\n",
       "        0.82308409, 0.83007221, 0.83181924, 0.8293734 , 0.82983927,\n",
       "        0.82401584, 0.83484743, 0.84393198, 0.83624505, 0.83484743,\n",
       "        0.82436525, 0.83321686, 0.83601211, 0.84043792, 0.83438155,\n",
       "        0.81947356, 0.83647799, 0.83740974, 0.83973911, 0.83251805,\n",
       "        0.81795947, 0.82762637, 0.83647799, 0.83624505, 0.83298393,\n",
       "        0.82599581, 0.83461449, 0.84043792, 0.83880736, 0.83298393,\n",
       "        0.82879106, 0.83508036, 0.83647799, 0.84113673, 0.8371768 ,\n",
       "        0.82459818, 0.8390403 , 0.83997205, 0.84300023, 0.83251805,\n",
       "        0.83636152, 0.83869089, 0.8334498 , 0.83240158, 0.8297228 ,\n",
       "        0.82483112, 0.82879106, 0.82692756, 0.82459818, 0.82226881,\n",
       "        0.8234335 , 0.82459818, 0.82483112, 0.82063825, 0.82017237,\n",
       "        0.82483112, 0.82669462, 0.82110412, 0.82087119, 0.82087119,\n",
       "        0.83612858, 0.83298393, 0.83566271, 0.83007221, 0.82739343,\n",
       "        0.8234335 , 0.82599581, 0.81830887, 0.82366643, 0.82063825,\n",
       "        0.82063825, 0.82320056, 0.81993944, 0.82157   , 0.8159795 ,\n",
       "        0.82506406, 0.825297  , 0.82133706, 0.82296762, 0.81993944,\n",
       "        0.82308409, 0.82692756, 0.82820871, 0.82436525, 0.8200559 ,\n",
       "        0.81248544, 0.82040531, 0.82273468, 0.82063825, 0.81877475,\n",
       "        0.82273468, 0.82599581, 0.82483112, 0.82669462, 0.82296762,\n",
       "        0.82273468, 0.82576287, 0.82646168, 0.82739343, 0.82506406,\n",
       "        0.81889122, 0.82203587, 0.82110412, 0.8200559 , 0.81807594,\n",
       "        0.81761006, 0.82273468, 0.82087119, 0.82180294, 0.817843  ,\n",
       "        0.83205218, 0.82879106, 0.82692756, 0.8234335 , 0.82494759,\n",
       "        0.82692756, 0.82692756, 0.83181924, 0.82879106, 0.82552993,\n",
       "        0.81458188, 0.81795947, 0.81644538, 0.81889122, 0.81679478,\n",
       "        0.82389937, 0.82762637, 0.82622874, 0.82203587, 0.81912416,\n",
       "        0.82879106, 0.82110412, 0.81947356, 0.82739343, 0.82902399,\n",
       "        0.82366643, 0.83112043, 0.82552993, 0.82925693, 0.82716049,\n",
       "        0.81749359, 0.81656184, 0.8200559 , 0.81528069, 0.81865828,\n",
       "        0.81621244, 0.82133706, 0.81551363, 0.82180294, 0.82599581,\n",
       "        0.82552993, 0.82692756, 0.82157   , 0.82948987, 0.82948987,\n",
       "        0.82366643, 0.825297  , 0.81924062, 0.82389937, 0.82855812,\n",
       "        0.8427673 , 0.84206848, 0.8371768 , 0.83985558, 0.84008852,\n",
       "        0.82902399, 0.83205218, 0.82669462, 0.82902399, 0.8297228 ,\n",
       "        0.82692756, 0.82622874, 0.82739343, 0.82762637, 0.82855812,\n",
       "        0.83391568, 0.83205218, 0.82692756, 0.82739343, 0.82622874,\n",
       "        0.83473096, 0.83484743, 0.83205218, 0.83275099, 0.82576287,\n",
       "        0.83368274, 0.82855812, 0.82017237, 0.83088749, 0.82354997,\n",
       "        0.83740974, 0.8315863 , 0.82669462, 0.82459818, 0.82145353,\n",
       "        0.83880736, 0.83414861, 0.82599581, 0.82902399, 0.82646168,\n",
       "        0.82273468, 0.83146983, 0.82692756, 0.8275099 , 0.82413231,\n",
       "        0.82622874, 0.82902399, 0.82832518, 0.82087119, 0.825297  ,\n",
       "        0.83112043, 0.83251805, 0.82925693, 0.82879106, 0.82762637,\n",
       "        0.84043792, 0.83484743, 0.82879106, 0.82855812, 0.82832518,\n",
       "        0.82681109, 0.82820871, 0.82622874, 0.82541346, 0.82215234,\n",
       "        0.82739343, 0.83484743, 0.83438155, 0.82669462, 0.83065455,\n",
       "        0.83321686, 0.83438155, 0.83251805, 0.82576287, 0.83146983,\n",
       "        0.83601211, 0.83414861, 0.82855812, 0.82762637, 0.8297228 ,\n",
       "        0.82820871, 0.82471465, 0.82436525, 0.82552993, 0.82238528,\n",
       "        0.82646168, 0.82995574, 0.81993944, 0.82785931, 0.83973911,\n",
       "        0.83694386, 0.83414861, 0.82320056, 0.8197065 , 0.8297228 ,\n",
       "        0.84579548, 0.83391568, 0.83810855, 0.82459818, 0.82925693,\n",
       "        0.82797577, 0.83170277, 0.82646168, 0.82634521, 0.82226881,\n",
       "        0.82087119, 0.83228512, 0.82250175, 0.83251805, 0.83822502,\n",
       "        0.85161891, 0.83950617, 0.82436525, 0.83577918, 0.83321686,\n",
       "        0.83694386, 0.83810855, 0.83018868, 0.82133706, 0.8219194 ,\n",
       "        0.84300023, 0.8431167 , 0.83997205, 0.84195201, 0.83752621,\n",
       "        0.83042162, 0.82436525, 0.83065455, 0.83077102, 0.83007221,\n",
       "        0.83601211, 0.8297228 , 0.82855812, 0.83088749, 0.83135337,\n",
       "        0.83927324, 0.83577918, 0.8315863 , 0.83042162, 0.83321686,\n",
       "        0.83647799, 0.83275099, 0.83519683, 0.83298393, 0.83473096,\n",
       "        0.82506406, 0.83228512, 0.8315863 , 0.83065455, 0.82599581,\n",
       "        0.83508036, 0.83484743, 0.84136967, 0.83508036, 0.83275099,\n",
       "        0.84369904, 0.83927324, 0.83997205, 0.83764267, 0.83414861,\n",
       "        0.82879106, 0.82203587, 0.82762637, 0.82681109, 0.82518053,\n",
       "        0.82483112, 0.82250175, 0.83764267, 0.83694386, 0.82832518,\n",
       "        0.84160261, 0.8390403 , 0.83880736, 0.8353133 , 0.83391568,\n",
       "        0.84300023, 0.8427673 , 0.84183555, 0.83368274, 0.83112043,\n",
       "        0.82098765, 0.82040531, 0.82110412, 0.8237829 , 0.8234335 ,\n",
       "        0.82739343, 0.83857442, 0.83042162, 0.83508036, 0.82809224,\n",
       "        0.84206848, 0.84253436, 0.83880736, 0.83624505, 0.83438155,\n",
       "        0.84812485, 0.8390403 , 0.8371768 , 0.83018868, 0.82925693,\n",
       "        0.82098765, 0.82331703, 0.82098765, 0.82285115, 0.83240158,\n",
       "        0.82552993, 0.83414861, 0.83577918, 0.83671092, 0.83659446,\n",
       "        0.84626136, 0.84672723, 0.84160261, 0.83473096, 0.83845795,\n",
       "        0.84602842, 0.85324948, 0.8409038 , 0.83275099, 0.83636152,\n",
       "        0.8293734 , 0.82809224, 0.82226881, 0.82413231, 0.82785931,\n",
       "        0.82692756, 0.84532961, 0.83042162, 0.83647799, 0.8390403 ,\n",
       "        0.83997205, 0.8527836 , 0.83880736, 0.83508036, 0.84230142,\n",
       "        0.84346611, 0.84812485, 0.83624505, 0.83112043, 0.83589564,\n",
       "        0.84008852, 0.84067086, 0.84241789, 0.83706033, 0.83519683,\n",
       "        0.82960634, 0.83321686, 0.83391568, 0.83508036, 0.83042162,\n",
       "        0.83438155, 0.83787561, 0.83647799, 0.8334498 , 0.83496389,\n",
       "        0.83810855, 0.83508036, 0.83484743, 0.83205218, 0.83449802,\n",
       "        0.83519683, 0.8297228 , 0.82983927, 0.8315863 , 0.83135337,\n",
       "        0.82622874, 0.82902399, 0.83508036, 0.83391568, 0.83391568,\n",
       "        0.8390403 , 0.84136967, 0.84206848, 0.83624505, 0.83577918,\n",
       "        0.84323317, 0.83484743, 0.84300023, 0.83577918, 0.83624505,\n",
       "        0.82867459, 0.82983927, 0.82215234, 0.83007221, 0.82354997,\n",
       "        0.82809224, 0.83368274, 0.83624505, 0.83647799, 0.83298393,\n",
       "        0.83764267, 0.84602842, 0.83088749, 0.83088749, 0.83799208,\n",
       "        0.84020498, 0.84346611, 0.83554624, 0.83810855, 0.83473096,\n",
       "        0.81889122, 0.82413231, 0.82238528, 0.82413231, 0.82226881,\n",
       "        0.8315863 , 0.8409038 , 0.84300023, 0.84043792, 0.83612858,\n",
       "        0.84369904, 0.84719311, 0.84346611, 0.83624505, 0.82820871,\n",
       "        0.84113673, 0.84486373, 0.83647799, 0.83810855, 0.8293734 ,\n",
       "        0.82704403, 0.82692756, 0.81982297, 0.82692756, 0.83181924,\n",
       "        0.82413231, 0.83950617, 0.83601211, 0.84113673, 0.83298393,\n",
       "        0.85860703, 0.84323317, 0.84323317, 0.83636152, 0.83205218,\n",
       "        0.84230142, 0.84369904, 0.84183555, 0.83787561, 0.83205218,\n",
       "        0.82576287, 0.82669462, 0.81924062, 0.83368274, 0.83112043,\n",
       "        0.84300023, 0.8353133 , 0.84369904, 0.84416492, 0.84078733,\n",
       "        0.84626136, 0.84556254, 0.84765898, 0.83810855, 0.83216865,\n",
       "        0.8409038 , 0.83880736, 0.84067086, 0.83857442, 0.83542977,\n",
       "        0.8412532 , 0.84102027, 0.84404845, 0.83950617, 0.83612858,\n",
       "        0.83461449, 0.83740974, 0.83624505, 0.83251805, 0.83112043,\n",
       "        0.84253436, 0.84416492, 0.83647799, 0.83508036, 0.83205218,\n",
       "        0.84323317, 0.84136967, 0.83787561, 0.83368274, 0.82948987,\n",
       "        0.83251805, 0.83356627, 0.82832518, 0.83216865, 0.83414861,\n",
       "        0.83088749, 0.84532961, 0.83414861, 0.83647799, 0.82855812,\n",
       "        0.83298393, 0.84416492, 0.83973911, 0.83403215, 0.82902399,\n",
       "        0.83787561, 0.84509667, 0.84253436, 0.83496389, 0.83181924,\n",
       "        0.82785931, 0.83170277, 0.83298393, 0.82622874, 0.83135337,\n",
       "        0.8390403 , 0.84206848, 0.83740974, 0.83228512, 0.82879106,\n",
       "        0.84812485, 0.84439786, 0.8371768 , 0.83228512, 0.83379921,\n",
       "        0.85301654, 0.85161891, 0.83181924, 0.83205218, 0.83240158,\n",
       "        0.82459818, 0.82657815, 0.82518053, 0.8234335 , 0.82879106,\n",
       "        0.84136967, 0.84230142, 0.83810855, 0.82762637, 0.83589564,\n",
       "        0.84789192, 0.84742604, 0.83554624, 0.83391568, 0.83764267,\n",
       "        0.84719311, 0.84253436, 0.8427673 , 0.83647799, 0.83554624,\n",
       "        0.83205218, 0.83554624, 0.82354997, 0.82413231, 0.83018868,\n",
       "        0.83810855, 0.8390403 , 0.83228512, 0.83135337, 0.8331004 ,\n",
       "        0.84719311, 0.84300023, 0.84696017, 0.83799208, 0.82867459,\n",
       "        0.84649429, 0.85208479, 0.83438155, 0.83333333, 0.83496389,\n",
       "        0.8409038 , 0.83764267, 0.83251805, 0.8297228 , 0.83892383,\n",
       "        0.84532961, 0.84020498, 0.83321686, 0.83216865, 0.8312369 ,\n",
       "        0.85231773, 0.84952248, 0.84136967, 0.83193571, 0.83379921,\n",
       "        0.85418123, 0.84998835, 0.8334498 , 0.84067086, 0.8393897 ,\n",
       "        0.83985558, 0.84043792, 0.8412532 , 0.8409038 , 0.83612858,\n",
       "        0.83496389, 0.83787561, 0.83554624, 0.83438155, 0.83216865,\n",
       "        0.84486373, 0.83694386, 0.83787561, 0.83671092, 0.83053809,\n",
       "        0.8468437 , 0.84463079, 0.83927324, 0.83694386, 0.83170277,\n",
       "        0.83251805, 0.83461449, 0.83018868, 0.83496389, 0.83240158,\n",
       "        0.83368274, 0.84160261, 0.84020498, 0.83321686, 0.82599581,\n",
       "        0.8427673 , 0.84626136, 0.84183555, 0.83682739, 0.8312369 ,\n",
       "        0.84439786, 0.84952248, 0.83834149, 0.83659446, 0.83146983,\n",
       "        0.82634521, 0.83112043, 0.82902399, 0.82471465, 0.82389937,\n",
       "        0.83950617, 0.84649429, 0.83950617, 0.84206848, 0.83135337,\n",
       "        0.85196832, 0.85371535, 0.83694386, 0.83857442, 0.82879106,\n",
       "        0.85604472, 0.85185185, 0.85371535, 0.8353133 , 0.83018868,\n",
       "        0.82110412, 0.82413231, 0.82413231, 0.82308409, 0.82646168,\n",
       "        0.84928954, 0.85418123, 0.84160261, 0.83298393, 0.83706033,\n",
       "        0.85138598, 0.85418123, 0.84486373, 0.83321686, 0.83333333,\n",
       "        0.8565106 , 0.85324948, 0.84602842, 0.83566271, 0.83030515,\n",
       "        0.82599581, 0.83368274, 0.82506406, 0.83205218, 0.82948987,\n",
       "        0.85022129, 0.85720941, 0.83484743, 0.84043792, 0.83030515,\n",
       "        0.85324948, 0.85814116, 0.82762637, 0.8371768 , 0.83880736,\n",
       "        0.8546471 , 0.84812485, 0.8490566 , 0.84102027, 0.84439786,\n",
       "        0.8409038 , 0.84113673, 0.83205218, 0.82785931, 0.82646168,\n",
       "        0.85022129, 0.84812485, 0.84020498, 0.83566271, 0.84917307,\n",
       "        0.84859073, 0.84696017, 0.84439786, 0.84102027, 0.84824132,\n",
       "        0.85814116, 0.84509667, 0.83927324, 0.83950617, 0.84544608]),\n",
       " 'split3_test_score': array([0.91971154, 0.91802885, 0.91658654, 0.91189904, 0.90853365,\n",
       "        0.91201923, 0.90552885, 0.90384615, 0.90036058, 0.89567308,\n",
       "        0.90288462, 0.90168269, 0.89783654, 0.89543269, 0.89350962,\n",
       "        0.89951923, 0.89591346, 0.89735577, 0.89254808, 0.88870192,\n",
       "        0.92391827, 0.92379808, 0.91911058, 0.91322115, 0.90961538,\n",
       "        0.91081731, 0.903125  , 0.89807692, 0.89543269, 0.89543269,\n",
       "        0.90192308, 0.89759615, 0.89471154, 0.89326923, 0.89471154,\n",
       "        0.89879808, 0.89038462, 0.89639423, 0.89278846, 0.89182692,\n",
       "        0.92175481, 0.91598558, 0.91117788, 0.91009615, 0.91322115,\n",
       "        0.90504808, 0.89879808, 0.89927885, 0.9       , 0.90180288,\n",
       "        0.90168269, 0.89134615, 0.89302885, 0.89927885, 0.89699519,\n",
       "        0.89591346, 0.890625  , 0.89110577, 0.89543269, 0.89362981,\n",
       "        0.91153846, 0.91057692, 0.90588942, 0.91045673, 0.90865385,\n",
       "        0.89375   , 0.89807692, 0.89927885, 0.90384615, 0.90192308,\n",
       "        0.89639423, 0.89350962, 0.89158654, 0.89951923, 0.89519231,\n",
       "        0.89182692, 0.88870192, 0.89375   , 0.896875  , 0.89639423,\n",
       "        0.90408654, 0.90492788, 0.90420673, 0.91165865, 0.91033654,\n",
       "        0.9       , 0.89927885, 0.89735577, 0.90096154, 0.90132212,\n",
       "        0.89495192, 0.88918269, 0.90168269, 0.89831731, 0.89867788,\n",
       "        0.89134615, 0.88870192, 0.89471154, 0.89519231, 0.89867788,\n",
       "        0.90865385, 0.90012019, 0.90024038, 0.9078125 , 0.91225962,\n",
       "        0.89543269, 0.89495192, 0.89879808, 0.9       , 0.90516827,\n",
       "        0.88846154, 0.8875    , 0.89711538, 0.89399038, 0.89651442,\n",
       "        0.89014423, 0.8875    , 0.89807692, 0.89591346, 0.89771635,\n",
       "        0.91971154, 0.91802885, 0.91658654, 0.91189904, 0.90853365,\n",
       "        0.91201923, 0.90552885, 0.90384615, 0.90036058, 0.89567308,\n",
       "        0.90288462, 0.90168269, 0.89783654, 0.89543269, 0.89350962,\n",
       "        0.89951923, 0.89591346, 0.89735577, 0.89254808, 0.88870192,\n",
       "        0.92391827, 0.92379808, 0.91911058, 0.91322115, 0.90961538,\n",
       "        0.91081731, 0.903125  , 0.89807692, 0.89543269, 0.89543269,\n",
       "        0.90192308, 0.89759615, 0.89471154, 0.89326923, 0.89471154,\n",
       "        0.89879808, 0.89038462, 0.89639423, 0.89278846, 0.89182692,\n",
       "        0.92175481, 0.91598558, 0.91117788, 0.91009615, 0.91322115,\n",
       "        0.90504808, 0.89879808, 0.89927885, 0.9       , 0.90180288,\n",
       "        0.90168269, 0.89134615, 0.89302885, 0.89927885, 0.89699519,\n",
       "        0.89591346, 0.890625  , 0.89110577, 0.89543269, 0.89362981,\n",
       "        0.91153846, 0.91057692, 0.90588942, 0.91045673, 0.90865385,\n",
       "        0.89375   , 0.89807692, 0.89927885, 0.90384615, 0.90192308,\n",
       "        0.89639423, 0.89350962, 0.89158654, 0.89951923, 0.89519231,\n",
       "        0.89182692, 0.88870192, 0.89375   , 0.896875  , 0.89639423,\n",
       "        0.90408654, 0.90492788, 0.90420673, 0.91165865, 0.91033654,\n",
       "        0.9       , 0.89927885, 0.89735577, 0.90096154, 0.90132212,\n",
       "        0.89495192, 0.88918269, 0.90168269, 0.89831731, 0.89867788,\n",
       "        0.89134615, 0.88870192, 0.89471154, 0.89519231, 0.89867788,\n",
       "        0.90865385, 0.90012019, 0.90024038, 0.9078125 , 0.91225962,\n",
       "        0.89543269, 0.89495192, 0.89879808, 0.9       , 0.90516827,\n",
       "        0.88846154, 0.8875    , 0.89711538, 0.89399038, 0.89651442,\n",
       "        0.89014423, 0.8875    , 0.89807692, 0.89591346, 0.89771635,\n",
       "        0.92211538, 0.91862981, 0.91298077, 0.909375  , 0.909375  ,\n",
       "        0.90913462, 0.90384615, 0.90360577, 0.90024038, 0.90276442,\n",
       "        0.89831731, 0.89567308, 0.9       , 0.89711538, 0.89867788,\n",
       "        0.89639423, 0.88846154, 0.89326923, 0.89639423, 0.89651442,\n",
       "        0.90733173, 0.90805288, 0.90540865, 0.90913462, 0.90697115,\n",
       "        0.89615385, 0.89038462, 0.88990385, 0.89411058, 0.89675481,\n",
       "        0.88725962, 0.88774038, 0.884375  , 0.88557692, 0.89314904,\n",
       "        0.88774038, 0.88293269, 0.88389423, 0.88569712, 0.89290865,\n",
       "        0.89867788, 0.89747596, 0.90084135, 0.90216346, 0.89867788,\n",
       "        0.88846154, 0.88725962, 0.88305288, 0.88834135, 0.89555288,\n",
       "        0.88100962, 0.87620192, 0.88293269, 0.88713942, 0.88786058,\n",
       "        0.87956731, 0.87668269, 0.87932692, 0.88473558, 0.88737981,\n",
       "        0.89915865, 0.89254808, 0.90072115, 0.90348558, 0.89747596,\n",
       "        0.88004808, 0.88076923, 0.88016827, 0.89266827, 0.89242788,\n",
       "        0.87908654, 0.87163462, 0.88016827, 0.88569712, 0.88617788,\n",
       "        0.87283654, 0.871875  , 0.88112981, 0.88762019, 0.88377404,\n",
       "        0.89302885, 0.89459135, 0.88641827, 0.89543269, 0.89302885,\n",
       "        0.87475962, 0.88173077, 0.88689904, 0.88858173, 0.88665865,\n",
       "        0.87139423, 0.87836538, 0.88040865, 0.88497596, 0.88137019,\n",
       "        0.86802885, 0.86802885, 0.88617788, 0.88473558, 0.88401442,\n",
       "        0.89651442, 0.89254808, 0.89026442, 0.88641827, 0.89086538,\n",
       "        0.88149038, 0.88016827, 0.88221154, 0.89651442, 0.88401442,\n",
       "        0.87572115, 0.88605769, 0.8828125 , 0.8859375 , 0.89074519,\n",
       "        0.86850962, 0.87860577, 0.88329327, 0.88485577, 0.88786058,\n",
       "        0.92007212, 0.91622596, 0.91694712, 0.91430288, 0.91598558,\n",
       "        0.91370192, 0.90949519, 0.90661058, 0.9078125 , 0.90853365,\n",
       "        0.90576923, 0.89747596, 0.90060096, 0.90180288, 0.90973558,\n",
       "        0.89927885, 0.89423077, 0.90048077, 0.90228365, 0.90540865,\n",
       "        0.91322115, 0.9125    , 0.91021635, 0.90889423, 0.90973558,\n",
       "        0.89831731, 0.89711538, 0.89711538, 0.89771635, 0.90709135,\n",
       "        0.89110577, 0.89278846, 0.89423077, 0.89915865, 0.90733173,\n",
       "        0.89134615, 0.89302885, 0.89543269, 0.90036058, 0.90588942,\n",
       "        0.90240385, 0.90144231, 0.90204327, 0.90324519, 0.90733173,\n",
       "        0.90168269, 0.89399038, 0.89483173, 0.89723558, 0.90492788,\n",
       "        0.88774038, 0.88846154, 0.89350962, 0.89819712, 0.90516827,\n",
       "        0.88269231, 0.88774038, 0.89783654, 0.90348558, 0.9015625 ,\n",
       "        0.90384615, 0.90480769, 0.90228365, 0.90649038, 0.90697115,\n",
       "        0.890625  , 0.88762019, 0.89182692, 0.89170673, 0.89555288,\n",
       "        0.884375  , 0.88918269, 0.89338942, 0.90300481, 0.89795673,\n",
       "        0.88317308, 0.88533654, 0.89170673, 0.90204327, 0.90132212,\n",
       "        0.90288462, 0.8921875 , 0.90637019, 0.90012019, 0.90360577,\n",
       "        0.88918269, 0.88257212, 0.89230769, 0.90060096, 0.90012019,\n",
       "        0.87932692, 0.88509615, 0.90012019, 0.89915865, 0.90132212,\n",
       "        0.88293269, 0.88245192, 0.89483173, 0.89963942, 0.9015625 ,\n",
       "        0.89795673, 0.89435096, 0.90168269, 0.89939904, 0.90180288,\n",
       "        0.88197115, 0.88545673, 0.89302885, 0.89459135, 0.89242788,\n",
       "        0.88197115, 0.88233173, 0.89050481, 0.89362981, 0.89579327,\n",
       "        0.88389423, 0.88762019, 0.88786058, 0.89399038, 0.89507212,\n",
       "        0.91935096, 0.91754808, 0.91418269, 0.91322115, 0.91237981,\n",
       "        0.90733173, 0.90504808, 0.90336538, 0.90829327, 0.90504808,\n",
       "        0.90336538, 0.89879808, 0.89879808, 0.90709135, 0.90516827,\n",
       "        0.90024038, 0.896875  , 0.90048077, 0.90516827, 0.90516827,\n",
       "        0.91394231, 0.90661058, 0.90396635, 0.90985577, 0.90612981,\n",
       "        0.89591346, 0.89615385, 0.89783654, 0.90180288, 0.90612981,\n",
       "        0.89206731, 0.88846154, 0.89879808, 0.89795673, 0.9015625 ,\n",
       "        0.89447115, 0.89855769, 0.8984375 , 0.90324519, 0.90348558,\n",
       "        0.90961538, 0.90036058, 0.90637019, 0.90612981, 0.90600962,\n",
       "        0.89278846, 0.896875  , 0.8984375 , 0.90108173, 0.90396635,\n",
       "        0.88677885, 0.89302885, 0.89819712, 0.90180288, 0.90444712,\n",
       "        0.88701923, 0.88834135, 0.90372596, 0.90444712, 0.90973558,\n",
       "        0.89855769, 0.89819712, 0.90276442, 0.90048077, 0.90480769,\n",
       "        0.88798077, 0.88485577, 0.89483173, 0.89795673, 0.90300481,\n",
       "        0.87644231, 0.87884615, 0.89555288, 0.90757212, 0.90588942,\n",
       "        0.88461538, 0.89158654, 0.90084135, 0.90733173, 0.91069712,\n",
       "        0.89915865, 0.89675481, 0.89963942, 0.90276442, 0.90348558,\n",
       "        0.88317308, 0.88257212, 0.89651442, 0.90372596, 0.90060096,\n",
       "        0.88581731, 0.88834135, 0.8984375 , 0.90805288, 0.90805288,\n",
       "        0.88004808, 0.89074519, 0.89987981, 0.90588942, 0.90805288,\n",
       "        0.90048077, 0.88737981, 0.88762019, 0.89987981, 0.90108173,\n",
       "        0.87548077, 0.88485577, 0.89290865, 0.90504808, 0.89963942,\n",
       "        0.88076923, 0.88882212, 0.89867788, 0.90036058, 0.89194712,\n",
       "        0.88557692, 0.8953125 , 0.90084135, 0.90036058, 0.90612981,\n",
       "        0.91766827, 0.91586538, 0.91550481, 0.91165865, 0.91189904,\n",
       "        0.90697115, 0.90432692, 0.90432692, 0.90913462, 0.90516827,\n",
       "        0.90384615, 0.89879808, 0.90024038, 0.90420673, 0.90564904,\n",
       "        0.90240385, 0.89759615, 0.90012019, 0.90733173, 0.90300481,\n",
       "        0.91322115, 0.90829327, 0.90540865, 0.90805288, 0.90721154,\n",
       "        0.896875  , 0.89423077, 0.89759615, 0.90300481, 0.90420673,\n",
       "        0.89447115, 0.8875    , 0.90012019, 0.89987981, 0.90612981,\n",
       "        0.89350962, 0.89302885, 0.90180288, 0.90372596, 0.90420673,\n",
       "        0.90072115, 0.90180288, 0.90661058, 0.90252404, 0.90733173,\n",
       "        0.89567308, 0.8875    , 0.89411058, 0.90084135, 0.89963942,\n",
       "        0.88317308, 0.88221154, 0.90396635, 0.90108173, 0.90588942,\n",
       "        0.88725962, 0.88846154, 0.90492788, 0.91153846, 0.90733173,\n",
       "        0.89639423, 0.89122596, 0.89771635, 0.89555288, 0.89819712,\n",
       "        0.88725962, 0.88161058, 0.90024038, 0.89939904, 0.90516827,\n",
       "        0.87764423, 0.88257212, 0.90084135, 0.90516827, 0.90661058,\n",
       "        0.88653846, 0.89459135, 0.90012019, 0.90324519, 0.90372596,\n",
       "        0.89543269, 0.88858173, 0.89591346, 0.89555288, 0.90204327,\n",
       "        0.88221154, 0.8953125 , 0.89915865, 0.90132212, 0.90348558,\n",
       "        0.88317308, 0.88882212, 0.91045673, 0.91117788, 0.91262019,\n",
       "        0.87692308, 0.89170673, 0.89963942, 0.90348558, 0.90204327,\n",
       "        0.89855769, 0.89002404, 0.89663462, 0.89867788, 0.91021635,\n",
       "        0.88653846, 0.88425481, 0.88930288, 0.90661058, 0.90444712,\n",
       "        0.87235577, 0.88810096, 0.90204327, 0.90132212, 0.90829327,\n",
       "        0.88052885, 0.89939904, 0.89819712, 0.90396635, 0.90757212,\n",
       "        0.91778846, 0.91370192, 0.91670673, 0.91502404, 0.91514423,\n",
       "        0.90480769, 0.90192308, 0.89879808, 0.90108173, 0.90300481,\n",
       "        0.90168269, 0.90048077, 0.90504808, 0.90516827, 0.90180288,\n",
       "        0.89975962, 0.89975962, 0.90408654, 0.90564904, 0.90516827,\n",
       "        0.90961538, 0.90889423, 0.90564904, 0.90612981, 0.90637019,\n",
       "        0.90144231, 0.89759615, 0.89435096, 0.90036058, 0.90612981,\n",
       "        0.90216346, 0.89350962, 0.89699519, 0.90084135, 0.90492788,\n",
       "        0.89591346, 0.89423077, 0.90132212, 0.90252404, 0.90252404,\n",
       "        0.90096154, 0.89411058, 0.89939904, 0.90564904, 0.90757212,\n",
       "        0.88798077, 0.88834135, 0.88930288, 0.9015625 , 0.91009615,\n",
       "        0.88918269, 0.89891827, 0.90120192, 0.90612981, 0.9140625 ,\n",
       "        0.88846154, 0.8953125 , 0.90372596, 0.90180288, 0.91382212,\n",
       "        0.89879808, 0.896875  , 0.90192308, 0.90180288, 0.90432692,\n",
       "        0.88100962, 0.88882212, 0.89435096, 0.90348558, 0.90925481,\n",
       "        0.89038462, 0.89314904, 0.9015625 , 0.89723558, 0.91165865,\n",
       "        0.88990385, 0.8953125 , 0.90276442, 0.90685096, 0.90516827,\n",
       "        0.89230769, 0.89110577, 0.89951923, 0.8984375 , 0.90528846,\n",
       "        0.87596154, 0.8921875 , 0.90216346, 0.90396635, 0.90925481,\n",
       "        0.88461538, 0.89507212, 0.90420673, 0.90588942, 0.91213942,\n",
       "        0.88774038, 0.89987981, 0.90060096, 0.90516827, 0.90396635,\n",
       "        0.89266827, 0.88473558, 0.89399038, 0.89507212, 0.90240385,\n",
       "        0.87788462, 0.88762019, 0.8921875 , 0.89699519, 0.89987981,\n",
       "        0.87596154, 0.89074519, 0.89867788, 0.90877404, 0.90372596,\n",
       "        0.87716346, 0.89651442, 0.90420673, 0.90853365, 0.90733173,\n",
       "        0.91826923, 0.91730769, 0.91478365, 0.91225962, 0.91358173,\n",
       "        0.90240385, 0.90096154, 0.89987981, 0.90108173, 0.90516827,\n",
       "        0.89927885, 0.89615385, 0.90204327, 0.90324519, 0.90685096,\n",
       "        0.89543269, 0.90096154, 0.90120192, 0.90492788, 0.90685096,\n",
       "        0.90697115, 0.90745192, 0.90901442, 0.9046875 , 0.90877404,\n",
       "        0.89350962, 0.90120192, 0.89795673, 0.90060096, 0.90540865,\n",
       "        0.89735577, 0.89927885, 0.90012019, 0.90228365, 0.90901442,\n",
       "        0.89302885, 0.89567308, 0.90324519, 0.90805288, 0.90925481,\n",
       "        0.90745192, 0.90168269, 0.9       , 0.89675481, 0.90372596,\n",
       "        0.88942308, 0.89302885, 0.89459135, 0.90204327, 0.91237981,\n",
       "        0.89423077, 0.89387019, 0.90420673, 0.90420673, 0.90901442,\n",
       "        0.8875    , 0.8953125 , 0.90564904, 0.90408654, 0.90805288,\n",
       "        0.89831731, 0.89579327, 0.89242788, 0.89338942, 0.90204327,\n",
       "        0.88557692, 0.8953125 , 0.89507212, 0.90745192, 0.90637019,\n",
       "        0.88725962, 0.89579327, 0.90300481, 0.90564904, 0.90420673,\n",
       "        0.88918269, 0.89867788, 0.89723558, 0.90757212, 0.9078125 ,\n",
       "        0.89326923, 0.89651442, 0.89086538, 0.89098558, 0.89555288,\n",
       "        0.88461538, 0.89387019, 0.90252404, 0.89747596, 0.90889423,\n",
       "        0.88774038, 0.89435096, 0.89338942, 0.90300481, 0.90516827,\n",
       "        0.88197115, 0.8921875 , 0.90060096, 0.91045673, 0.90805288,\n",
       "        0.89182692, 0.88449519, 0.89194712, 0.88737981, 0.89314904,\n",
       "        0.88918269, 0.89411058, 0.88112981, 0.90396635, 0.90516827,\n",
       "        0.878125  , 0.90012019, 0.89242788, 0.89771635, 0.89915865,\n",
       "        0.88125   , 0.89170673, 0.90108173, 0.90060096, 0.90396635]),\n",
       " 'split4_test_score': array([0.87740385, 0.87596154, 0.878125  , 0.87716346, 0.87608173,\n",
       "        0.87800481, 0.884375  , 0.88052885, 0.87956731, 0.88052885,\n",
       "        0.87115385, 0.88076923, 0.87860577, 0.88173077, 0.87788462,\n",
       "        0.86274038, 0.87596154, 0.87620192, 0.88197115, 0.87908654,\n",
       "        0.87980769, 0.88137019, 0.88545673, 0.88413462, 0.88209135,\n",
       "        0.88737981, 0.88677885, 0.88605769, 0.88894231, 0.88149038,\n",
       "        0.87860577, 0.88221154, 0.88173077, 0.88581731, 0.88341346,\n",
       "        0.87475962, 0.87740385, 0.88149038, 0.88629808, 0.88485577,\n",
       "        0.88293269, 0.88689904, 0.88449519, 0.87896635, 0.88569712,\n",
       "        0.8875    , 0.88810096, 0.88413462, 0.8875    , 0.88629808,\n",
       "        0.88269231, 0.88389423, 0.88389423, 0.88365385, 0.88653846,\n",
       "        0.87043269, 0.87980769, 0.88509615, 0.88245192, 0.88653846,\n",
       "        0.88461538, 0.88762019, 0.88425481, 0.88569712, 0.88786058,\n",
       "        0.88533654, 0.88533654, 0.88317308, 0.88798077, 0.89110577,\n",
       "        0.87860577, 0.88052885, 0.88461538, 0.88774038, 0.89254808,\n",
       "        0.87355769, 0.87427885, 0.88942308, 0.89158654, 0.89278846,\n",
       "        0.88954327, 0.88834135, 0.88401442, 0.88677885, 0.88629808,\n",
       "        0.89302885, 0.88701923, 0.88846154, 0.89110577, 0.89158654,\n",
       "        0.87692308, 0.88173077, 0.88942308, 0.88389423, 0.89230769,\n",
       "        0.87980769, 0.88052885, 0.88918269, 0.88846154, 0.89447115,\n",
       "        0.88858173, 0.88737981, 0.88473558, 0.88786058, 0.88894231,\n",
       "        0.89567308, 0.8828125 , 0.88966346, 0.88966346, 0.89350962,\n",
       "        0.88173077, 0.88629808, 0.88629808, 0.88870192, 0.89206731,\n",
       "        0.87451923, 0.87956731, 0.88581731, 0.88942308, 0.89519231,\n",
       "        0.87740385, 0.87596154, 0.878125  , 0.87716346, 0.87608173,\n",
       "        0.87800481, 0.884375  , 0.88052885, 0.87956731, 0.88052885,\n",
       "        0.87115385, 0.88076923, 0.87860577, 0.88173077, 0.87788462,\n",
       "        0.86274038, 0.87596154, 0.87620192, 0.88197115, 0.87908654,\n",
       "        0.87980769, 0.88137019, 0.88545673, 0.88413462, 0.88209135,\n",
       "        0.88737981, 0.88677885, 0.88605769, 0.88894231, 0.88149038,\n",
       "        0.87860577, 0.88221154, 0.88173077, 0.88581731, 0.88341346,\n",
       "        0.87475962, 0.87740385, 0.88149038, 0.88629808, 0.88485577,\n",
       "        0.88293269, 0.88689904, 0.88449519, 0.87896635, 0.88569712,\n",
       "        0.8875    , 0.88810096, 0.88413462, 0.8875    , 0.88629808,\n",
       "        0.88269231, 0.88389423, 0.88389423, 0.88365385, 0.88653846,\n",
       "        0.87043269, 0.87980769, 0.88509615, 0.88245192, 0.88653846,\n",
       "        0.88461538, 0.88762019, 0.88425481, 0.88569712, 0.88786058,\n",
       "        0.88533654, 0.88533654, 0.88317308, 0.88798077, 0.89110577,\n",
       "        0.87860577, 0.88052885, 0.88461538, 0.88774038, 0.89254808,\n",
       "        0.87355769, 0.87427885, 0.88942308, 0.89158654, 0.89278846,\n",
       "        0.88954327, 0.88834135, 0.88401442, 0.88677885, 0.88629808,\n",
       "        0.89302885, 0.88701923, 0.88846154, 0.89110577, 0.89158654,\n",
       "        0.87692308, 0.88173077, 0.88942308, 0.88389423, 0.89230769,\n",
       "        0.87980769, 0.88052885, 0.88918269, 0.88846154, 0.89447115,\n",
       "        0.88858173, 0.88737981, 0.88473558, 0.88786058, 0.88894231,\n",
       "        0.89567308, 0.8828125 , 0.88966346, 0.88966346, 0.89350962,\n",
       "        0.88173077, 0.88629808, 0.88629808, 0.88870192, 0.89206731,\n",
       "        0.87451923, 0.87956731, 0.88581731, 0.88942308, 0.89519231,\n",
       "        0.88653846, 0.88485577, 0.88221154, 0.87992788, 0.8796875 ,\n",
       "        0.89110577, 0.89038462, 0.88581731, 0.88774038, 0.884375  ,\n",
       "        0.89278846, 0.89182692, 0.88653846, 0.88509615, 0.88389423,\n",
       "        0.89110577, 0.88966346, 0.88533654, 0.884375  , 0.88389423,\n",
       "        0.89362981, 0.89266827, 0.88810096, 0.88545673, 0.88978365,\n",
       "        0.89399038, 0.89399038, 0.8875    , 0.88557692, 0.89110577,\n",
       "        0.89182692, 0.89038462, 0.89302885, 0.89110577, 0.89399038,\n",
       "        0.89302885, 0.89831731, 0.88894231, 0.89375   , 0.89326923,\n",
       "        0.89591346, 0.88762019, 0.88930288, 0.88641827, 0.88665865,\n",
       "        0.88942308, 0.88846154, 0.88870192, 0.88701923, 0.88653846,\n",
       "        0.89014423, 0.88990385, 0.88966346, 0.88798077, 0.89038462,\n",
       "        0.89038462, 0.88581731, 0.88629808, 0.89350962, 0.89206731,\n",
       "        0.89266827, 0.89170673, 0.89026442, 0.89026442, 0.88918269,\n",
       "        0.88653846, 0.88798077, 0.88293269, 0.88461538, 0.89375   ,\n",
       "        0.88725962, 0.88701923, 0.89110577, 0.89038462, 0.88990385,\n",
       "        0.88798077, 0.88413462, 0.884375  , 0.89254808, 0.89110577,\n",
       "        0.89675481, 0.88377404, 0.88641827, 0.88401442, 0.88762019,\n",
       "        0.87884615, 0.87836538, 0.88389423, 0.878125  , 0.89350962,\n",
       "        0.88317308, 0.87932692, 0.88317308, 0.88629808, 0.89855769,\n",
       "        0.87860577, 0.87908654, 0.88485577, 0.88485577, 0.88942308,\n",
       "        0.89338942, 0.88569712, 0.88858173, 0.89074519, 0.89038462,\n",
       "        0.87403846, 0.88365385, 0.87355769, 0.87692308, 0.89302885,\n",
       "        0.87884615, 0.87596154, 0.88197115, 0.88485577, 0.88509615,\n",
       "        0.86850962, 0.88557692, 0.89423077, 0.89230769, 0.89519231,\n",
       "        0.88629808, 0.88665865, 0.88341346, 0.88341346, 0.8828125 ,\n",
       "        0.89567308, 0.89471154, 0.88942308, 0.89110577, 0.89543269,\n",
       "        0.90336538, 0.90216346, 0.89567308, 0.89423077, 0.89759615,\n",
       "        0.90360577, 0.90384615, 0.89663462, 0.89399038, 0.89855769,\n",
       "        0.89338942, 0.88858173, 0.88401442, 0.88786058, 0.88834135,\n",
       "        0.89158654, 0.88846154, 0.88629808, 0.89302885, 0.89879808,\n",
       "        0.89254808, 0.88918269, 0.89134615, 0.89591346, 0.89711538,\n",
       "        0.88942308, 0.89399038, 0.89350962, 0.89759615, 0.896875  ,\n",
       "        0.88545673, 0.88569712, 0.87992788, 0.88774038, 0.89435096,\n",
       "        0.89134615, 0.88269231, 0.88317308, 0.89038462, 0.89903846,\n",
       "        0.89278846, 0.88293269, 0.89254808, 0.88990385, 0.89879808,\n",
       "        0.89230769, 0.87860577, 0.89110577, 0.89399038, 0.90264423,\n",
       "        0.88858173, 0.88016827, 0.87536058, 0.88810096, 0.89435096,\n",
       "        0.88918269, 0.87548077, 0.88581731, 0.88629808, 0.89254808,\n",
       "        0.89158654, 0.88197115, 0.88798077, 0.89639423, 0.89495192,\n",
       "        0.88798077, 0.88293269, 0.88221154, 0.890625  , 0.89254808,\n",
       "        0.89170673, 0.87872596, 0.88161058, 0.88004808, 0.88858173,\n",
       "        0.88173077, 0.87043269, 0.87788462, 0.8875    , 0.8953125 ,\n",
       "        0.88533654, 0.87572115, 0.88413462, 0.88653846, 0.89134615,\n",
       "        0.87884615, 0.88221154, 0.88293269, 0.89206731, 0.89471154,\n",
       "        0.87704327, 0.87247596, 0.88353365, 0.88100962, 0.89435096,\n",
       "        0.88774038, 0.87956731, 0.88004808, 0.87956731, 0.89927885,\n",
       "        0.87644231, 0.88125   , 0.88485577, 0.89591346, 0.89627404,\n",
       "        0.87596154, 0.87884615, 0.88918269, 0.89182692, 0.90012019,\n",
       "        0.88677885, 0.88449519, 0.87956731, 0.88329327, 0.88317308,\n",
       "        0.89555288, 0.89411058, 0.89350962, 0.89338942, 0.90048077,\n",
       "        0.90048077, 0.90264423, 0.90072115, 0.90060096, 0.9015625 ,\n",
       "        0.90144231, 0.90336538, 0.9       , 0.90108173, 0.90132212,\n",
       "        0.88569712, 0.88569712, 0.88377404, 0.88293269, 0.89050481,\n",
       "        0.88461538, 0.89663462, 0.89254808, 0.89675481, 0.90216346,\n",
       "        0.88990385, 0.89447115, 0.89110577, 0.89723558, 0.903125  ,\n",
       "        0.89254808, 0.89158654, 0.88822115, 0.90132212, 0.90240385,\n",
       "        0.88377404, 0.8859375 , 0.87848558, 0.8875    , 0.88822115,\n",
       "        0.87956731, 0.88389423, 0.88173077, 0.88834135, 0.89831731,\n",
       "        0.88990385, 0.88894231, 0.88725962, 0.89266827, 0.90144231,\n",
       "        0.88677885, 0.88894231, 0.88485577, 0.89435096, 0.90168269,\n",
       "        0.88774038, 0.88233173, 0.88858173, 0.88161058, 0.88497596,\n",
       "        0.88125   , 0.88004808, 0.88822115, 0.88629808, 0.89122596,\n",
       "        0.88774038, 0.88125   , 0.89038462, 0.88629808, 0.89675481,\n",
       "        0.88557692, 0.88557692, 0.88557692, 0.88293269, 0.90060096,\n",
       "        0.88665865, 0.87752404, 0.88052885, 0.87560096, 0.89555288,\n",
       "        0.88125   , 0.87788462, 0.87403846, 0.88653846, 0.89915865,\n",
       "        0.88317308, 0.88293269, 0.87596154, 0.88918269, 0.89675481,\n",
       "        0.88990385, 0.87644231, 0.87427885, 0.9       , 0.8984375 ,\n",
       "        0.86502404, 0.87199519, 0.87247596, 0.88521635, 0.89314904,\n",
       "        0.88052885, 0.87355769, 0.87548077, 0.89350962, 0.89447115,\n",
       "        0.87355769, 0.87139423, 0.88822115, 0.88701923, 0.89711538,\n",
       "        0.87139423, 0.87956731, 0.87884615, 0.88653846, 0.90264423,\n",
       "        0.88822115, 0.884375  , 0.88365385, 0.89026442, 0.88641827,\n",
       "        0.90240385, 0.89951923, 0.89362981, 0.89939904, 0.89939904,\n",
       "        0.90240385, 0.90384615, 0.90096154, 0.90540865, 0.90444712,\n",
       "        0.90432692, 0.90144231, 0.90048077, 0.90444712, 0.90444712,\n",
       "        0.88834135, 0.8890625 , 0.88870192, 0.89519231, 0.89122596,\n",
       "        0.88701923, 0.88870192, 0.89230769, 0.89471154, 0.89543269,\n",
       "        0.88894231, 0.88774038, 0.89495192, 0.89591346, 0.90120192,\n",
       "        0.89879808, 0.89447115, 0.89663462, 0.89987981, 0.90048077,\n",
       "        0.89338942, 0.88209135, 0.88473558, 0.88954327, 0.89122596,\n",
       "        0.88389423, 0.88004808, 0.88389423, 0.89038462, 0.89182692,\n",
       "        0.89182692, 0.88870192, 0.88052885, 0.89471154, 0.89807692,\n",
       "        0.89326923, 0.89038462, 0.88149038, 0.89447115, 0.89567308,\n",
       "        0.88701923, 0.8796875 , 0.88725962, 0.88257212, 0.89122596,\n",
       "        0.88100962, 0.88557692, 0.88509615, 0.89675481, 0.89399038,\n",
       "        0.88004808, 0.89326923, 0.88413462, 0.890625  , 0.90360577,\n",
       "        0.89038462, 0.88870192, 0.884375  , 0.89362981, 0.89543269,\n",
       "        0.88197115, 0.87439904, 0.87692308, 0.88677885, 0.89723558,\n",
       "        0.87259615, 0.88100962, 0.87932692, 0.89447115, 0.890625  ,\n",
       "        0.88581731, 0.88221154, 0.88798077, 0.89423077, 0.8875    ,\n",
       "        0.88846154, 0.88846154, 0.88485577, 0.88557692, 0.89951923,\n",
       "        0.86983173, 0.87415865, 0.87475962, 0.88990385, 0.89675481,\n",
       "        0.87331731, 0.88461538, 0.88653846, 0.896875  , 0.89711538,\n",
       "        0.87283654, 0.87572115, 0.88425481, 0.89086538, 0.90384615,\n",
       "        0.87884615, 0.88774038, 0.88461538, 0.896875  , 0.89591346,\n",
       "        0.89026442, 0.88918269, 0.88641827, 0.89254808, 0.89326923,\n",
       "        0.90072115, 0.89735577, 0.89182692, 0.90252404, 0.896875  ,\n",
       "        0.90192308, 0.90168269, 0.89663462, 0.9015625 , 0.89855769,\n",
       "        0.90408654, 0.90288462, 0.903125  , 0.90324519, 0.89951923,\n",
       "        0.89002404, 0.88569712, 0.890625  , 0.89290865, 0.89338942,\n",
       "        0.88413462, 0.87956731, 0.89098558, 0.89639423, 0.89399038,\n",
       "        0.88245192, 0.88389423, 0.89170673, 0.89735577, 0.89783654,\n",
       "        0.88966346, 0.88798077, 0.89002404, 0.89927885, 0.89759615,\n",
       "        0.88689904, 0.88209135, 0.88269231, 0.88786058, 0.89038462,\n",
       "        0.87307692, 0.86899038, 0.88810096, 0.88076923, 0.89134615,\n",
       "        0.88485577, 0.88365385, 0.88617788, 0.89435096, 0.89471154,\n",
       "        0.88605769, 0.88725962, 0.88689904, 0.89639423, 0.89350962,\n",
       "        0.87992788, 0.88497596, 0.87860577, 0.88221154, 0.88774038,\n",
       "        0.87548077, 0.8765625 , 0.87920673, 0.89158654, 0.88894231,\n",
       "        0.88533654, 0.88125   , 0.87259615, 0.88605769, 0.89375   ,\n",
       "        0.89134615, 0.89002404, 0.88701923, 0.88966346, 0.89663462,\n",
       "        0.86983173, 0.87271635, 0.87560096, 0.88677885, 0.88858173,\n",
       "        0.87596154, 0.88317308, 0.87752404, 0.88221154, 0.89735577,\n",
       "        0.88581731, 0.88125   , 0.87632212, 0.890625  , 0.89615385,\n",
       "        0.89375   , 0.87644231, 0.88016827, 0.88221154, 0.90144231,\n",
       "        0.86322115, 0.86081731, 0.87776442, 0.88653846, 0.88545673,\n",
       "        0.86538462, 0.86442308, 0.86742788, 0.88581731, 0.89206731,\n",
       "        0.88725962, 0.87836538, 0.87247596, 0.88966346, 0.89747596,\n",
       "        0.87716346, 0.87620192, 0.87896635, 0.89230769, 0.89747596,\n",
       "        0.88882212, 0.88701923, 0.8875    , 0.89350962, 0.89435096,\n",
       "        0.896875  , 0.89987981, 0.89375   , 0.90396635, 0.896875  ,\n",
       "        0.89975962, 0.90120192, 0.90048077, 0.90228365, 0.90144231,\n",
       "        0.90600962, 0.90360577, 0.90264423, 0.90228365, 0.89903846,\n",
       "        0.88978365, 0.88882212, 0.88653846, 0.88894231, 0.89711538,\n",
       "        0.88822115, 0.88185096, 0.89122596, 0.90012019, 0.89519231,\n",
       "        0.88774038, 0.88581731, 0.89338942, 0.90084135, 0.90360577,\n",
       "        0.896875  , 0.89182692, 0.88810096, 0.90204327, 0.89639423,\n",
       "        0.87992788, 0.87584135, 0.88341346, 0.8953125 , 0.89435096,\n",
       "        0.875     , 0.87884615, 0.88112981, 0.88822115, 0.89591346,\n",
       "        0.88894231, 0.86538462, 0.88545673, 0.89519231, 0.89447115,\n",
       "        0.89543269, 0.88365385, 0.88617788, 0.89326923, 0.89423077,\n",
       "        0.87019231, 0.87572115, 0.88137019, 0.89302885, 0.89459135,\n",
       "        0.87211538, 0.87331731, 0.87367788, 0.8875    , 0.89663462,\n",
       "        0.8875    , 0.87776442, 0.87223558, 0.89194712, 0.90264423,\n",
       "        0.89014423, 0.88677885, 0.88137019, 0.890625  , 0.89038462,\n",
       "        0.86177885, 0.86213942, 0.87403846, 0.88533654, 0.89266827,\n",
       "        0.87884615, 0.87620192, 0.87415865, 0.88341346, 0.89399038,\n",
       "        0.88677885, 0.88100962, 0.88305288, 0.88461538, 0.89375   ,\n",
       "        0.88365385, 0.89134615, 0.88882212, 0.89735577, 0.89783654,\n",
       "        0.87055288, 0.86346154, 0.86394231, 0.88665865, 0.89362981,\n",
       "        0.87067308, 0.87331731, 0.87824519, 0.88461538, 0.89663462,\n",
       "        0.89375   , 0.87692308, 0.87980769, 0.88942308, 0.89663462,\n",
       "        0.89543269, 0.89567308, 0.87740385, 0.89158654, 0.88846154]),\n",
       " 'mean_test_score': array([0.86233479, 0.86236202, 0.86225002, 0.85976917, 0.85773836,\n",
       "        0.86194145, 0.8612186 , 0.85960037, 0.85703379, 0.85503074,\n",
       "        0.85764953, 0.85928563, 0.85673267, 0.85543269, 0.85264828,\n",
       "        0.85683115, 0.85749636, 0.85643527, 0.85527655, 0.85243959,\n",
       "        0.8707615 , 0.87086361, 0.87079   , 0.86850912, 0.86612145,\n",
       "        0.86795529, 0.865295  , 0.86514279, 0.86442217, 0.86088194,\n",
       "        0.86514374, 0.86523244, 0.86199706, 0.8628986 , 0.86144843,\n",
       "        0.86412221, 0.86306154, 0.86312409, 0.86275884, 0.86088045,\n",
       "        0.87300365, 0.87320213, 0.87015255, 0.86876055, 0.86856539,\n",
       "        0.8686191 , 0.86790858, 0.86660582, 0.86625844, 0.86554006,\n",
       "        0.86745036, 0.86580978, 0.86591338, 0.86613697, 0.86509248,\n",
       "        0.86403094, 0.86633904, 0.86642138, 0.8653137 , 0.86409329,\n",
       "        0.86994589, 0.87151923, 0.87004841, 0.87022541, 0.86906296,\n",
       "        0.86574044, 0.86865567, 0.86841678, 0.86861463, 0.86762044,\n",
       "        0.86291987, 0.86678067, 0.86996202, 0.86802728, 0.86805355,\n",
       "        0.86178878, 0.86610652, 0.86991204, 0.8696187 , 0.86843519,\n",
       "        0.86790369, 0.87011436, 0.86945415, 0.87042591, 0.86824842,\n",
       "        0.86626941, 0.86853378, 0.87006819, 0.86894223, 0.86852816,\n",
       "        0.86299368, 0.86713401, 0.87098612, 0.86794941, 0.86954116,\n",
       "        0.86201088, 0.86721676, 0.87042897, 0.8670266 , 0.86927504,\n",
       "        0.86727381, 0.86858782, 0.86917782, 0.87101442, 0.87102538,\n",
       "        0.86721254, 0.86766563, 0.87015446, 0.86759959, 0.87038081,\n",
       "        0.86354231, 0.86771093, 0.86963401, 0.86881426, 0.86999214,\n",
       "        0.86155137, 0.86743629, 0.87042897, 0.86990216, 0.8690872 ,\n",
       "        0.86233479, 0.86236202, 0.86225002, 0.85976917, 0.85773836,\n",
       "        0.86194145, 0.8612186 , 0.85960037, 0.85703379, 0.85503074,\n",
       "        0.85764953, 0.85928563, 0.85673267, 0.85543269, 0.85264828,\n",
       "        0.85683115, 0.85749636, 0.85643527, 0.85527655, 0.85243959,\n",
       "        0.8707615 , 0.87086361, 0.87079   , 0.86850912, 0.86612145,\n",
       "        0.86795529, 0.865295  , 0.86514279, 0.86442217, 0.86088194,\n",
       "        0.86514374, 0.86523244, 0.86199706, 0.8628986 , 0.86144843,\n",
       "        0.86412221, 0.86306154, 0.86312409, 0.86275884, 0.86088045,\n",
       "        0.87300365, 0.87320213, 0.87015255, 0.86876055, 0.86856539,\n",
       "        0.8686191 , 0.86790858, 0.86660582, 0.86625844, 0.86554006,\n",
       "        0.86745036, 0.86580978, 0.86591338, 0.86613697, 0.86509248,\n",
       "        0.86403094, 0.86633904, 0.86642138, 0.8653137 , 0.86409329,\n",
       "        0.86994589, 0.87151923, 0.87004841, 0.87022541, 0.86906296,\n",
       "        0.86574044, 0.86865567, 0.86841678, 0.86861463, 0.86762044,\n",
       "        0.86291987, 0.86678067, 0.86996202, 0.86802728, 0.86805355,\n",
       "        0.86178878, 0.86610652, 0.86991204, 0.8696187 , 0.86843519,\n",
       "        0.86790369, 0.87011436, 0.86945415, 0.87042591, 0.86824842,\n",
       "        0.86626941, 0.86853378, 0.87006819, 0.86894223, 0.86852816,\n",
       "        0.86299368, 0.86713401, 0.87098612, 0.86794941, 0.86954116,\n",
       "        0.86201088, 0.86721676, 0.87042897, 0.8670266 , 0.86927504,\n",
       "        0.86727381, 0.86858782, 0.86917782, 0.87101442, 0.87102538,\n",
       "        0.86721254, 0.86766563, 0.87015446, 0.86759959, 0.87038081,\n",
       "        0.86354231, 0.86771093, 0.86963401, 0.86881426, 0.86999214,\n",
       "        0.86155137, 0.86743629, 0.87042897, 0.86990216, 0.8690872 ,\n",
       "        0.872772  , 0.87243716, 0.86858889, 0.86540775, 0.86349617,\n",
       "        0.86866676, 0.86807047, 0.86589766, 0.86234808, 0.86087536,\n",
       "        0.86763182, 0.86649138, 0.86601955, 0.86114764, 0.86019483,\n",
       "        0.86779583, 0.86652647, 0.86433984, 0.86123188, 0.86008825,\n",
       "        0.87503042, 0.87349154, 0.87335368, 0.86981974, 0.86871504,\n",
       "        0.86948936, 0.8687548 , 0.86349381, 0.86488229, 0.86644703,\n",
       "        0.86615973, 0.86904219, 0.86647541, 0.86470062, 0.86634938,\n",
       "        0.8685927 , 0.87059894, 0.86607441, 0.86534668, 0.86671613,\n",
       "        0.87026232, 0.87027337, 0.87105004, 0.86868769, 0.86736314,\n",
       "        0.86591934, 0.86944658, 0.86744204, 0.86671898, 0.86645787,\n",
       "        0.86550494, 0.86826891, 0.86919428, 0.86932639, 0.86680674,\n",
       "        0.86610313, 0.86945784, 0.86831251, 0.86887988, 0.86718688,\n",
       "        0.87092071, 0.86845124, 0.86881906, 0.86895266, 0.86616006,\n",
       "        0.86529029, 0.86893751, 0.86650326, 0.8670803 , 0.86802058,\n",
       "        0.86645348, 0.86780345, 0.86995478, 0.86982152, 0.86821425,\n",
       "        0.86637264, 0.86592356, 0.86917364, 0.87017295, 0.86855621,\n",
       "        0.87044217, 0.86729665, 0.86532901, 0.86560291, 0.86568024,\n",
       "        0.86427811, 0.86748627, 0.86876383, 0.86750394, 0.86782028,\n",
       "        0.86470335, 0.86714526, 0.86829985, 0.86848728, 0.87068392,\n",
       "        0.86097379, 0.86801148, 0.86904484, 0.86889614, 0.86794163,\n",
       "        0.87165419, 0.86797142, 0.86811491, 0.86598475, 0.86635948,\n",
       "        0.86261287, 0.86664749, 0.86492044, 0.86747576, 0.86950136,\n",
       "        0.86605439, 0.86936168, 0.86723579, 0.86978875, 0.8710313 ,\n",
       "        0.8587246 , 0.86956144, 0.86969069, 0.86971175, 0.87009765,\n",
       "        0.87692743, 0.87746488, 0.87591185, 0.87365927, 0.87152295,\n",
       "        0.87394115, 0.8755164 , 0.87187852, 0.87143052, 0.87083287,\n",
       "        0.8756634 , 0.87478929, 0.87206636, 0.8712263 , 0.87148265,\n",
       "        0.8759042 , 0.87708569, 0.87395837, 0.87150731, 0.87006418,\n",
       "        0.87606705, 0.87533398, 0.87449954, 0.8740959 , 0.87128555,\n",
       "        0.87209677, 0.87151091, 0.87047258, 0.87100718, 0.87324404,\n",
       "        0.87289662, 0.87414406, 0.87323465, 0.87294408, 0.87346804,\n",
       "        0.87315827, 0.87599225, 0.87455995, 0.87412664, 0.87357408,\n",
       "        0.87224717, 0.87329092, 0.87279302, 0.87173077, 0.87223964,\n",
       "        0.87272177, 0.8703377 , 0.87125435, 0.87198945, 0.87581773,\n",
       "        0.86984907, 0.87123668, 0.87496136, 0.8735764 , 0.87786758,\n",
       "        0.87079315, 0.87190422, 0.87633027, 0.87484576, 0.87647144,\n",
       "        0.87462813, 0.87397632, 0.87239351, 0.87433872, 0.87391455,\n",
       "        0.87152175, 0.87055637, 0.8724401 , 0.87155717, 0.87446156,\n",
       "        0.86842315, 0.87323846, 0.87565443, 0.87816539, 0.87707696,\n",
       "        0.86895241, 0.87256836, 0.87267324, 0.87686582, 0.87803796,\n",
       "        0.87638857, 0.8701389 , 0.87189851, 0.87348066, 0.87348525,\n",
       "        0.86983608, 0.86853714, 0.8687834 , 0.87516037, 0.87783799,\n",
       "        0.86714187, 0.86982011, 0.87304011, 0.87332857, 0.8762136 ,\n",
       "        0.86726376, 0.87133474, 0.87537585, 0.87439073, 0.87679499,\n",
       "        0.87400769, 0.87099874, 0.87351194, 0.87173512, 0.87350983,\n",
       "        0.86582211, 0.86833208, 0.87029207, 0.87213898, 0.87692975,\n",
       "        0.86803489, 0.87205015, 0.87172714, 0.87675328, 0.87679222,\n",
       "        0.86520199, 0.87076357, 0.87304201, 0.87144252, 0.87408621,\n",
       "        0.87648336, 0.87627169, 0.87410057, 0.87453699, 0.87194542,\n",
       "        0.87408729, 0.87199106, 0.87255925, 0.87403325, 0.87354462,\n",
       "        0.87702826, 0.87626796, 0.87555724, 0.87616693, 0.87539228,\n",
       "        0.878692  , 0.87756496, 0.87640177, 0.87657728, 0.87613619,\n",
       "        0.87674596, 0.87416152, 0.87434286, 0.87432755, 0.87404856,\n",
       "        0.87157281, 0.87579506, 0.87531429, 0.87522517, 0.87645017,\n",
       "        0.87279302, 0.874942  , 0.87969053, 0.87902449, 0.87768569,\n",
       "        0.87669106, 0.87661725, 0.8778768 , 0.88020067, 0.87764655,\n",
       "        0.87246778, 0.87146879, 0.87099398, 0.87391348, 0.87291557,\n",
       "        0.86937914, 0.87176065, 0.87513452, 0.87658948, 0.87743786,\n",
       "        0.87075699, 0.87577462, 0.87633197, 0.87769227, 0.88100085,\n",
       "        0.87222992, 0.87530299, 0.87793531, 0.87846448, 0.880709  ,\n",
       "        0.87235396, 0.87136316, 0.87196595, 0.87263071, 0.87242214,\n",
       "        0.86959272, 0.87166234, 0.87543191, 0.87574218, 0.87638634,\n",
       "        0.86797747, 0.87186549, 0.87537991, 0.87747891, 0.88016546,\n",
       "        0.87094971, 0.87532554, 0.87505677, 0.87624529, 0.88119742,\n",
       "        0.87265379, 0.8718602 , 0.87112799, 0.87258433, 0.87695188,\n",
       "        0.86606887, 0.87142515, 0.87332791, 0.87927348, 0.87993795,\n",
       "        0.86987075, 0.87568504, 0.8757277 , 0.87915361, 0.8826713 ,\n",
       "        0.86959719, 0.87505409, 0.87302414, 0.88048839, 0.88114434,\n",
       "        0.87064114, 0.86841355, 0.86795214, 0.87362766, 0.87494212,\n",
       "        0.86564404, 0.87190165, 0.87454907, 0.88149118, 0.87808603,\n",
       "        0.86577337, 0.87352009, 0.87811123, 0.87506273, 0.87791499,\n",
       "        0.86499777, 0.87538127, 0.87410661, 0.87496658, 0.87987733,\n",
       "        0.8778329 , 0.87656333, 0.87762814, 0.87459379, 0.8714035 ,\n",
       "        0.87568818, 0.87446595, 0.87449934, 0.87740671, 0.87481982,\n",
       "        0.87643751, 0.87874455, 0.87924807, 0.87939338, 0.87855778,\n",
       "        0.8786301 , 0.87737117, 0.87810295, 0.87991924, 0.87788917,\n",
       "        0.87477791, 0.87409962, 0.87398634, 0.87613937, 0.87517783,\n",
       "        0.87014945, 0.8719604 , 0.87514825, 0.87680393, 0.87676925,\n",
       "        0.87443252, 0.87405576, 0.88018843, 0.87786352, 0.88063709,\n",
       "        0.87658406, 0.87622816, 0.88095468, 0.87928626, 0.88057412,\n",
       "        0.87314776, 0.8732941 , 0.87336357, 0.87560424, 0.87443318,\n",
       "        0.87245201, 0.87132932, 0.8747717 , 0.87848745, 0.8771379 ,\n",
       "        0.87046704, 0.87395907, 0.87597661, 0.87837598, 0.88101223,\n",
       "        0.87241138, 0.87685006, 0.87673392, 0.88223616, 0.88119262,\n",
       "        0.87049588, 0.86890433, 0.87330081, 0.86971771, 0.87304884,\n",
       "        0.87186955, 0.87393599, 0.87649428, 0.880661  , 0.88158808,\n",
       "        0.8684963 , 0.87461854, 0.87900777, 0.88126441, 0.88156342,\n",
       "        0.87010626, 0.87825195, 0.87586014, 0.88245909, 0.87921212,\n",
       "        0.87227555, 0.86911141, 0.8705934 , 0.87405312, 0.87716301,\n",
       "        0.86456508, 0.87424882, 0.87628878, 0.88084512, 0.87990294,\n",
       "        0.87164108, 0.87211961, 0.88186313, 0.88321078, 0.88091852,\n",
       "        0.86817126, 0.87492487, 0.87658224, 0.87782182, 0.8807411 ,\n",
       "        0.87068231, 0.86825699, 0.87002541, 0.87614169, 0.87856171,\n",
       "        0.87288893, 0.87345724, 0.87359361, 0.88457319, 0.87987924,\n",
       "        0.8659963 , 0.87091024, 0.88057664, 0.87858658, 0.88306613,\n",
       "        0.86780791, 0.87499136, 0.87918072, 0.88143545, 0.8808229 ,\n",
       "        0.88017568, 0.87734841, 0.87811859, 0.87863532, 0.87610151,\n",
       "        0.8773648 , 0.8761148 , 0.87489649, 0.87749264, 0.87649122,\n",
       "        0.87921638, 0.87869498, 0.87776025, 0.88040043, 0.87821793,\n",
       "        0.87963716, 0.87888431, 0.88091587, 0.88162511, 0.87899014,\n",
       "        0.87590739, 0.87557329, 0.87425597, 0.87555426, 0.87725917,\n",
       "        0.87411981, 0.87555848, 0.87563241, 0.87875464, 0.87975383,\n",
       "        0.87439338, 0.87560656, 0.87821558, 0.88120942, 0.88070196,\n",
       "        0.87472545, 0.87680103, 0.87865125, 0.88174422, 0.8820832 ,\n",
       "        0.87201423, 0.87017167, 0.87363234, 0.87507742, 0.87699177,\n",
       "        0.8698216 , 0.87254718, 0.87660819, 0.87787329, 0.87922627,\n",
       "        0.87451419, 0.87675668, 0.87827727, 0.88159627, 0.88267258,\n",
       "        0.87423765, 0.87801454, 0.87827408, 0.88155883, 0.88317611,\n",
       "        0.86972143, 0.87106854, 0.87224833, 0.87322497, 0.87590706,\n",
       "        0.86928084, 0.87534573, 0.87646789, 0.88051454, 0.882048  ,\n",
       "        0.87452462, 0.87658957, 0.87786923, 0.87923028, 0.8831409 ,\n",
       "        0.8729749 , 0.87763567, 0.8822521 , 0.88159499, 0.87953132,\n",
       "        0.86891985, 0.87056084, 0.86960592, 0.87546861, 0.87631422,\n",
       "        0.87102286, 0.87550072, 0.87664563, 0.87887546, 0.88289211,\n",
       "        0.87258176, 0.87424882, 0.87951597, 0.88278297, 0.88187761,\n",
       "        0.87567846, 0.88119035, 0.87793349, 0.87871985, 0.87892473,\n",
       "        0.87013898, 0.86676723, 0.87263654, 0.87595873, 0.8776979 ,\n",
       "        0.86766153, 0.87058103, 0.87444812, 0.88102092, 0.8785618 ,\n",
       "        0.86918278, 0.87536882, 0.87486902, 0.88018599, 0.87959748,\n",
       "        0.87075823, 0.87622974, 0.87878696, 0.88297283, 0.88218213,\n",
       "        0.87830623, 0.87719429, 0.87741457, 0.87855427, 0.87654112,\n",
       "        0.87669714, 0.87712611, 0.87596332, 0.87882932, 0.87778578,\n",
       "        0.8794676 , 0.8778732 , 0.88051413, 0.88265247, 0.88089928,\n",
       "        0.88164882, 0.88145853, 0.88103478, 0.88210385, 0.88018557,\n",
       "        0.87495776, 0.8752809 , 0.8757656 , 0.87628944, 0.87697104,\n",
       "        0.87423574, 0.87657331, 0.87938324, 0.87954791, 0.87957046,\n",
       "        0.87663255, 0.87858923, 0.88048156, 0.882195  , 0.88381451,\n",
       "        0.87596353, 0.88074715, 0.88112038, 0.8840551 , 0.88232711,\n",
       "        0.87478321, 0.87288193, 0.87422287, 0.87490543, 0.87487278,\n",
       "        0.87198548, 0.87620123, 0.87743633, 0.88020523, 0.88171456,\n",
       "        0.87650417, 0.87435271, 0.8805975 , 0.88263779, 0.88028715,\n",
       "        0.87708502, 0.87999546, 0.88503671, 0.88031905, 0.8802797 ,\n",
       "        0.86949507, 0.87079485, 0.87169416, 0.87615158, 0.87696028,\n",
       "        0.87054615, 0.87620441, 0.87520353, 0.88095638, 0.8809131 ,\n",
       "        0.87605605, 0.87723658, 0.87906392, 0.88055347, 0.88154257,\n",
       "        0.8770161 , 0.88166624, 0.87764056, 0.88288657, 0.87929933,\n",
       "        0.86838674, 0.86985329, 0.87187199, 0.8757862 , 0.8759764 ,\n",
       "        0.87277151, 0.87798368, 0.87450728, 0.87907563, 0.87914203,\n",
       "        0.87465693, 0.87913454, 0.87702152, 0.87972297, 0.88244846,\n",
       "        0.87404277, 0.87941812, 0.88353102, 0.88564822, 0.88249165,\n",
       "        0.87222905, 0.86929785, 0.87207236, 0.87556241, 0.87745826,\n",
       "        0.87321504, 0.87694237, 0.87197749, 0.87998516, 0.88381737,\n",
       "        0.87151919, 0.87476575, 0.87659946, 0.88137384, 0.88042583,\n",
       "        0.87709289, 0.8781375 , 0.87696429, 0.88058984, 0.87816878]),\n",
       " 'std_test_score': array([0.04305061, 0.04126119, 0.04106539, 0.0416154 , 0.04168983,\n",
       "        0.04370459, 0.04243431, 0.04035554, 0.04136217, 0.04210703,\n",
       "        0.0407114 , 0.04074322, 0.03963937, 0.04141499, 0.04159786,\n",
       "        0.03874911, 0.038081  , 0.03920674, 0.04140716, 0.0407652 ,\n",
       "        0.03780253, 0.03892332, 0.03779317, 0.03693296, 0.03841884,\n",
       "        0.04049082, 0.03623259, 0.03459746, 0.03580075, 0.0365038 ,\n",
       "        0.03691368, 0.03435159, 0.03391391, 0.03545248, 0.03615368,\n",
       "        0.03549921, 0.03129713, 0.0332287 , 0.03578248, 0.0353297 ,\n",
       "        0.03598884, 0.03531355, 0.03482677, 0.03462609, 0.03791846,\n",
       "        0.03678321, 0.03500773, 0.03489336, 0.03393731, 0.03589907,\n",
       "        0.03525101, 0.03237686, 0.03145149, 0.03316128, 0.034316  ,\n",
       "        0.03348368, 0.03007566, 0.03080591, 0.03220008, 0.03354216,\n",
       "        0.03444848, 0.03442746, 0.03341104, 0.0350442 , 0.03599418,\n",
       "        0.03645303, 0.03325401, 0.03075063, 0.0349884 , 0.03669014,\n",
       "        0.03409405, 0.03154658, 0.02846675, 0.0339982 , 0.034453  ,\n",
       "        0.03210217, 0.0291905 , 0.02876529, 0.03212831, 0.03403842,\n",
       "        0.03377805, 0.03339778, 0.03210788, 0.03555722, 0.03670471,\n",
       "        0.03901396, 0.03411351, 0.03029687, 0.03426406, 0.03552669,\n",
       "        0.0329678 , 0.03150209, 0.03231479, 0.03185429, 0.03627438,\n",
       "        0.03350482, 0.02680201, 0.02930239, 0.03368974, 0.03727685,\n",
       "        0.03733441, 0.03432428, 0.03020359, 0.03212326, 0.035587  ,\n",
       "        0.03738931, 0.03322583, 0.03183398, 0.03388148, 0.03775901,\n",
       "        0.03172872, 0.03080908, 0.02860293, 0.03323078, 0.03563996,\n",
       "        0.03060393, 0.02714551, 0.02846823, 0.03185187, 0.03843876,\n",
       "        0.04305061, 0.04126119, 0.04106539, 0.0416154 , 0.04168983,\n",
       "        0.04370459, 0.04243431, 0.04035554, 0.04136217, 0.04210703,\n",
       "        0.0407114 , 0.04074322, 0.03963937, 0.04141499, 0.04159786,\n",
       "        0.03874911, 0.038081  , 0.03920674, 0.04140716, 0.0407652 ,\n",
       "        0.03780253, 0.03892332, 0.03779317, 0.03693296, 0.03841884,\n",
       "        0.04049082, 0.03623259, 0.03459746, 0.03580075, 0.0365038 ,\n",
       "        0.03691368, 0.03435159, 0.03391391, 0.03545248, 0.03615368,\n",
       "        0.03549921, 0.03129713, 0.0332287 , 0.03578248, 0.0353297 ,\n",
       "        0.03598884, 0.03531355, 0.03482677, 0.03462609, 0.03791846,\n",
       "        0.03678321, 0.03500773, 0.03489336, 0.03393731, 0.03589907,\n",
       "        0.03525101, 0.03237686, 0.03145149, 0.03316128, 0.034316  ,\n",
       "        0.03348368, 0.03007566, 0.03080591, 0.03220008, 0.03354216,\n",
       "        0.03444848, 0.03442746, 0.03341104, 0.0350442 , 0.03599418,\n",
       "        0.03645303, 0.03325401, 0.03075063, 0.0349884 , 0.03669014,\n",
       "        0.03409405, 0.03154658, 0.02846675, 0.0339982 , 0.034453  ,\n",
       "        0.03210217, 0.0291905 , 0.02876529, 0.03212831, 0.03403842,\n",
       "        0.03377805, 0.03339778, 0.03210788, 0.03555722, 0.03670471,\n",
       "        0.03901396, 0.03411351, 0.03029687, 0.03426406, 0.03552669,\n",
       "        0.0329678 , 0.03150209, 0.03231479, 0.03185429, 0.03627438,\n",
       "        0.03350482, 0.02680201, 0.02930239, 0.03368974, 0.03727685,\n",
       "        0.03733441, 0.03432428, 0.03020359, 0.03212326, 0.035587  ,\n",
       "        0.03738931, 0.03322583, 0.03183398, 0.03388148, 0.03775901,\n",
       "        0.03172872, 0.03080908, 0.02860293, 0.03323078, 0.03563996,\n",
       "        0.03060393, 0.02714551, 0.02846823, 0.03185187, 0.03843876,\n",
       "        0.04216455, 0.04019132, 0.03841026, 0.03914928, 0.04037744,\n",
       "        0.04300985, 0.04045076, 0.03884121, 0.04115919, 0.04258617,\n",
       "        0.0408591 , 0.03997344, 0.03845141, 0.04127104, 0.04250976,\n",
       "        0.03982686, 0.03606512, 0.03795065, 0.04111156, 0.04206   ,\n",
       "        0.03503884, 0.03564682, 0.03242149, 0.03624015, 0.03671608,\n",
       "        0.03840147, 0.03553169, 0.03788106, 0.03710901, 0.03790258,\n",
       "        0.03902818, 0.03718456, 0.03493737, 0.03728395, 0.03859198,\n",
       "        0.03562387, 0.03630692, 0.03368025, 0.03617587, 0.03834406,\n",
       "        0.03759494, 0.03566178, 0.03377004, 0.03443954, 0.03512837,\n",
       "        0.04219291, 0.03677767, 0.03394321, 0.03605136, 0.03695671,\n",
       "        0.03652476, 0.03307781, 0.03360011, 0.03455178, 0.03528686,\n",
       "        0.03350524, 0.03145407, 0.03221401, 0.03579099, 0.0348487 ,\n",
       "        0.03750532, 0.03578631, 0.0355496 , 0.03755558, 0.03591532,\n",
       "        0.03780971, 0.0358298 , 0.03434576, 0.03701378, 0.03970097,\n",
       "        0.03102249, 0.03195409, 0.03296798, 0.03548314, 0.03687308,\n",
       "        0.02817915, 0.03178527, 0.03076109, 0.03440506, 0.03577702,\n",
       "        0.03823747, 0.03661871, 0.03565207, 0.03636776, 0.03620979,\n",
       "        0.03178487, 0.03377183, 0.03425694, 0.03612256, 0.03862135,\n",
       "        0.02721706, 0.03219435, 0.03484694, 0.03422309, 0.03624831,\n",
       "        0.03184537, 0.0286562 , 0.03230852, 0.03103766, 0.03528062,\n",
       "        0.03703146, 0.03744876, 0.0350284 , 0.03700224, 0.03645628,\n",
       "        0.03716588, 0.03449133, 0.03608125, 0.03660171, 0.03635943,\n",
       "        0.0314789 , 0.03026506, 0.03205418, 0.03315262, 0.03573381,\n",
       "        0.03109822, 0.03338356, 0.03364061, 0.0349833 , 0.03658687,\n",
       "        0.0388582 , 0.03770649, 0.037828  , 0.03743548, 0.03852262,\n",
       "        0.04195733, 0.03843739, 0.03801319, 0.03891407, 0.04232582,\n",
       "        0.04248549, 0.03934251, 0.03837795, 0.03864615, 0.04294707,\n",
       "        0.03955848, 0.03788344, 0.0384199 , 0.03871241, 0.04232502,\n",
       "        0.03539008, 0.03480859, 0.034673  , 0.0351582 , 0.03671359,\n",
       "        0.03610587, 0.03559025, 0.03902779, 0.03748207, 0.04078169,\n",
       "        0.03576862, 0.03382101, 0.03548344, 0.03867906, 0.04135945,\n",
       "        0.0325011 , 0.03422169, 0.0357563 , 0.03743794, 0.03887681,\n",
       "        0.03794322, 0.03594624, 0.03332884, 0.03488503, 0.03730766,\n",
       "        0.03951905, 0.03474902, 0.03485617, 0.03823823, 0.04069735,\n",
       "        0.03129561, 0.03127451, 0.03542544, 0.03773274, 0.04077929,\n",
       "        0.02926671, 0.03084046, 0.03554744, 0.03867316, 0.03972883,\n",
       "        0.03633987, 0.03407417, 0.03338201, 0.03510464, 0.0394609 ,\n",
       "        0.03219723, 0.03338075, 0.03213321, 0.04008478, 0.03846226,\n",
       "        0.03219806, 0.03183669, 0.03306491, 0.03955533, 0.03726449,\n",
       "        0.02863304, 0.03004256, 0.03355235, 0.03761748, 0.03670694,\n",
       "        0.03525568, 0.03444912, 0.03714728, 0.03746119, 0.04129555,\n",
       "        0.03269599, 0.03433417, 0.03657615, 0.03824957, 0.03825662,\n",
       "        0.02707479, 0.02936688, 0.03617914, 0.04067757, 0.04025778,\n",
       "        0.02673926, 0.02813166, 0.02983528, 0.03964587, 0.03924314,\n",
       "        0.03339183, 0.03423561, 0.0370434 , 0.03666558, 0.03960396,\n",
       "        0.03390836, 0.03217793, 0.0389194 , 0.03538583, 0.03839728,\n",
       "        0.02291768, 0.02420819, 0.03476274, 0.03697847, 0.03763082,\n",
       "        0.02642455, 0.02802908, 0.0318723 , 0.03999633, 0.0428416 ,\n",
       "        0.03540284, 0.03352551, 0.03258258, 0.03292464, 0.03537878,\n",
       "        0.03884466, 0.03946142, 0.03623128, 0.03697333, 0.03866323,\n",
       "        0.03729486, 0.03735516, 0.03738382, 0.03841482, 0.03721354,\n",
       "        0.03726357, 0.03494007, 0.03659123, 0.03762609, 0.03652022,\n",
       "        0.03667908, 0.03420426, 0.03404086, 0.03408887, 0.03530106,\n",
       "        0.03544348, 0.03619262, 0.03541095, 0.03636123, 0.0392288 ,\n",
       "        0.03257971, 0.03342686, 0.03086657, 0.03496185, 0.03725593,\n",
       "        0.03057354, 0.03395348, 0.03035331, 0.0357054 , 0.03832881,\n",
       "        0.03702844, 0.0352325 , 0.03392279, 0.03642464, 0.03682812,\n",
       "        0.03583408, 0.0354795 , 0.03095296, 0.03466864, 0.03828257,\n",
       "        0.03100094, 0.03082298, 0.03016233, 0.0343294 , 0.03577751,\n",
       "        0.02921007, 0.02922658, 0.03129347, 0.03752985, 0.03758979,\n",
       "        0.0361835 , 0.03468249, 0.03663225, 0.03622836, 0.03735241,\n",
       "        0.03096715, 0.0283585 , 0.0355867 , 0.03356148, 0.03522268,\n",
       "        0.02919996, 0.02632954, 0.03300815, 0.03645602, 0.03631619,\n",
       "        0.02496846, 0.02825586, 0.03247925, 0.03503992, 0.03730299,\n",
       "        0.03616661, 0.03567013, 0.03870545, 0.03707421, 0.03679435,\n",
       "        0.03292498, 0.02748947, 0.03087869, 0.03407821, 0.03346575,\n",
       "        0.03055687, 0.02516503, 0.03106104, 0.03356576, 0.0368465 ,\n",
       "        0.02953027, 0.02459367, 0.02927906, 0.03499057, 0.03596423,\n",
       "        0.03276898, 0.03160857, 0.03563565, 0.0391415 , 0.03815115,\n",
       "        0.02756412, 0.02557069, 0.03238208, 0.03057872, 0.03195682,\n",
       "        0.02987313, 0.0199031 , 0.03111935, 0.03295422, 0.03271927,\n",
       "        0.03227383, 0.02748897, 0.03051975, 0.03544045, 0.0362592 ,\n",
       "        0.03753132, 0.03522722, 0.03414559, 0.03533585, 0.03719618,\n",
       "        0.04069916, 0.03893418, 0.03767232, 0.03808939, 0.03697929,\n",
       "        0.03851366, 0.03526462, 0.03801168, 0.03963393, 0.03658171,\n",
       "        0.03675127, 0.03611994, 0.03678171, 0.03957813, 0.03732906,\n",
       "        0.03781913, 0.0356516 , 0.03528976, 0.0366864 , 0.03605243,\n",
       "        0.03697345, 0.03441648, 0.03578928, 0.03706385, 0.03693039,\n",
       "        0.03550788, 0.03026165, 0.03426092, 0.03734127, 0.03842715,\n",
       "        0.03550827, 0.03398882, 0.03487397, 0.03769777, 0.03810758,\n",
       "        0.03936352, 0.03463665, 0.03871275, 0.03610843, 0.04035916,\n",
       "        0.03541095, 0.02783146, 0.02966956, 0.03587923, 0.03486125,\n",
       "        0.0328632 , 0.02909449, 0.03431697, 0.03651805, 0.0368313 ,\n",
       "        0.03232555, 0.02997644, 0.03377149, 0.0371963 , 0.03799035,\n",
       "        0.03903389, 0.03642986, 0.03694185, 0.03752285, 0.03923187,\n",
       "        0.03217397, 0.02656857, 0.03197605, 0.03345   , 0.03441146,\n",
       "        0.02922024, 0.02703468, 0.032736  , 0.03624911, 0.03772477,\n",
       "        0.03174026, 0.02911694, 0.03516357, 0.03576435, 0.03502246,\n",
       "        0.03386483, 0.03404714, 0.03693843, 0.03666887, 0.03902918,\n",
       "        0.03377661, 0.02885155, 0.03079044, 0.03374602, 0.03645845,\n",
       "        0.02949101, 0.02627584, 0.02882309, 0.03618952, 0.03625838,\n",
       "        0.02979488, 0.02892687, 0.0280181 , 0.03106941, 0.0357935 ,\n",
       "        0.0345632 , 0.03521012, 0.03832562, 0.03616724, 0.04053171,\n",
       "        0.02383251, 0.02763884, 0.0262137 , 0.02997283, 0.03117037,\n",
       "        0.02795524, 0.02450948, 0.02573318, 0.02897454, 0.03765574,\n",
       "        0.02650071, 0.03246273, 0.02693756, 0.03118001, 0.03464235,\n",
       "        0.03708092, 0.03546836, 0.03353365, 0.03477865, 0.03628856,\n",
       "        0.03803245, 0.03655659, 0.03412216, 0.03676349, 0.03569299,\n",
       "        0.03437506, 0.03508589, 0.03583709, 0.03660631, 0.03553643,\n",
       "        0.03534964, 0.03316619, 0.03569908, 0.03803257, 0.03644731,\n",
       "        0.0377218 , 0.03520848, 0.03665172, 0.03656197, 0.03628445,\n",
       "        0.0340205 , 0.02854353, 0.03176023, 0.03488815, 0.0381298 ,\n",
       "        0.03434923, 0.0305538 , 0.03153077, 0.03501025, 0.03794451,\n",
       "        0.03463964, 0.03064642, 0.03080708, 0.03659969, 0.03630919,\n",
       "        0.03797386, 0.0326356 , 0.03376621, 0.03668366, 0.03723698,\n",
       "        0.02973878, 0.02803955, 0.03181263, 0.03333065, 0.03625532,\n",
       "        0.02727629, 0.02932688, 0.03180017, 0.03584169, 0.0356276 ,\n",
       "        0.02602311, 0.02533146, 0.03315775, 0.03529743, 0.03685045,\n",
       "        0.0344914 , 0.03215999, 0.03459499, 0.03549474, 0.03620134,\n",
       "        0.02900338, 0.02902699, 0.02735321, 0.0344162 , 0.03223049,\n",
       "        0.02836631, 0.02417199, 0.02981298, 0.03295289, 0.03455618,\n",
       "        0.0314909 , 0.0256377 , 0.02940253, 0.03523592, 0.03381028,\n",
       "        0.03050381, 0.0299356 , 0.03574612, 0.03781657, 0.0372683 ,\n",
       "        0.0276218 , 0.02657743, 0.03221921, 0.03275047, 0.03507093,\n",
       "        0.02483535, 0.02754646, 0.02565366, 0.0317065 , 0.03655039,\n",
       "        0.0247696 , 0.02556565, 0.02955493, 0.03125185, 0.03487567,\n",
       "        0.02608041, 0.02781476, 0.0307952 , 0.03420899, 0.02976098,\n",
       "        0.02540621, 0.02416677, 0.02930393, 0.03141835, 0.03192757,\n",
       "        0.02486059, 0.02214678, 0.02390996, 0.0308542 , 0.03328223,\n",
       "        0.0218784 , 0.02276331, 0.02861076, 0.02997146, 0.03134237,\n",
       "        0.0362556 , 0.03559588, 0.03345395, 0.03467306, 0.03669073,\n",
       "        0.03595111, 0.03351654, 0.03237405, 0.03415619, 0.03441437,\n",
       "        0.03294305, 0.03348911, 0.03416302, 0.03532949, 0.03621595,\n",
       "        0.03312767, 0.03236714, 0.03346364, 0.0353174 , 0.03521236,\n",
       "        0.03660497, 0.0345161 , 0.03546934, 0.03580103, 0.03573605,\n",
       "        0.02947024, 0.02903514, 0.02914356, 0.03563004, 0.03819323,\n",
       "        0.03185222, 0.02863712, 0.02969648, 0.03546586, 0.03845723,\n",
       "        0.02885409, 0.02840655, 0.03324037, 0.03624025, 0.03740699,\n",
       "        0.03784298, 0.03218982, 0.03501355, 0.03648254, 0.03823667,\n",
       "        0.02783504, 0.02269   , 0.02873043, 0.03312868, 0.03849002,\n",
       "        0.02514332, 0.0237564 , 0.03365947, 0.03299773, 0.03787563,\n",
       "        0.02532635, 0.02260505, 0.02897052, 0.03390254, 0.03707181,\n",
       "        0.03478929, 0.0338706 , 0.03369339, 0.03725384, 0.0383029 ,\n",
       "        0.02448004, 0.02161272, 0.0305582 , 0.03336745, 0.03331104,\n",
       "        0.02620093, 0.02447808, 0.02792342, 0.03391505, 0.03596156,\n",
       "        0.02517798, 0.02399751, 0.02800826, 0.03291843, 0.0353941 ,\n",
       "        0.03317442, 0.03172115, 0.03491642, 0.03348702, 0.03804622,\n",
       "        0.02613075, 0.02388125, 0.03090264, 0.02720116, 0.03732841,\n",
       "        0.02415863, 0.02224991, 0.03006285, 0.03050324, 0.03225496,\n",
       "        0.02316455, 0.02511761, 0.02456177, 0.02906807, 0.03120158,\n",
       "        0.02663515, 0.02854469, 0.03024662, 0.03426989, 0.034941  ,\n",
       "        0.02223079, 0.02401442, 0.02428302, 0.03146538, 0.02796303,\n",
       "        0.02768802, 0.02474869, 0.02443942, 0.02721351, 0.03038649,\n",
       "        0.02350884, 0.02320431, 0.02664189, 0.02783957, 0.02954113]),\n",
       " 'rank_test_score': array([902, 899, 904, 930, 937, 910, 919, 932, 943, 955, 939, 934, 947,\n",
       "        951, 957, 945, 941, 949, 953, 959, 560, 552, 557, 708, 821, 740,\n",
       "        854, 862, 872, 923, 860, 857, 908, 894, 916, 876, 888, 886, 896,\n",
       "        925, 447, 439, 595, 684, 700, 692, 746, 797, 815, 846, 767, 838,\n",
       "        834, 819, 864, 880, 811, 806, 852, 878, 615, 517, 607, 589, 666,\n",
       "        841, 690, 716, 694, 760, 892, 791, 612, 734, 731, 912, 823, 617,\n",
       "        636, 713, 748, 600, 648, 581, 725, 813, 704, 604, 672, 706, 890,\n",
       "        785, 547, 743, 642, 906, 778, 579, 788, 656, 774, 698, 660, 542,\n",
       "        539, 780, 756, 593, 762, 583, 882, 754, 634, 680, 610, 914, 770,\n",
       "        577, 619, 664, 902, 899, 904, 930, 937, 910, 919, 932, 943, 955,\n",
       "        939, 934, 947, 951, 957, 945, 941, 949, 953, 959, 560, 552, 557,\n",
       "        708, 821, 740, 854, 862, 872, 923, 860, 857, 908, 894, 916, 876,\n",
       "        888, 886, 896, 925, 447, 439, 595, 684, 700, 692, 746, 797, 815,\n",
       "        846, 767, 838, 834, 819, 864, 880, 811, 806, 852, 878, 615, 517,\n",
       "        607, 589, 666, 841, 690, 716, 694, 760, 892, 791, 612, 734, 731,\n",
       "        912, 823, 617, 636, 713, 748, 600, 648, 581, 725, 813, 704, 604,\n",
       "        672, 706, 890, 785, 547, 743, 642, 906, 778, 579, 788, 656, 774,\n",
       "        698, 660, 542, 539, 780, 756, 593, 762, 583, 882, 754, 634, 680,\n",
       "        610, 914, 770, 577, 619, 664, 457, 472, 697, 849, 884, 689, 730,\n",
       "        836, 901, 927, 759, 801, 829, 921, 928, 753, 799, 874, 918, 929,\n",
       "        339, 422, 428, 628, 687, 646, 686, 885, 868, 805, 818, 669, 802,\n",
       "        870, 810, 696, 567, 826, 850, 795, 588, 587, 537, 688, 772, 833,\n",
       "        650, 769, 794, 803, 848, 723, 658, 653, 790, 825, 647, 721, 678,\n",
       "        782, 550, 712, 679, 670, 817, 856, 674, 800, 787, 736, 804, 752,\n",
       "        614, 626, 727, 808, 832, 662, 591, 702, 576, 773, 851, 845, 843,\n",
       "        875, 765, 683, 764, 750, 869, 783, 722, 711, 564, 922, 737, 668,\n",
       "        677, 745, 511, 739, 729, 831, 809, 898, 796, 867, 766, 644, 828,\n",
       "        652, 777, 629, 538, 936, 641, 633, 632, 603, 223, 194, 289, 412,\n",
       "        515, 408, 314, 500, 525, 554, 304, 353, 487, 534, 522, 292, 210,\n",
       "        407, 521, 606, 281, 324, 369, 395, 531, 485, 520, 574, 544, 434,\n",
       "        452, 389, 436, 450, 425, 441, 283, 363, 390, 417, 479, 433, 455,\n",
       "        507, 480, 459, 585, 532, 491, 294, 623, 533, 342, 416, 177, 556,\n",
       "        497, 262, 351, 254, 360, 405, 475, 380, 410, 516, 571, 471, 514,\n",
       "        372, 715, 435, 305, 160, 212, 671, 466, 460, 224, 166, 259, 599,\n",
       "        499, 424, 423, 624, 703, 682, 332, 179, 784, 627, 445, 429, 271,\n",
       "        776, 529, 321, 377, 228, 403, 545, 420, 506, 421, 837, 720, 586,\n",
       "        483, 222, 733, 488, 508, 232, 229, 859, 559, 444, 524, 397, 253,\n",
       "        266, 393, 365, 496, 396, 490, 467, 402, 418, 213, 267, 312, 274,\n",
       "        318, 140, 191, 258, 246, 278, 233, 388, 379, 381, 400, 513, 295,\n",
       "        326, 329, 256, 455, 345, 103, 128, 186, 236, 239, 173,  88, 187,\n",
       "        469, 523, 546, 411, 451, 651, 505, 334, 243, 196, 563, 297, 261,\n",
       "        185,  58, 481, 327, 169, 151,  69, 476, 528, 494, 463, 473, 640,\n",
       "        510, 317, 299, 260, 738, 503, 320, 193,  93, 549, 325, 337, 268,\n",
       "         50, 461, 504, 535, 464, 220, 827, 526, 430, 116,  96, 621, 302,\n",
       "        300, 123,  17, 639, 338, 446,  80,  53, 566, 718, 742, 414, 344,\n",
       "        844, 498, 364,  44, 165, 840, 419, 163, 336, 171, 866, 319, 392,\n",
       "        341, 100, 180, 248, 190, 362, 527, 301, 371, 370, 199, 352, 257,\n",
       "        137, 117, 112, 148, 143, 200, 164,  97, 172, 355, 394, 404, 277,\n",
       "        331, 597, 495, 333, 226, 230, 375, 398,  89, 178,  72, 244, 270,\n",
       "         60, 115,  76, 442, 432, 427, 308, 374, 470, 530, 356, 150, 207,\n",
       "        575, 406, 284, 152,  57, 474, 225, 234,  25,  51, 573, 676, 431,\n",
       "        631, 443, 502, 409, 251,  71,  40, 710, 361, 129,  48,  41, 602,\n",
       "        156, 293,  21, 121, 477, 663, 568, 399, 206, 871, 383, 265,  65,\n",
       "         98, 512, 484,  32,   8,  61, 728, 346, 245, 181,  68, 565, 724,\n",
       "        609, 276, 147, 453, 426, 415,   3,  99, 830, 551,  75, 145,  11,\n",
       "        751, 340, 122,  46,  66,  92, 202, 162, 142, 280, 201, 279, 348,\n",
       "        192, 252, 120, 139, 183,  83, 157, 104, 132,  62,  37, 130, 290,\n",
       "        309, 382, 313, 203, 391, 311, 306, 136, 101, 376, 307, 158,  49,\n",
       "         70, 358, 227, 141,  33,  29, 489, 592, 413, 335, 216, 625, 468,\n",
       "        240, 174, 119, 367, 231, 154,  38,  16, 385, 167, 155,  42,   9,\n",
       "        630, 536, 478, 437, 291, 655, 323, 255,  78,  30, 366, 242, 176,\n",
       "        118,  10, 449, 189,  24,  39, 108, 675, 570, 638, 316, 263, 541,\n",
       "        315, 237, 133,  13, 465, 383, 109,  15,  31, 303,  52, 170, 138,\n",
       "        131, 598, 793, 462, 288, 184, 758, 569, 373,  56, 146, 659, 322,\n",
       "        350,  90, 105, 562, 269, 135,  12,  27, 153, 205, 198, 149, 249,\n",
       "        235, 208, 287, 134, 182, 110, 175,  79,  18,  64,  36,  45,  55,\n",
       "         28,  91, 343, 328, 298, 264, 217, 386, 247, 113, 107, 106, 238,\n",
       "        144,  81,  26,   6, 286,  67,  54,   4,  23, 354, 454, 387, 347,\n",
       "        349, 492, 273, 197,  87,  34, 250, 378,  73,  19,  85, 211,  94,\n",
       "          2,  84,  86, 645, 555, 509, 275, 219, 572, 272, 330,  59,  63,\n",
       "        282, 204, 127,  77,  43, 215,  35, 188,  14, 114, 719, 622, 501,\n",
       "        296, 285, 458, 168, 368, 126, 124, 359, 125, 214, 102,  22, 401,\n",
       "        111,   7,   1,  20, 482, 654, 486, 310, 195, 438, 221, 493,  95,\n",
       "          5, 519, 357, 241,  47,  82, 209, 161, 218,  74, 159], dtype=int32)}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'learning_rate': 0.25,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 4}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8856482209410668"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784753\n",
      "F1: 0.671233\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa8187d5320>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 40,\n",
       " 'Q': 20,\n",
       " 'Fare': 161,\n",
       " 'Parch': 64,\n",
       " 'Pclass': 51,\n",
       " 'SibSp': 82,\n",
       " 'Age': 121,\n",
       " 'male': 32,\n",
       " 'youngin': 32}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bytree=0.8, gamma=0, learning_rate=0.25,\n",
      "              max_delta_step=0, max_depth=8, min_child_weight=4, missing=nan,\n",
      "              n_estimators=140, n_jobs=1, nthread=4,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
      "              subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
