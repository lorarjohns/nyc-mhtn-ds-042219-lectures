{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 2, \n",
    "                           alpha = 1, \n",
    "                           n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=0.3, gamma=0, learning_rate=0.1,\n",
       "              max_delta_step=0, max_depth=2, min_child_weight=1, missing=None,\n",
       "              n_estimators=100, n_jobs=1, nthread=None,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "              subsample=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 2, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.685093</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.671230</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.675102</td>\n",
       "      <td>0.008816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.659008</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.663125</td>\n",
       "      <td>0.007590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.644336</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.650664</td>\n",
       "      <td>0.011789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.636066</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.643942</td>\n",
       "      <td>0.013259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.623169</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.012165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.602033</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.610209</td>\n",
       "      <td>0.013601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.591650</td>\n",
       "      <td>0.015947</td>\n",
       "      <td>0.599721</td>\n",
       "      <td>0.021180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.585461</td>\n",
       "      <td>0.019497</td>\n",
       "      <td>0.593338</td>\n",
       "      <td>0.024632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.578619</td>\n",
       "      <td>0.018259</td>\n",
       "      <td>0.586940</td>\n",
       "      <td>0.022570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.570801</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.580262</td>\n",
       "      <td>0.021636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.560062</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.569363</td>\n",
       "      <td>0.024691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.556937</td>\n",
       "      <td>0.020256</td>\n",
       "      <td>0.566690</td>\n",
       "      <td>0.023334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.554872</td>\n",
       "      <td>0.019017</td>\n",
       "      <td>0.564808</td>\n",
       "      <td>0.022910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.546951</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.557979</td>\n",
       "      <td>0.020297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.541852</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.553449</td>\n",
       "      <td>0.020814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.536363</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.548007</td>\n",
       "      <td>0.021016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.529293</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.541409</td>\n",
       "      <td>0.021934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.523798</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>0.536885</td>\n",
       "      <td>0.024915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.520973</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.534507</td>\n",
       "      <td>0.025665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.516376</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.025853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.513121</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.527367</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.509301</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.523904</td>\n",
       "      <td>0.027239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.502340</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>0.517755</td>\n",
       "      <td>0.029960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.495447</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.510592</td>\n",
       "      <td>0.026663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.489835</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>0.505060</td>\n",
       "      <td>0.024653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.488421</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.504394</td>\n",
       "      <td>0.024912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.485823</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.501844</td>\n",
       "      <td>0.026256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.480789</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.498244</td>\n",
       "      <td>0.023527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.479223</td>\n",
       "      <td>0.012897</td>\n",
       "      <td>0.496588</td>\n",
       "      <td>0.025181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.401454</td>\n",
       "      <td>0.010871</td>\n",
       "      <td>0.435937</td>\n",
       "      <td>0.035241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.401162</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.435649</td>\n",
       "      <td>0.035531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.400962</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.435636</td>\n",
       "      <td>0.035789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.400442</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.435524</td>\n",
       "      <td>0.036278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.400060</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>0.435140</td>\n",
       "      <td>0.036231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.399748</td>\n",
       "      <td>0.010698</td>\n",
       "      <td>0.435059</td>\n",
       "      <td>0.036187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.399014</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>0.434782</td>\n",
       "      <td>0.036432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.398543</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>0.434627</td>\n",
       "      <td>0.036380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.398242</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>0.434306</td>\n",
       "      <td>0.036436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.398027</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>0.434199</td>\n",
       "      <td>0.036489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.397627</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.433942</td>\n",
       "      <td>0.036524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.397133</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.433835</td>\n",
       "      <td>0.037015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.396799</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.433926</td>\n",
       "      <td>0.037013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.396528</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>0.433716</td>\n",
       "      <td>0.037159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.396403</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.433744</td>\n",
       "      <td>0.037349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.396163</td>\n",
       "      <td>0.011408</td>\n",
       "      <td>0.433820</td>\n",
       "      <td>0.037432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.395454</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.433707</td>\n",
       "      <td>0.037657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.395105</td>\n",
       "      <td>0.011678</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.037710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.394756</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.433355</td>\n",
       "      <td>0.037991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.394392</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>0.432757</td>\n",
       "      <td>0.038672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.394160</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>0.432578</td>\n",
       "      <td>0.038866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.393845</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.432501</td>\n",
       "      <td>0.039088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.393636</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.432425</td>\n",
       "      <td>0.039194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.393192</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>0.432096</td>\n",
       "      <td>0.039302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.392460</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>0.431824</td>\n",
       "      <td>0.039392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.392240</td>\n",
       "      <td>0.011336</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>0.039511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.391888</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.431388</td>\n",
       "      <td>0.040072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.391675</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.431431</td>\n",
       "      <td>0.040272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.391332</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.431478</td>\n",
       "      <td>0.040197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.391117</td>\n",
       "      <td>0.011108</td>\n",
       "      <td>0.431293</td>\n",
       "      <td>0.040253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.685093           0.000379           0.686558   \n",
       "1              0.671230           0.009434           0.675102   \n",
       "2              0.659008           0.010491           0.663125   \n",
       "3              0.644336           0.014084           0.650664   \n",
       "4              0.636066           0.016528           0.643942   \n",
       "5              0.623169           0.012498           0.631285   \n",
       "6              0.602033           0.009843           0.610209   \n",
       "7              0.591650           0.015947           0.599721   \n",
       "8              0.585461           0.019497           0.593338   \n",
       "9              0.578619           0.018259           0.586940   \n",
       "10             0.570801           0.018858           0.580262   \n",
       "11             0.560062           0.022019           0.569363   \n",
       "12             0.556937           0.020256           0.566690   \n",
       "13             0.554872           0.019017           0.564808   \n",
       "14             0.546951           0.015151           0.557979   \n",
       "15             0.541852           0.011557           0.553449   \n",
       "16             0.536363           0.015298           0.548007   \n",
       "17             0.529293           0.017539           0.541409   \n",
       "18             0.523798           0.019147           0.536885   \n",
       "19             0.520973           0.019159           0.534507   \n",
       "20             0.516376           0.021708           0.530500   \n",
       "21             0.513121           0.022248           0.527367   \n",
       "22             0.509301           0.024330           0.523904   \n",
       "23             0.502340           0.024646           0.517755   \n",
       "24             0.495447           0.019774           0.510592   \n",
       "25             0.489835           0.016329           0.505060   \n",
       "26             0.488421           0.016179           0.504394   \n",
       "27             0.485823           0.015821           0.501844   \n",
       "28             0.480789           0.013510           0.498244   \n",
       "29             0.479223           0.012897           0.496588   \n",
       "..                  ...                ...                ...   \n",
       "102            0.401454           0.010871           0.435937   \n",
       "103            0.401162           0.010739           0.435649   \n",
       "104            0.400962           0.010795           0.435636   \n",
       "105            0.400442           0.010828           0.435524   \n",
       "106            0.400060           0.010824           0.435140   \n",
       "107            0.399748           0.010698           0.435059   \n",
       "108            0.399014           0.010926           0.434782   \n",
       "109            0.398543           0.010926           0.434627   \n",
       "110            0.398242           0.010878           0.434306   \n",
       "111            0.398027           0.010912           0.434199   \n",
       "112            0.397627           0.011141           0.433942   \n",
       "113            0.397133           0.011175           0.433835   \n",
       "114            0.396799           0.011203           0.433926   \n",
       "115            0.396528           0.011347           0.433716   \n",
       "116            0.396403           0.011355           0.433744   \n",
       "117            0.396163           0.011408           0.433820   \n",
       "118            0.395454           0.011635           0.433707   \n",
       "119            0.395105           0.011678           0.433470   \n",
       "120            0.394756           0.011652           0.433355   \n",
       "121            0.394392           0.011326           0.432757   \n",
       "122            0.394160           0.011433           0.432578   \n",
       "123            0.393845           0.011346           0.432501   \n",
       "124            0.393636           0.011348           0.432425   \n",
       "125            0.393192           0.011366           0.432096   \n",
       "126            0.392460           0.011382           0.431824   \n",
       "127            0.392240           0.011336           0.431829   \n",
       "128            0.391888           0.011091           0.431388   \n",
       "129            0.391675           0.011100           0.431431   \n",
       "130            0.391332           0.011102           0.431478   \n",
       "131            0.391117           0.011108           0.431293   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.000999  \n",
       "1            0.008816  \n",
       "2            0.007590  \n",
       "3            0.011789  \n",
       "4            0.013259  \n",
       "5            0.012165  \n",
       "6            0.013601  \n",
       "7            0.021180  \n",
       "8            0.024632  \n",
       "9            0.022570  \n",
       "10           0.021636  \n",
       "11           0.024691  \n",
       "12           0.023334  \n",
       "13           0.022910  \n",
       "14           0.020297  \n",
       "15           0.020814  \n",
       "16           0.021016  \n",
       "17           0.021934  \n",
       "18           0.024915  \n",
       "19           0.025665  \n",
       "20           0.025853  \n",
       "21           0.026611  \n",
       "22           0.027239  \n",
       "23           0.029960  \n",
       "24           0.026663  \n",
       "25           0.024653  \n",
       "26           0.024912  \n",
       "27           0.026256  \n",
       "28           0.023527  \n",
       "29           0.025181  \n",
       "..                ...  \n",
       "102          0.035241  \n",
       "103          0.035531  \n",
       "104          0.035789  \n",
       "105          0.036278  \n",
       "106          0.036231  \n",
       "107          0.036187  \n",
       "108          0.036432  \n",
       "109          0.036380  \n",
       "110          0.036436  \n",
       "111          0.036489  \n",
       "112          0.036524  \n",
       "113          0.037015  \n",
       "114          0.037013  \n",
       "115          0.037159  \n",
       "116          0.037349  \n",
       "117          0.037432  \n",
       "118          0.037657  \n",
       "119          0.037710  \n",
       "120          0.037991  \n",
       "121          0.038672  \n",
       "122          0.038866  \n",
       "123          0.039088  \n",
       "124          0.039194  \n",
       "125          0.039302  \n",
       "126          0.039392  \n",
       "127          0.039511  \n",
       "128          0.040072  \n",
       "129          0.040272  \n",
       "130          0.040197  \n",
       "131          0.040253  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8829\n",
      "AUC Score (Train): 0.928483\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807175\n",
      "F1: 0.703448\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup our parameters for XGBoost to test\n",
    "param_test1 = {\n",
    " 'max_depth':range(2,9,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n",
    "    param_grid = param_test1, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bytree=0.8,\n",
       "                                     gamma=0, learning_rate=0.1,\n",
       "                                     max_delta_step=0, max_depth=5,\n",
       "                                     min_child_weight=1, missing=None,\n",
       "                                     n_estimators=140, n_jobs=1, nthread=4,\n",
       "                                     objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=27, silent=True,\n",
       "                                     subsample=0.8),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'max_depth': range(2, 9, 2),\n",
       "                         'min_child_weight': range(1, 6, 2)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.27842159, 0.2208766 , 0.1593586 , 0.37185698, 0.30205517,\n",
       "        0.26691289, 0.4104702 , 0.26880445, 0.3407918 , 0.43998938,\n",
       "        0.35393462, 0.24527669]),\n",
       " 'std_fit_time': array([0.13502412, 0.05963471, 0.075033  , 0.09462327, 0.06235456,\n",
       "        0.10903319, 0.05103867, 0.18050905, 0.09514759, 0.10229821,\n",
       "        0.0745524 , 0.07693062]),\n",
       " 'mean_score_time': array([0.00309076, 0.00337448, 0.00328403, 0.0032238 , 0.00306163,\n",
       "        0.0031415 , 0.00300846, 0.00318093, 0.00313935, 0.00376949,\n",
       "        0.00312657, 0.00297556]),\n",
       " 'std_score_time': array([0.00045596, 0.00037775, 0.00026888, 0.00017142, 0.0001863 ,\n",
       "        0.0002487 , 0.00020451, 0.0001699 , 0.00023073, 0.0007054 ,\n",
       "        0.00042213, 0.00099571]),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 2, 'min_child_weight': 1},\n",
       "  {'max_depth': 2, 'min_child_weight': 3},\n",
       "  {'max_depth': 2, 'min_child_weight': 5},\n",
       "  {'max_depth': 4, 'min_child_weight': 1},\n",
       "  {'max_depth': 4, 'min_child_weight': 3},\n",
       "  {'max_depth': 4, 'min_child_weight': 5},\n",
       "  {'max_depth': 6, 'min_child_weight': 1},\n",
       "  {'max_depth': 6, 'min_child_weight': 3},\n",
       "  {'max_depth': 6, 'min_child_weight': 5},\n",
       "  {'max_depth': 8, 'min_child_weight': 1},\n",
       "  {'max_depth': 8, 'min_child_weight': 3},\n",
       "  {'max_depth': 8, 'min_child_weight': 5}],\n",
       " 'split0_test_score': array([0.91544375, 0.91521081, 0.91171675, 0.91031912, 0.91684137,\n",
       "        0.92709061, 0.91870487, 0.91777312, 0.92988586, 0.90752388,\n",
       "        0.92709061, 0.92872117]),\n",
       " 'split1_test_score': array([0.83007221, 0.83787561, 0.83484743, 0.84544608, 0.85068717,\n",
       "        0.84416492, 0.83659446, 0.84928954, 0.84532961, 0.83799208,\n",
       "        0.84882367, 0.84579548]),\n",
       " 'split2_test_score': array([0.83251805, 0.83018868, 0.83240158, 0.83368274, 0.84020498,\n",
       "        0.82599581, 0.8427673 , 0.84183555, 0.8312369 , 0.84439786,\n",
       "        0.83834149, 0.83146983]),\n",
       " 'split3_test_score': array([0.90697115, 0.90901442, 0.90877404, 0.89350962, 0.89795673,\n",
       "        0.90540865, 0.89735577, 0.90012019, 0.90901442, 0.89302885,\n",
       "        0.90324519, 0.90925481]),\n",
       " 'split4_test_score': array([0.88978365, 0.88653846, 0.89711538, 0.88822115, 0.89122596,\n",
       "        0.89519231, 0.88774038, 0.89338942, 0.90360577, 0.896875  ,\n",
       "        0.88810096, 0.89639423]),\n",
       " 'mean_test_score': array([0.87495776, 0.8757656 , 0.87697104, 0.87423574, 0.87938324,\n",
       "        0.87957046, 0.87663255, 0.88048156, 0.88381451, 0.87596353,\n",
       "        0.88112038, 0.88232711]),\n",
       " 'std_test_score': array([0.03660497, 0.03546934, 0.03573605, 0.02947024, 0.02914356,\n",
       "        0.03819323, 0.03185222, 0.02969648, 0.03845723, 0.02885409,\n",
       "        0.03324037, 0.03740699]),\n",
       " 'rank_test_score': array([11, 10,  7, 12,  6,  5,  8,  4,  1,  9,  3,  2], dtype=int32)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'min_child_weight': 5}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8838145113243383"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.780269\n",
      "F1: 0.662069\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa7c88389e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 40,\n",
       " 'Q': 20,\n",
       " 'Fare': 161,\n",
       " 'Parch': 64,\n",
       " 'Pclass': 51,\n",
       " 'SibSp': 82,\n",
       " 'Age': 121,\n",
       " 'male': 32,\n",
       " 'youngin': 32}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bytree=0.8, gamma=0, learning_rate=0.1,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=5, missing=nan,\n",
      "              n_estimators=140, n_jobs=1, nthread=4,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
      "              subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
